[{"_id":{"$oid":"5aea850b5a59986e57287de2"},"id":"arXiv:1804.11227v1","url":"http://arxiv.org/abs/1804.11227v1","updated":"2018-04-30T14:26:15Z","published":"2018-04-30T14:26:15Z","title":"Decoupling Respiratory and Angular Variation in Rotational X-ray Scans\n  Using a Prior Bilinear Model","summary":"Data-driven respiratory signal extraction from rotational X-ray scans has always been challenging due to angular effects overlapping with respiration-induced change in the scene. In the context of motion modelling their main drawback is the fact that most of these methods only extract a 1D signal, that, at best, can be decomposed into amplitude and phase. In this paper, we use the linearity of the Radon operator to propose a bilinear model based on a prior 4D scan to separate angular and respiratory variation. Out-of-sample extension is enhanced by a B-spline interpolation using prior knowledge about the trajectory angle to extract respiratory feature weights independent of the acquisition angle. Though the prerequisite of a prior 4D scan seems steep, our proposed application of respiratory motion estimation in radiation therapy usually meets this requirement. We test our approach on DRRs of a patient's 4D CT in a leave-one-out manner and achieve a mean estimation error of 5.2% in the gray values for previously unseen viewing angles.","author":["Tobias Geimer","Paul Keall","Katharina Breininger","Vincent Caillet","Michelle Dunbar","Christoph Bert","Andreas Maier"],"primaryCategory":"cs.CV","category":["cs.CV"],"timestamp_ms":{"$numberLong":"1525123575000"},"text":"Data-driven respiratory signal extraction from rotational X-ray scans has always been challenging due to angular effects overlapping with respiration-induced change in the scene. In the context of motion modelling their main drawback is the fact that most of these methods only extract a 1D signal, that, at best, can be decomposed into amplitude and phase. In this paper, we use the linearity of the Radon operator to propose a bilinear model based on a prior 4D scan to separate angular and respiratory variation. Out-of-sample extension is enhanced by a B-spline interpolation using prior knowledge about the trajectory angle to extract respiratory feature weights independent of the acquisition angle. Though the prerequisite of a prior 4D scan seems steep, our proposed application of respiratory motion estimation in radiation therapy usually meets this requirement. We test our approach on DRRs of a patient's 4D CT in a leave-one-out manner and achieve a mean estimation error of 5.2% in the gray values for previously unseen viewing angles.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287de1"},"id":"arXiv:1804.10959v1","url":"http://arxiv.org/abs/1804.10959v1","updated":"2018-04-29T16:13:44Z","published":"2018-04-29T16:13:44Z","title":"Subword Regularization: Improving Neural Network Translation Models with\n  Multiple Subword Candidates","summary":"Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.","author":["Taku Kudo"],"primaryCategory":"cs.CL","category":["cs.CL"],"timestamp_ms":{"$numberLong":"1525043624000"},"text":"Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287de0"},"id":"arXiv:1805.00289v2","url":"http://arxiv.org/abs/1805.00289v2","updated":"2018-05-02T12:02:20Z","published":"2018-05-01T12:26:33Z","title":"Denotational semantics of recursive types in synthetic guarded domain\n  theory","summary":"Just like any other branch of mathematics, denotational semantics of programming languages should be formalised in type theory, but adapting traditional domain theoretic semantics, as originally formulated in classical set theory to type theory has proven challenging. This paper is part of a project on formulating denotational semantics in type theories with guarded recursion. This should have the benefit of not only giving simpler semantics and proofs of properties such as adequacy, but also hopefully in the future to scale to languages with advanced features, such as general references, outside the reach of traditional domain theoretic techniques. Working in Guarded Dependent Type Theory (GDTT), we develop denotational semantics for FPC, the simply typed lambda calculus extended with recursive types, modelling the recursive types of FPC using the guarded recursive types of GDTT. We prove soundness and computational adequacy of the model in GDTT using a logical relation between syntax and semantics constructed also using guarded recursive types. The denotational semantics is intensional in the sense that it counts the number of unfold-fold reductions needed to compute the value of a term, but we construct a relation relating the denotations of extensionally equal terms, i.e., pairs of terms that compute the same value in a different number of steps. Finally we show how the denotational semantics of terms can be executed inside type theory and prove that executing the denotation of a boolean term computes the same value as the operational semantics of FPC.","author":["Rasmus E. Møgelberg","Marco Paviotti"],"primaryCategory":"cs.LO","category":["cs.LO"],"timestamp_ms":{"$numberLong":"1525287740000"},"text":"Just like any other branch of mathematics, denotational semantics of programming languages should be formalised in type theory, but adapting traditional domain theoretic semantics, as originally formulated in classical set theory to type theory has proven challenging. This paper is part of a project on formulating denotational semantics in type theories with guarded recursion. This should have the benefit of not only giving simpler semantics and proofs of properties such as adequacy, but also hopefully in the future to scale to languages with advanced features, such as general references, outside the reach of traditional domain theoretic techniques. Working in Guarded Dependent Type Theory (GDTT), we develop denotational semantics for FPC, the simply typed lambda calculus extended with recursive types, modelling the recursive types of FPC using the guarded recursive types of GDTT. We prove soundness and computational adequacy of the model in GDTT using a logical relation between syntax and semantics constructed also using guarded recursive types. The denotational semantics is intensional in the sense that it counts the number of unfold-fold reductions needed to compute the value of a term, but we construct a relation relating the denotations of extensionally equal terms, i.e., pairs of terms that compute the same value in a different number of steps. Finally we show how the denotational semantics of terms can be executed inside type theory and prove that executing the denotation of a boolean term computes the same value as the operational semantics of FPC.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287ddf"},"id":"arXiv:1804.11258v1","url":"http://arxiv.org/abs/1804.11258v1","updated":"2018-04-30T15:04:21Z","published":"2018-04-30T15:04:21Z","title":"Towards Diverse Text Generation with Inverse Reinforcement Learning","summary":"Text generation is a crucial task in NLP. Recently, several adversarial generative models have been proposed to improve the exposure bias problem in text generation. Though these models gain great success, they still suffer from the problems of reward sparsity and mode collapse. In order to address these two problems, in this paper, we employ inverse reinforcement learning (IRL) for text generation. Specifically, the IRL framework learns a reward function on training data, and then an optimal policy to maximum the expected total reward. Similar to the adversarial models, the reward and policy function in IRL are optimized alternately. Our method has two advantages: (1) the reward function can produce more dense reward signals. (2) the generation policy, trained by \"entropy regularized\" policy gradient, encourages to generate more diversified texts. Experiment results demonstrate that our proposed method can generate higher quality texts than the previous methods.","author":["Zhan Shi","Xinchi Chen","Xipeng Qiu","Xuanjing Huang"],"primaryCategory":"cs.CL","category":["cs.CL","cs.LG","stat.ML"],"timestamp_ms":{"$numberLong":"1525125861000"},"text":"Text generation is a crucial task in NLP. Recently, several adversarial generative models have been proposed to improve the exposure bias problem in text generation. Though these models gain great success, they still suffer from the problems of reward sparsity and mode collapse. In order to address these two problems, in this paper, we employ inverse reinforcement learning (IRL) for text generation. Specifically, the IRL framework learns a reward function on training data, and then an optimal policy to maximum the expected total reward. Similar to the adversarial models, the reward and policy function in IRL are optimized alternately. Our method has two advantages: (1) the reward function can produce more dense reward signals. (2) the generation policy, trained by \"entropy regularized\" policy gradient, encourages to generate more diversified texts. Experiment results demonstrate that our proposed method can generate higher quality texts than the previous methods.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dde"},"id":"arXiv:1804.10363v1","url":"http://arxiv.org/abs/1804.10363v1","updated":"2018-04-27T07:14:59Z","published":"2018-04-27T07:14:59Z","title":"SaC2Vec: Information Network Representation with Structure and Content","summary":"Network representation learning (also known as information network embedding) has been the central piece of research in social and information network analytics for the last couple of years. An information network can be viewed as a linked structure of a set of entities. A set of linked web pages and documents, a set of users in a social network are common examples of information network. Typically a node in the information network is formed with a unique id, some content information and the links to its direct neighbors. Information network representation techniques traditionally use only link structure of the network. But the textual or other types of content in each node plays an important role to understand the underlying semantics of the network.   In this paper, we propose Sac2Vec, a network representation technique using structure and content. It is a multi-layered graph approach which uses a random walk to generate the node embedding. Our approach is simple and computationally fast, yet able to use the content as a complement to structure and the vice-versa. Experimental evaluations on three real world publicly available datasets show the merit of our approach compared to state-of-the-art algorithms in the domain.","author":["Sambaran Bandyopadhyay","Harsh Kara","Anirban Biswas","M N Murty"],"primaryCategory":"cs.SI","category":["cs.SI","physics.soc-ph"],"timestamp_ms":{"$numberLong":"1524838499000"},"text":"Network representation learning (also known as information network embedding) has been the central piece of research in social and information network analytics for the last couple of years. An information network can be viewed as a linked structure of a set of entities. A set of linked web pages and documents, a set of users in a social network are common examples of information network. Typically a node in the information network is formed with a unique id, some content information and the links to its direct neighbors. Information network representation techniques traditionally use only link structure of the network. But the textual or other types of content in each node plays an important role to understand the underlying semantics of the network.   In this paper, we propose Sac2Vec, a network representation technique using structure and content. It is a multi-layered graph approach which uses a random walk to generate the node embedding. Our approach is simple and computationally fast, yet able to use the content as a complement to structure and the vice-versa. Experimental evaluations on three real world publicly available datasets show the merit of our approach compared to state-of-the-art algorithms in the domain.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287ddd"},"id":"arXiv:1804.10723v1","url":"http://arxiv.org/abs/1804.10723v1","updated":"2018-04-28T02:38:19Z","published":"2018-04-28T02:38:19Z","title":"Extracting the Most Weighted Throughput in UAV Empowered Wireless\n  Systems With Nonlinear Energy Harvester","summary":"With the maturity of unmanned aerial vehicle (UAV) technology, this work investigates the integration of UAV into wireless communication systems. Since the UAV is powered by a capacity-limited battery, this work proposes to use the radio energy harvesting technology at the UAV in order to extend the lifetime of UAV empowered base station. To extract the most weighted throughput of UAV empowered wireless systems, the dirty paper coding scheme and information-theoretic uplink-downlink channel duality are exploited to propose an extracting the most weighted throughput algorithm. Numerical results are used to verify the proposed algorithm.","author":["Yanjie Dong","Julian Cheng","Md. Jahangir Hossain","Victor C. M. Leung"],"primaryCategory":"cs.IT","category":["cs.IT","math.IT"],"timestamp_ms":{"$numberLong":"1524908299000"},"text":"With the maturity of unmanned aerial vehicle (UAV) technology, this work investigates the integration of UAV into wireless communication systems. Since the UAV is powered by a capacity-limited battery, this work proposes to use the radio energy harvesting technology at the UAV in order to extend the lifetime of UAV empowered base station. To extract the most weighted throughput of UAV empowered wireless systems, the dirty paper coding scheme and information-theoretic uplink-downlink channel duality are exploited to propose an extracting the most weighted throughput algorithm. Numerical results are used to verify the proposed algorithm.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287ddc"},"id":"arXiv:1805.00566v1","url":"http://arxiv.org/abs/1805.00566v1","updated":"2018-05-01T21:57:34Z","published":"2018-05-01T21:57:34Z","title":"How to end password reuse on the web","summary":"We present a framework by which websites can coordinate to make it difficult for users to set similar passwords at these websites, in an effort to break the culture of password reuse on the web today. Though the design of such a framework is fraught with risks to users' security and privacy, we show that these risks can be effectively mitigated through careful scoping of the goals for such a framework and through principled design. At the core of our framework is a private set-membership-test protocol that enables one website to determine, upon a user setting a password for use at it, whether that user has already set a similar password at another website, but with neither side disclosing to the other the password(s) it employs in the protocol. Our framework then layers over this protocol a collection of techniques to mitigate the leakage necessitated by such a test. These mechanisms are consistent with common user experience today, and so our framework should be unobtrusive to users who do not reuse similar passwords across websites (e.g., due to having adopted a password manager). Through a working implementation of our framework and optimization of its parameters based on insights of how passwords tend to be reused, we show that our design can meet the scalability challenges facing such a service.","author":["Ke Coby Wang","Michael K. Reiter"],"primaryCategory":"cs.CR","category":["cs.CR"],"timestamp_ms":{"$numberLong":"1525237054000"},"text":"We present a framework by which websites can coordinate to make it difficult for users to set similar passwords at these websites, in an effort to break the culture of password reuse on the web today. Though the design of such a framework is fraught with risks to users' security and privacy, we show that these risks can be effectively mitigated through careful scoping of the goals for such a framework and through principled design. At the core of our framework is a private set-membership-test protocol that enables one website to determine, upon a user setting a password for use at it, whether that user has already set a similar password at another website, but with neither side disclosing to the other the password(s) it employs in the protocol. Our framework then layers over this protocol a collection of techniques to mitigate the leakage necessitated by such a test. These mechanisms are consistent with common user experience today, and so our framework should be unobtrusive to users who do not reuse similar passwords across websites (e.g., due to having adopted a password manager). Through a working implementation of our framework and optimization of its parameters based on insights of how passwords tend to be reused, we show that our design can meet the scalability challenges facing such a service.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287ddb"},"id":"arXiv:1804.10411v1","url":"http://arxiv.org/abs/1804.10411v1","updated":"2018-04-27T09:27:29Z","published":"2018-04-27T09:27:29Z","title":"Automation Of Road Intersections Using Consensus-based Auction\n  Algorithms","summary":"This paper investigates a consensus-based auction algorithm in the context of decentralized traffic control. In particular, we study the automation of a road intersection, where a set of vehicles is required to cross without collisions. The crossing order will be negotiated in a decentralized fashion. An on-board model predictive controller (MPC) will compute an optimal trajectory which avoids collisions with higher priority vehicles, thus retaining convex safety constraints. Simulations are then performed in a time-variant traffic environment.","author":["Fabio Molinari","Jörg Raisch"],"primaryCategory":"cs.SY","category":["cs.SY"],"timestamp_ms":{"$numberLong":"1524846449000"},"text":"This paper investigates a consensus-based auction algorithm in the context of decentralized traffic control. In particular, we study the automation of a road intersection, where a set of vehicles is required to cross without collisions. The crossing order will be negotiated in a decentralized fashion. An on-board model predictive controller (MPC) will compute an optimal trajectory which avoids collisions with higher priority vehicles, thus retaining convex safety constraints. Simulations are then performed in a time-variant traffic environment.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dda"},"id":"arXiv:1805.00181v1","url":"http://arxiv.org/abs/1805.00181v1","updated":"2018-05-01T04:18:15Z","published":"2018-05-01T04:18:15Z","title":"Spectrally Robust Graph Isomorphism","summary":"We initiate the study of spectral generalizations of the graph isomorphism problem.   (a)The Spectral Graph Dominance (SGD) problem: On input of two graphs $G$ and $H$ does there exist a permutation $\\pi$ such that $G\\preceq \\pi(H)$?   (b) The Spectrally Robust Graph Isomorphism (SRGI) problem: On input of two graphs $G$ and $H$, find the smallest number $\\kappa$ over all permutations $\\pi$ such that $ \\pi(H) \\preceq G\\preceq \\kappa c \\pi(H)$ for some $c$. SRGI is a natural formulation of the network alignment problem that has various applications, most notably in computational biology.   Here $G\\preceq c H$ means that for all vectors $x$ we have $x^T L_G x \\leq c x^T L_H x$, where $L_G$ is the Laplacian $G$.   We prove NP-hardness for SGD. We also present a $\\kappa$-approximation algorithm for SRGI for the case when both $G$ and $H$ are bounded-degree trees. The algorithm runs in polynomial time when $\\kappa$ is a constant.","author":["Alexandra Kolla","Ioannis Koutis","Vivek Madan","Ali Kemal Sinop"],"primaryCategory":"cs.DS","category":["cs.DS"],"timestamp_ms":{"$numberLong":"1525173495000"},"text":"We initiate the study of spectral generalizations of the graph isomorphism problem.   (a)The Spectral Graph Dominance (SGD) problem: On input of two graphs $G$ and $H$ does there exist a permutation $\\pi$ such that $G\\preceq \\pi(H)$?   (b) The Spectrally Robust Graph Isomorphism (SRGI) problem: On input of two graphs $G$ and $H$, find the smallest number $\\kappa$ over all permutations $\\pi$ such that $ \\pi(H) \\preceq G\\preceq \\kappa c \\pi(H)$ for some $c$. SRGI is a natural formulation of the network alignment problem that has various applications, most notably in computational biology.   Here $G\\preceq c H$ means that for all vectors $x$ we have $x^T L_G x \\leq c x^T L_H x$, where $L_G$ is the Laplacian $G$.   We prove NP-hardness for SGD. We also present a $\\kappa$-approximation algorithm for SRGI for the case when both $G$ and $H$ are bounded-degree trees. The algorithm runs in polynomial time when $\\kappa$ is a constant.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dd9"},"id":"arXiv:1805.00808v1","url":"http://arxiv.org/abs/1805.00808v1","updated":"2018-05-01T16:03:23Z","published":"2018-05-01T16:03:23Z","title":"Formal Process Virtual Machine for Smart Contracts Verification","summary":"This paper reports on the development and verification of a novel formal symbolic process virtual machine (FSPVM) for verifying the reliability and security of Ethereum smart contracts, denoted as FSPVM-E, completely in Coq proof assistant. It adopts execution-verification isomorphism (EVI), an extension of Curry-Howard isomorphism (CHI), as its fundamental theoretical framework. The current version of FSPVM-E is constructed on a general, extensible, and reusable formal memory (GERM) framework, an extensible and universal formal intermediate programming language Lolisa, which is a large subset of the Solidity programming language using generalized algebraic datatypes, and the corresponding formally verified interpreter of Lolisa, denoted as FEther. It supports the ERC20 standard and can automatically simultaneously symbolically execute the smart contract programs of Ethereum and verify their reliability and security properties using Hoare logic in Coq. In addition, this work, contributes to solving the problems of automation, inconsistency and reusability in higher-order logic theorem proving.","author":["Zheng Yang","Hang Lei"],"primaryCategory":"cs.PL","category":["cs.PL"],"timestamp_ms":{"$numberLong":"1525215803000"},"text":"This paper reports on the development and verification of a novel formal symbolic process virtual machine (FSPVM) for verifying the reliability and security of Ethereum smart contracts, denoted as FSPVM-E, completely in Coq proof assistant. It adopts execution-verification isomorphism (EVI), an extension of Curry-Howard isomorphism (CHI), as its fundamental theoretical framework. The current version of FSPVM-E is constructed on a general, extensible, and reusable formal memory (GERM) framework, an extensible and universal formal intermediate programming language Lolisa, which is a large subset of the Solidity programming language using generalized algebraic datatypes, and the corresponding formally verified interpreter of Lolisa, denoted as FEther. It supports the ERC20 standard and can automatically simultaneously symbolically execute the smart contract programs of Ethereum and verify their reliability and security properties using Hoare logic in Coq. In addition, this work, contributes to solving the problems of automation, inconsistency and reusability in higher-order logic theorem proving.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dd8"},"id":"arXiv:1805.00754v1","url":"http://arxiv.org/abs/1805.00754v1","updated":"2018-05-02T12:04:37Z","published":"2018-05-02T12:04:37Z","title":"TR-SVD: Fast and Memory Efficient Method for Time Ranged Singular Value\n  Decomposition","summary":"Given multiple time series data, how can we efficiently find latent patterns in an arbitrary time range? Singular value decomposition (SVD) is a crucial tool to discover hidden factors in multiple time series data, and has been used in many data mining applications including dimensionality reduction, principal component analysis, recommender systems, etc. Along with its static version, incremental SVD has been used to deal with multiple semi-infinite time series data and to identify patterns of the data. However, existing SVD methods for the multiple time series data analysis do not provide functionality for detecting patterns of data in an arbitrary time range: standard SVD requires data for all intervals corresponding to a time range query, and incremental SVD does not consider an arbitrary time range. In this paper, we propose TR-SVD (Time Ranged Singular Value Decomposition), a fast and memory efficient method for finding latent factors of time series data in an arbitrary time range. TR-SVD incrementally compresses multiple time series data block by block to reduce the space cost in storage phase, and efficiently computes singular value decomposition (SVD) for a given time range query in query phase by carefully stitching stored SVD results. Through extensive experiments, we demonstrate that TR-SVD is up to 15 x faster, and requires 15 x less space than existing methods. Our case study shows that TR-SVD is useful for capturing past time ranges whose patterns are similar to a query time range.","author":["Jun-Gi Jang","Dongjin Choi","Jinhong Jung","U Kang"],"primaryCategory":"cs.NA","category":["cs.NA"],"timestamp_ms":{"$numberLong":"1525287877000"},"text":"Given multiple time series data, how can we efficiently find latent patterns in an arbitrary time range? Singular value decomposition (SVD) is a crucial tool to discover hidden factors in multiple time series data, and has been used in many data mining applications including dimensionality reduction, principal component analysis, recommender systems, etc. Along with its static version, incremental SVD has been used to deal with multiple semi-infinite time series data and to identify patterns of the data. However, existing SVD methods for the multiple time series data analysis do not provide functionality for detecting patterns of data in an arbitrary time range: standard SVD requires data for all intervals corresponding to a time range query, and incremental SVD does not consider an arbitrary time range. In this paper, we propose TR-SVD (Time Ranged Singular Value Decomposition), a fast and memory efficient method for finding latent factors of time series data in an arbitrary time range. TR-SVD incrementally compresses multiple time series data block by block to reduce the space cost in storage phase, and efficiently computes singular value decomposition (SVD) for a given time range query in query phase by carefully stitching stored SVD results. Through extensive experiments, we demonstrate that TR-SVD is up to 15 x faster, and requires 15 x less space than existing methods. Our case study shows that TR-SVD is useful for capturing past time ranges whose patterns are similar to a query time range.","relevant":1},{"_id":{"$oid":"5aea850b5a59986e57287dd7"},"id":"arXiv:1804.10112v1","url":"http://arxiv.org/abs/1804.10112v1","updated":"2018-04-23T20:57:59Z","published":"2018-04-23T20:57:59Z","title":"Unified Sparse Formats for Tensor Algebra Compilers","summary":"This paper shows how to build a sparse tensor algebra compiler that is agnostic to tensor formats (data layouts). We develop an interface that describes formats in terms of their capabilities and properties, and show how to build a modular code generator where new formats can be added as plugins. We then describe six implementations of the interface that compose to form the dense, CSR/CSF, COO, DIA, ELL, and HASH tensor formats and countless variants thereof. With these implementations at hand, our code generator can generate code for any tensor algebra expression on any combination of the aforementioned formats.   To demonstrate our modular code generator design, we have implemented it in the open-source taco tensor algebra compiler. Our evaluation shows that we get better performance by supporting more formats specialized to different tensor structures, and our plugins makes it easy to add new formats. For example, when data is provided in the COO format, computing a single matrix-vector multiplication with COO is up to 3.6$\\times$ faster than with CSR. Furthermore, DIA is specialized to tensor convolutions and stencil operations and therefore performs up to 22% faster than CSR for such operations. To further demonstrate the importance of support for many formats, we show that the best vector format for matrix-vector multiplication varies with input sparsities, from hash maps to sparse vectors to dense vectors. Finally, we show that the performance of generated code for these formats is competitive with hand-optimized implementations.","author":["Stephen Chou","Fredrik Kjolstad","Saman Amarasinghe"],"primaryCategory":"cs.MS","category":["cs.MS","cs.PL"],"timestamp_ms":{"$numberLong":"1524542279000"},"text":"This paper shows how to build a sparse tensor algebra compiler that is agnostic to tensor formats (data layouts). We develop an interface that describes formats in terms of their capabilities and properties, and show how to build a modular code generator where new formats can be added as plugins. We then describe six implementations of the interface that compose to form the dense, CSR/CSF, COO, DIA, ELL, and HASH tensor formats and countless variants thereof. With these implementations at hand, our code generator can generate code for any tensor algebra expression on any combination of the aforementioned formats.   To demonstrate our modular code generator design, we have implemented it in the open-source taco tensor algebra compiler. Our evaluation shows that we get better performance by supporting more formats specialized to different tensor structures, and our plugins makes it easy to add new formats. For example, when data is provided in the COO format, computing a single matrix-vector multiplication with COO is up to 3.6$\\times$ faster than with CSR. Furthermore, DIA is specialized to tensor convolutions and stencil operations and therefore performs up to 22% faster than CSR for such operations. To further demonstrate the importance of support for many formats, we show that the best vector format for matrix-vector multiplication varies with input sparsities, from hash maps to sparse vectors to dense vectors. Finally, we show that the performance of generated code for these formats is competitive with hand-optimized implementations.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dd6"},"id":"arXiv:1804.10267v1","url":"http://arxiv.org/abs/1804.10267v1","updated":"2018-04-26T20:01:48Z","published":"2018-04-26T20:01:48Z","title":"Dismantling Efficiency and Network Fractality","summary":"Network dismantling is to identify a minimal set of nodes whose removal breaks the network into small components of subextensive size. Because finding the optimal set of nodes is an NP-hard problem, several heuristic algorithms have been developed as alternative methods, for instance, the so-called belief propagation-based decimation (BPD) algorithm and the collective influence (CI) algorithm. Here, we test the performance of each of these algorithms and analyze them in the perspective of the fractality of the network. Networks are classified into two types: fractal and non-fractal networks. Real-world examples include the World Wide Web and Internet at the autonomous system level, respectively. They have different ratios of long-range shortcuts to short-range ones. We find that the BPD algorithm works more efficiently for fractal networks than for non-fractal networks, whereas the opposite is true of the CI algorithm. Furthermore, we construct diverse fractal and non-fractal model networks by controlling parameters such as the degree exponent, shortcut number, and system size, and investigate how the performance of the two algorithms depends on structural features.","author":["Yoon Seok Im","B. Kahng"],"primaryCategory":"physics.soc-ph","category":["physics.soc-ph","cs.SI"],"timestamp_ms":{"$numberLong":"1524798108000"},"text":"Network dismantling is to identify a minimal set of nodes whose removal breaks the network into small components of subextensive size. Because finding the optimal set of nodes is an NP-hard problem, several heuristic algorithms have been developed as alternative methods, for instance, the so-called belief propagation-based decimation (BPD) algorithm and the collective influence (CI) algorithm. Here, we test the performance of each of these algorithms and analyze them in the perspective of the fractality of the network. Networks are classified into two types: fractal and non-fractal networks. Real-world examples include the World Wide Web and Internet at the autonomous system level, respectively. They have different ratios of long-range shortcuts to short-range ones. We find that the BPD algorithm works more efficiently for fractal networks than for non-fractal networks, whereas the opposite is true of the CI algorithm. Furthermore, we construct diverse fractal and non-fractal model networks by controlling parameters such as the degree exponent, shortcut number, and system size, and investigate how the performance of the two algorithms depends on structural features.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dd5"},"id":"arXiv:1805.00578v1","url":"http://arxiv.org/abs/1805.00578v1","updated":"2018-05-02T00:06:14Z","published":"2018-05-02T00:06:14Z","title":"Greedy Bipartite Matching in Random Type Poisson Arrival Model","summary":"We introduce a new random input model for bipartite matching which we call the Random Type Poisson Arrival Model. Just like in the known i.i.d. model (introduced by Feldman et al. 2009), online nodes have types in our model. In contrast to the adversarial types studied in the known i.i.d. model, following the random graphs studied in Mastin and Jaillet 2016, in our model each type graph is generated randomly by including each offline node in the neighborhood of an online node with probability $c/n$ independently. In our model, nodes of the same type appear consecutively in the input and the number of times each type node appears is distributed according to the Poisson distribution with parameter 1. We analyze the performance of the simple greedy algorithm under this input model. The performance is controlled by the parameter $c$ and we are able to exactly characterize the competitive ratio for the regimes $c = o(1)$ and $c = \\omega(1)$. We also provide a precise bound on the expected size of the matching in the remaining regime of constant $c$. We compare our results to the previous work of Mastin and Jaillet who analyzed the simple greedy algorithm in the $G_{n,n,p}$ model where each online node type occurs exactly once. We essentially show that the approach of Mastin and Jaillet can be extended to work for the Random Type Poisson Arrival Model, although several nontrivial technical challenges need to be overcome. Intuitively, one can view the Random Type Poisson Arrival Model as the $G_{n,n,p}$ model with less randomness; that is, instead of each online node having a new type, each online node has a chance of repeating the previous type.","author":["Allan Borodin","Christodoulos Karavasilis","Denis Pankratov"],"primaryCategory":"cs.DS","category":["cs.DS"],"timestamp_ms":{"$numberLong":"1525244774000"},"text":"We introduce a new random input model for bipartite matching which we call the Random Type Poisson Arrival Model. Just like in the known i.i.d. model (introduced by Feldman et al. 2009), online nodes have types in our model. In contrast to the adversarial types studied in the known i.i.d. model, following the random graphs studied in Mastin and Jaillet 2016, in our model each type graph is generated randomly by including each offline node in the neighborhood of an online node with probability $c/n$ independently. In our model, nodes of the same type appear consecutively in the input and the number of times each type node appears is distributed according to the Poisson distribution with parameter 1. We analyze the performance of the simple greedy algorithm under this input model. The performance is controlled by the parameter $c$ and we are able to exactly characterize the competitive ratio for the regimes $c = o(1)$ and $c = \\omega(1)$. We also provide a precise bound on the expected size of the matching in the remaining regime of constant $c$. We compare our results to the previous work of Mastin and Jaillet who analyzed the simple greedy algorithm in the $G_{n,n,p}$ model where each online node type occurs exactly once. We essentially show that the approach of Mastin and Jaillet can be extended to work for the Random Type Poisson Arrival Model, although several nontrivial technical challenges need to be overcome. Intuitively, one can view the Random Type Poisson Arrival Model as the $G_{n,n,p}$ model with less randomness; that is, instead of each online node having a new type, each online node has a chance of repeating the previous type.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dd4"},"id":"arXiv:1805.00336v1","url":"http://arxiv.org/abs/1805.00336v1","updated":"2018-04-28T02:51:43Z","published":"2018-04-28T02:51:43Z","title":"Hyperparameter Optimization for Effort Estimation","summary":"Software analytics has been widely used in software engineering for many tasks such as generating effort estimates for software projects. One of the \"black arts\" of software analytics is tuning the parameters controlling a data mining algorithm. Such hyperparameter optimization has been widely studied in other software analytics domains (e.g. defect prediction and text mining) but, so far, has not been extensively explored for effort estimation. Accordingly, this paper seeks simple, automatic, effective, and fast methods for finding good tunings for automatic software effort estimation.   We introduce a hyperparameter optimization architecture called OIL (Optimized Inductive learning). We test OIL on a wide range of hyperparameter optimizers using data from 945 software projects. After tuning, large improvements in effort estimation accuracy were observed (measured in terms of the magnitude of the relative error and standardized accuracy).   From those results, we can recommend using regression trees (CART) tuned by either different evolution or MOEA/D. This particular combination of learner and optimizers often achieves in one or two hours what other optimizers need days to weeks of CPU to accomplish.   An important part of this analysis is its reproducibility and refutability. All our scripts and data are on-line. It is hoped that this paper will prompt and enable much more research on better methods to tune software effort estimators.","author":["Tianpei Xia","Rahul Krishna","Jianfeng Chen","George Mathew","Xipeng Shen","Tim Menzies"],"primaryCategory":"cs.SE","category":["cs.SE"],"timestamp_ms":{"$numberLong":"1524909103000"},"text":"Software analytics has been widely used in software engineering for many tasks such as generating effort estimates for software projects. One of the \"black arts\" of software analytics is tuning the parameters controlling a data mining algorithm. Such hyperparameter optimization has been widely studied in other software analytics domains (e.g. defect prediction and text mining) but, so far, has not been extensively explored for effort estimation. Accordingly, this paper seeks simple, automatic, effective, and fast methods for finding good tunings for automatic software effort estimation.   We introduce a hyperparameter optimization architecture called OIL (Optimized Inductive learning). We test OIL on a wide range of hyperparameter optimizers using data from 945 software projects. After tuning, large improvements in effort estimation accuracy were observed (measured in terms of the magnitude of the relative error and standardized accuracy).   From those results, we can recommend using regression trees (CART) tuned by either different evolution or MOEA/D. This particular combination of learner and optimizers often achieves in one or two hours what other optimizers need days to weeks of CPU to accomplish.   An important part of this analysis is its reproducibility and refutability. All our scripts and data are on-line. It is hoped that this paper will prompt and enable much more research on better methods to tune software effort estimators.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dd3"},"id":"arXiv:1804.10750v1","url":"http://arxiv.org/abs/1804.10750v1","updated":"2018-04-28T06:31:05Z","published":"2018-04-28T06:31:05Z","title":"Efficient Subpixel Refinement with Symbolic Linear Predictors","summary":"We present an efficient subpixel refinement method usinga learning-based approach called Linear Predictors. Two key ideas are shown in this paper. Firstly, we present a novel technique, called Symbolic Linear Predictors, which makes the learning step efficient for subpixel refinement. This makes our approach feasible for online applications without compromising accuracy, while taking advantage of the run-time efficiency of learning based approaches. Secondly, we show how Linear Predictors can be used to predict the expected alignment error, allowing us to use only the best keypoints in resource constrained applications. We show the efficiency and accuracy of our method through extensive experiments.","author":["Vincent Lui","Jonathon Geeves","Winston Yii","Tom Drummond"],"primaryCategory":"cs.CV","category":["cs.CV"],"timestamp_ms":{"$numberLong":"1524922265000"},"text":"We present an efficient subpixel refinement method usinga learning-based approach called Linear Predictors. Two key ideas are shown in this paper. Firstly, we present a novel technique, called Symbolic Linear Predictors, which makes the learning step efficient for subpixel refinement. This makes our approach feasible for online applications without compromising accuracy, while taking advantage of the run-time efficiency of learning based approaches. Secondly, we show how Linear Predictors can be used to predict the expected alignment error, allowing us to use only the best keypoints in resource constrained applications. We show the efficiency and accuracy of our method through extensive experiments.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dd2"},"id":"arXiv:1805.00833v1","url":"http://arxiv.org/abs/1805.00833v1","updated":"2018-05-02T14:13:26Z","published":"2018-05-02T14:13:26Z","title":"Learnable PINs: Cross-Modal Embeddings for Person Identity","summary":"We propose and investigate an identity sensitive joint embedding of face and voice. Such an embedding enables cross-modal retrieval from voice to face and from face to voice. We make the following four contributions: first, we show that the embedding can be learnt from videos of talking faces, without requiring any identity labels, using a form of cross-modal self-supervision; second, we develop a curriculum learning schedule for hard negative mining targeted to this task, that is essential for learning to proceed successfully; third, we demonstrate and evaluate cross-modal retrieval for identities unseen and unheard during training over a number of scenarios and establish a benchmark for this novel task; finally, we show an application of using the joint embedding for automatically retrieving and labelling characters in TV dramas.","author":["Arsha Nagrani","Samuel Albanie","Andrew Zisserman"],"primaryCategory":"cs.CV","category":["cs.CV"],"timestamp_ms":{"$numberLong":"1525295606000"},"text":"We propose and investigate an identity sensitive joint embedding of face and voice. Such an embedding enables cross-modal retrieval from voice to face and from face to voice. We make the following four contributions: first, we show that the embedding can be learnt from videos of talking faces, without requiring any identity labels, using a form of cross-modal self-supervision; second, we develop a curriculum learning schedule for hard negative mining targeted to this task, that is essential for learning to proceed successfully; third, we demonstrate and evaluate cross-modal retrieval for identities unseen and unheard during training over a number of scenarios and establish a benchmark for this novel task; finally, we show an application of using the joint embedding for automatically retrieving and labelling characters in TV dramas.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dd1"},"id":"arXiv:1804.10689v1","url":"http://arxiv.org/abs/1804.10689v1","updated":"2018-04-27T21:16:40Z","published":"2018-04-27T21:16:40Z","title":"Decoupling Dynamics and Reward for Transfer Learning","summary":"Current reinforcement learning (RL) methods can successfully learn single tasks but often generalize poorly to modest perturbations in task domain or training procedure. In this work, we present a decoupled learning strategy for RL that creates a shared representation space where knowledge can be robustly transferred. We separate learning the task representation, the forward dynamics, the inverse dynamics and the reward function of the domain, and show that this decoupling improves performance within the task, transfers well to changes in dynamics and reward, and can be effectively used for online planning. Empirical results show good performance in both continuous and discrete RL domains.","author":["Amy Zhang","Harsh Satija","Joelle Pineau"],"primaryCategory":"cs.LG","category":["cs.LG","cs.AI","stat.ML"],"timestamp_ms":{"$numberLong":"1524889000000"},"text":"Current reinforcement learning (RL) methods can successfully learn single tasks but often generalize poorly to modest perturbations in task domain or training procedure. In this work, we present a decoupled learning strategy for RL that creates a shared representation space where knowledge can be robustly transferred. We separate learning the task representation, the forward dynamics, the inverse dynamics and the reward function of the domain, and show that this decoupling improves performance within the task, transfers well to changes in dynamics and reward, and can be effectively used for online planning. Empirical results show good performance in both continuous and discrete RL domains.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dd0"},"id":"arXiv:1804.10862v1","url":"http://arxiv.org/abs/1804.10862v1","updated":"2018-04-29T03:17:36Z","published":"2018-04-29T03:17:36Z","title":"Collaborative Memory Network for Recommendation Systems","summary":"Recommendation systems play a vital role to keep users engaged with personalized content in modern online platforms. Deep learning has revolutionized many research fields and there is a recent surge of interest in applying it to collaborative filtering (CF). However, existing methods compose deep learning architectures with the latent factor model ignoring a major class of CF models, neighborhood or memory-based approaches. We propose Collaborative Memory Networks (CMN), a deep architecture to unify the two classes of CF models capitalizing on the strengths of the global structure of latent factor model and local neighborhood-based structure in a nonlinear fashion. Motivated by the success of Memory Networks, we fuse a memory component and neural attention mechanism as the neighborhood component. The associative addressing scheme with the user and item memories in the memory module encodes complex user-item relations coupled with the neural attention mechanism to learn a user-item specific neighborhood. Finally, the output module jointly exploits the neighborhood with the user and item memories to produce the ranking score. Stacking multiple memory modules together yield deeper architectures capturing increasingly complex user-item relations. Furthermore, we show strong connections between CMN components, memory networks and the three classes of CF models. Comprehensive experimental results demonstrate the effectiveness of CMN on three public datasets outperforming competitive baselines. Qualitative visualization of the attention weights provide insight into the model's recommendation process and suggest the presence of higher order interactions.","author":["Travis Ebesu","Bin Shen","Yi Fang"],"primaryCategory":"cs.IR","category":["cs.IR"],"timestamp_ms":{"$numberLong":"1524997056000"},"text":"Recommendation systems play a vital role to keep users engaged with personalized content in modern online platforms. Deep learning has revolutionized many research fields and there is a recent surge of interest in applying it to collaborative filtering (CF). However, existing methods compose deep learning architectures with the latent factor model ignoring a major class of CF models, neighborhood or memory-based approaches. We propose Collaborative Memory Networks (CMN), a deep architecture to unify the two classes of CF models capitalizing on the strengths of the global structure of latent factor model and local neighborhood-based structure in a nonlinear fashion. Motivated by the success of Memory Networks, we fuse a memory component and neural attention mechanism as the neighborhood component. The associative addressing scheme with the user and item memories in the memory module encodes complex user-item relations coupled with the neural attention mechanism to learn a user-item specific neighborhood. Finally, the output module jointly exploits the neighborhood with the user and item memories to produce the ranking score. Stacking multiple memory modules together yield deeper architectures capturing increasingly complex user-item relations. Furthermore, we show strong connections between CMN components, memory networks and the three classes of CF models. Comprehensive experimental results demonstrate the effectiveness of CMN on three public datasets outperforming competitive baselines. Qualitative visualization of the attention weights provide insight into the model's recommendation process and suggest the presence of higher order interactions.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dcf"},"id":"arXiv:1804.10645v1","url":"http://arxiv.org/abs/1804.10645v1","updated":"2018-04-27T18:46:07Z","published":"2018-04-27T18:46:07Z","title":"Enforceable Data Sharing Agreements Using Smart Contracts","summary":"As more and more data is collected for various reasons, the sharing of such data becomes paramount to increasing its value. Many applications ranging from smart cities to personalized health care require individuals and organizations to share data at an unprecedented scale. Data sharing is crucial in today's world, but due to privacy reasons, security concerns and regulation issues, the conditions under which the sharing occurs needs to be carefully specified. Currently, this process is done by lawyers and requires the costly signing of legal agreements. In many cases, these data sharing agreements are hard to track, manage or enforce. In this work, we propose a novel alternative for tracking, managing and especially enforcing such data sharing agreements using smart contracts and blockchain technology. We design a framework that generates smart contracts from parameters based on legal data sharing agreements. The terms in these agreements are automatically enforced by the system. Monetary punishment can be employed using secure voting by external auditors to hold the violators accountable. Our experimental evaluation shows that our proposed framework is efficient and low-cost.","author":["Kevin Liu","Harsh Desai","Lalana Kagal","Murat Kantarcioglu"],"primaryCategory":"cs.CY","category":["cs.CY","cs.CR"],"timestamp_ms":{"$numberLong":"1524879967000"},"text":"As more and more data is collected for various reasons, the sharing of such data becomes paramount to increasing its value. Many applications ranging from smart cities to personalized health care require individuals and organizations to share data at an unprecedented scale. Data sharing is crucial in today's world, but due to privacy reasons, security concerns and regulation issues, the conditions under which the sharing occurs needs to be carefully specified. Currently, this process is done by lawyers and requires the costly signing of legal agreements. In many cases, these data sharing agreements are hard to track, manage or enforce. In this work, we propose a novel alternative for tracking, managing and especially enforcing such data sharing agreements using smart contracts and blockchain technology. We design a framework that generates smart contracts from parameters based on legal data sharing agreements. The terms in these agreements are automatically enforced by the system. Monetary punishment can be employed using secure voting by external auditors to hold the violators accountable. Our experimental evaluation shows that our proposed framework is efficient and low-cost.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dce"},"id":"arXiv:1804.10541v1","url":"http://arxiv.org/abs/1804.10541v1","updated":"2018-04-27T14:52:41Z","published":"2018-04-27T14:52:41Z","title":"A matrix-free approach to parallel and memory-efficient deformable image\n  registration","summary":"We present a novel computational approach to fast and memory-efficient deformable image registration. In the variational registration model, the computation of the objective function derivatives is the computationally most expensive operation, both in terms of runtime and memory requirements. In order to target this bottleneck, we analyze the matrix structure of gradient and Hessian computations for the case of the normalized gradient fields distance measure and curvature regularization. Based on this analysis, we derive equivalent matrix-free closed-form expressions for derivative computations, eliminating the need for storing intermediate results and the costs of sparse matrix arithmetic. This has further benefits: (1) matrix computations can be fully parallelized, (2) memory complexity for derivative computation is reduced from linear to constant, and (3) overall computation times are substantially reduced.   In comparison with an optimized matrix-based reference implementation, the CPU implementation achieves speedup factors between 3.1 and 9.7, and we are able to handle substantially higher resolutions. Using a GPU implementation, we achieve an additional speedup factor of up to 9.2.   Furthermore, we evaluated the approach on real-world medical datasets. On ten publicly available lung CT images from the DIR-Lab 4DCT dataset, we achieve the best mean landmark error of 0.93 mm compared to other submissions on the DIR-Lab website, with an average runtime of only 9.23 s. Complete non-rigid registration of full-size 3D thorax-abdomen CT volumes from oncological follow-up is achieved in 12.6 s. The experimental results show that the proposed matrix-free algorithm enables the use of variational registration models also in applications which were previously impractical due to memory or runtime restrictions.","author":["Lars König","Jan Rühaak","Alexander Derksen","Jan Lellmann"],"primaryCategory":"cs.CV","category":["cs.CV","92C55, 65K10, 65Y05"],"timestamp_ms":{"$numberLong":"1524865961000"},"text":"We present a novel computational approach to fast and memory-efficient deformable image registration. In the variational registration model, the computation of the objective function derivatives is the computationally most expensive operation, both in terms of runtime and memory requirements. In order to target this bottleneck, we analyze the matrix structure of gradient and Hessian computations for the case of the normalized gradient fields distance measure and curvature regularization. Based on this analysis, we derive equivalent matrix-free closed-form expressions for derivative computations, eliminating the need for storing intermediate results and the costs of sparse matrix arithmetic. This has further benefits: (1) matrix computations can be fully parallelized, (2) memory complexity for derivative computation is reduced from linear to constant, and (3) overall computation times are substantially reduced.   In comparison with an optimized matrix-based reference implementation, the CPU implementation achieves speedup factors between 3.1 and 9.7, and we are able to handle substantially higher resolutions. Using a GPU implementation, we achieve an additional speedup factor of up to 9.2.   Furthermore, we evaluated the approach on real-world medical datasets. On ten publicly available lung CT images from the DIR-Lab 4DCT dataset, we achieve the best mean landmark error of 0.93 mm compared to other submissions on the DIR-Lab website, with an average runtime of only 9.23 s. Complete non-rigid registration of full-size 3D thorax-abdomen CT volumes from oncological follow-up is achieved in 12.6 s. The experimental results show that the proposed matrix-free algorithm enables the use of variational registration models also in applications which were previously impractical due to memory or runtime restrictions.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dcd"},"id":"arXiv:1804.10454v1","url":"http://arxiv.org/abs/1804.10454v1","updated":"2018-04-27T11:56:04Z","published":"2018-04-27T11:56:04Z","title":"Method to assess the functional role of noisy brain signals by mining\n  envelope dynamics","summary":"Data-driven spatial filtering approaches are commonly used to assess rhythmic brain activity from multichannel recordings such as electroencephalography (EEG). As spatial filter estimation is prone to noise, non-stationarity effects and limited data, a high model variability induced by slight changes of, e.g., involved hyperparameters is generally encountered. These aspects challenge the assessment of functionally relevant features which are of special importance in closed-loop applications as, e.g., in the field of rehabilitation. We propose a data-driven method to identify groups of reliable and functionally relevant oscillatory components computed by a spatial filtering approach. Therefore, we initially embrace the variability of decoding models in a large configuration space before condensing information by density-based clustering of components' functional signatures. Exemplified for a hand force task with rich within-trial structure, the approach was evaluated on EEG data of 18 healthy subjects. We found that functional characteristics of single components are revealed by distinct temporal dynamics of their event-related power changes. Based on a within-subject analysis, our clustering revealed seven groups of homogeneous envelope dynamics on average. To support introspection by practitioners, we provide a set of metrics to characterize and validate single clusterings. We show that identified clusters contain components of strictly confined frequency ranges, dominated by the alpha and beta band. Our method is applicable to any spatial filtering algorithm. Despite high model variability, it allows capturing and monitoring relevant oscillatory features. We foresee its application in closed-loop applications such as brain-computer interface based protocols in stroke rehabilitation.","author":["Andreas Meinel","Henrich Kolkhorst","Michael Tangermann"],"primaryCategory":"eess.SP","category":["eess.SP","cs.LG","q-bio.NC","stat.AP","stat.ML"],"timestamp_ms":{"$numberLong":"1524855364000"},"text":"Data-driven spatial filtering approaches are commonly used to assess rhythmic brain activity from multichannel recordings such as electroencephalography (EEG). As spatial filter estimation is prone to noise, non-stationarity effects and limited data, a high model variability induced by slight changes of, e.g., involved hyperparameters is generally encountered. These aspects challenge the assessment of functionally relevant features which are of special importance in closed-loop applications as, e.g., in the field of rehabilitation. We propose a data-driven method to identify groups of reliable and functionally relevant oscillatory components computed by a spatial filtering approach. Therefore, we initially embrace the variability of decoding models in a large configuration space before condensing information by density-based clustering of components' functional signatures. Exemplified for a hand force task with rich within-trial structure, the approach was evaluated on EEG data of 18 healthy subjects. We found that functional characteristics of single components are revealed by distinct temporal dynamics of their event-related power changes. Based on a within-subject analysis, our clustering revealed seven groups of homogeneous envelope dynamics on average. To support introspection by practitioners, we provide a set of metrics to characterize and validate single clusterings. We show that identified clusters contain components of strictly confined frequency ranges, dominated by the alpha and beta band. Our method is applicable to any spatial filtering algorithm. Despite high model variability, it allows capturing and monitoring relevant oscillatory features. We foresee its application in closed-loop applications such as brain-computer interface based protocols in stroke rehabilitation.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dcc"},"id":"arXiv:1804.10381v1","url":"http://arxiv.org/abs/1804.10381v1","updated":"2018-04-27T08:15:42Z","published":"2018-04-27T08:15:42Z","title":"Development of Rehabilitation System (ReHabgame) through Monte-Carlo\n  Tree Search Algorithm","summary":"Computational Intelligence (CI) in computer games plays an important role that could simulate various aspects of real-life problems. CI in real-time decision-making games can provide a platform for the examination of tree search algorithms. In this paper, we present a rehabilitation serious game (ReHabgame) in which the Monte-Carlo Tree Search (MCTS) algorithm is utilized. The game is designed to combat the physical impairment of post-stroke/brain injury casualties in order to improve upper limb movement. Through the process of ReHabgame the player chooses paths via upper limb according to his/her movement ability to reach virtual goal objects. The system adjusts the difficulty level of the game based on the player's quality of activity through MCTS. It learns from the movements made by a player and generates further subsequent objects for collection. The system collects orientation, muscle and joint activity data and utilizes them to make decisions. Players data are collected through Kinect Xbox One and Myo Armband. The results show the effectiveness of the MCTS in the ReHabgame that progresses from highly achievable paths to the less achievable ones, thus configuring and personalizing the rehabilitation process.","author":["Shabnam Sadeghi Esfahlani","George Wilson"],"primaryCategory":"cs.HC","category":["cs.HC"],"timestamp_ms":{"$numberLong":"1524842142000"},"text":"Computational Intelligence (CI) in computer games plays an important role that could simulate various aspects of real-life problems. CI in real-time decision-making games can provide a platform for the examination of tree search algorithms. In this paper, we present a rehabilitation serious game (ReHabgame) in which the Monte-Carlo Tree Search (MCTS) algorithm is utilized. The game is designed to combat the physical impairment of post-stroke/brain injury casualties in order to improve upper limb movement. Through the process of ReHabgame the player chooses paths via upper limb according to his/her movement ability to reach virtual goal objects. The system adjusts the difficulty level of the game based on the player's quality of activity through MCTS. It learns from the movements made by a player and generates further subsequent objects for collection. The system collects orientation, muscle and joint activity data and utilizes them to make decisions. Players data are collected through Kinect Xbox One and Myo Armband. The results show the effectiveness of the MCTS in the ReHabgame that progresses from highly achievable paths to the less achievable ones, thus configuring and personalizing the rehabilitation process.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dcb"},"id":"arXiv:1804.09787v1","url":"http://arxiv.org/abs/1804.09787v1","updated":"2018-04-25T20:18:58Z","published":"2018-04-25T20:18:58Z","title":"Interleaved group products","summary":"Let $G$ be the special linear group $\\mathrm{SL}(2,q)$. We show that if $(a_1,\\ldots,a_t)$ and $(b_1,\\ldots,b_t)$ are sampled uniformly from large subsets $A$ and $B$ of $G^t$ then their interleaved product $a_1 b_1 a_2 b_2 \\cdots a_t b_t$ is nearly uniform over $G$. This extends a result of the first author, which corresponds to the independent case where $A$ and $B$ are product sets. We obtain a number of other results. For example, we show that if $X$ is a probability distribution on $G^m$ such that any two coordinates are uniform in $G^2$, then a pointwise product of $s$ independent copies of $X$ is nearly uniform in $G^m$, where $s$ depends on $m$ only. Extensions to other groups are also discussed.   We obtain closely related results in communication complexity, which is the setting where some of these questions were first asked by Miles and Viola. For example, suppose party $A_i$ of $k$ parties $A_1,\\dots,A_k$ receives on its forehead a $t$-tuple $(a_{i1},\\dots,a_{it})$ of elements from $G$. The parties are promised that the interleaved product $a_{11}\\dots a_{k1}a_{12}\\dots a_{k2}\\dots a_{1t}\\dots a_{kt}$ is equal either to the identity $e$ or to some other fixed element $g\\in G$, and their goal is to determine which of the two the product is equal to. We show that for all fixed $k$ and all sufficiently large $t$ the communication is $\\Omega(t \\log |G|)$, which is tight. Even for $k=2$ the previous best lower bound was $\\Omega(t)$. As an application, we establish the security of the leakage-resilient circuits studied by Miles and Viola in the \"only computation leaks\" model.","author":["W. T. Gowers","Emanuele Viola"],"primaryCategory":"math.CO","category":["math.CO","cs.CC","math.GR","68Q17, 20P05"],"timestamp_ms":{"$numberLong":"1524712738000"},"text":"Let $G$ be the special linear group $\\mathrm{SL}(2,q)$. We show that if $(a_1,\\ldots,a_t)$ and $(b_1,\\ldots,b_t)$ are sampled uniformly from large subsets $A$ and $B$ of $G^t$ then their interleaved product $a_1 b_1 a_2 b_2 \\cdots a_t b_t$ is nearly uniform over $G$. This extends a result of the first author, which corresponds to the independent case where $A$ and $B$ are product sets. We obtain a number of other results. For example, we show that if $X$ is a probability distribution on $G^m$ such that any two coordinates are uniform in $G^2$, then a pointwise product of $s$ independent copies of $X$ is nearly uniform in $G^m$, where $s$ depends on $m$ only. Extensions to other groups are also discussed.   We obtain closely related results in communication complexity, which is the setting where some of these questions were first asked by Miles and Viola. For example, suppose party $A_i$ of $k$ parties $A_1,\\dots,A_k$ receives on its forehead a $t$-tuple $(a_{i1},\\dots,a_{it})$ of elements from $G$. The parties are promised that the interleaved product $a_{11}\\dots a_{k1}a_{12}\\dots a_{k2}\\dots a_{1t}\\dots a_{kt}$ is equal either to the identity $e$ or to some other fixed element $g\\in G$, and their goal is to determine which of the two the product is equal to. We show that for all fixed $k$ and all sufficiently large $t$ the communication is $\\Omega(t \\log |G|)$, which is tight. Even for $k=2$ the previous best lower bound was $\\Omega(t)$. As an application, we establish the security of the leakage-resilient circuits studied by Miles and Viola in the \"only computation leaks\" model.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dca"},"id":"arXiv:1804.11021v1","url":"http://arxiv.org/abs/1804.11021v1","updated":"2018-04-30T02:05:52Z","published":"2018-04-30T02:05:52Z","title":"On the Effect of Suboptimal Estimation of Mutual Information in Feature\n  Selection and Classification","summary":"This paper introduces a new property of estimators of the strength of statistical association, which helps characterize how well an estimator will perform in scenarios where dependencies between continuous and discrete random variables need to be rank ordered. The new property, termed the estimator response curve, is easily computable and provides a marginal distribution agnostic way to assess an estimator's performance. It overcomes notable drawbacks of current metrics of assessment, including statistical power, bias, and consistency. We utilize the estimator response curve to test various measures of the strength of association that satisfy the data processing inequality (DPI), and show that the CIM estimator's performance compares favorably to kNN, vME, AP, and H_{MI} estimators of mutual information. The estimators which were identified to be suboptimal, according to the estimator response curve, perform worse than the more optimal estimators when tested with real-world data from four different areas of science, all with varying dimensionalities and sizes.","author":["Kiran Karra","Lamine Mili"],"primaryCategory":"stat.ML","category":["stat.ML","cs.LG"],"timestamp_ms":{"$numberLong":"1525079152000"},"text":"This paper introduces a new property of estimators of the strength of statistical association, which helps characterize how well an estimator will perform in scenarios where dependencies between continuous and discrete random variables need to be rank ordered. The new property, termed the estimator response curve, is easily computable and provides a marginal distribution agnostic way to assess an estimator's performance. It overcomes notable drawbacks of current metrics of assessment, including statistical power, bias, and consistency. We utilize the estimator response curve to test various measures of the strength of association that satisfy the data processing inequality (DPI), and show that the CIM estimator's performance compares favorably to kNN, vME, AP, and H_{MI} estimators of mutual information. The estimators which were identified to be suboptimal, according to the estimator response curve, perform worse than the more optimal estimators when tested with real-world data from four different areas of science, all with varying dimensionalities and sizes.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dc9"},"id":"arXiv:1804.10990v2","url":"http://arxiv.org/abs/1804.10990v2","updated":"2018-05-02T00:36:38Z","published":"2018-04-29T20:59:09Z","title":"On Obtaining Stable Rankings","summary":"We often have to rank items with multiple attributes in a dataset. A typical method to achieve this is to compute a goodness score for each item as a weighted sum of its attribute values, and then to rank by sorting on this score. Clearly, the ranking obtained depends on the weights used for this summation. Ideally, we would want the ranked order not to change if the weights are changed slightly. We call this property {\\em stability} of the ranking. A consumer of a ranked list may trust the ranking more if it has high stability. A producer of a ranked list prefers to choose weights that result in a stable ranking, both to earn the trust of potential consumers and because a stable ranking is intrinsically likely to be more meaningful.   In this paper, we develop a framework that can be used to assess the stability of a provided ranking and to obtain a stable ranking within an \"acceptable\" range of weight values (called \"the region of interest\"). We address the case where the user cares about the rank order of the entire set of items, and also the case where the user cares only about the top-$k$ items.   Using a geometric interpretation, we propose the algorithms for producing the stable rankings. We also propose a randomized algorithm that uses Monte-Carlo estimation. To do so, we first propose an unbiased sampler for sampling the rankings (or top-$k$ results) uniformly at random from the region of interest. In addition to the theoretical analyses, we conduct extensive experiments on real datasets that validate our proposal.","author":["Abolfazl Asudeh","H. V. Jagadish","Gerome Miklau","Julia Stoyanovich"],"primaryCategory":"cs.DB","category":["cs.DB"],"timestamp_ms":{"$numberLong":"1525246598000"},"text":"We often have to rank items with multiple attributes in a dataset. A typical method to achieve this is to compute a goodness score for each item as a weighted sum of its attribute values, and then to rank by sorting on this score. Clearly, the ranking obtained depends on the weights used for this summation. Ideally, we would want the ranked order not to change if the weights are changed slightly. We call this property {\\em stability} of the ranking. A consumer of a ranked list may trust the ranking more if it has high stability. A producer of a ranked list prefers to choose weights that result in a stable ranking, both to earn the trust of potential consumers and because a stable ranking is intrinsically likely to be more meaningful.   In this paper, we develop a framework that can be used to assess the stability of a provided ranking and to obtain a stable ranking within an \"acceptable\" range of weight values (called \"the region of interest\"). We address the case where the user cares about the rank order of the entire set of items, and also the case where the user cares only about the top-$k$ items.   Using a geometric interpretation, we propose the algorithms for producing the stable rankings. We also propose a randomized algorithm that uses Monte-Carlo estimation. To do so, we first propose an unbiased sampler for sampling the rankings (or top-$k$ results) uniformly at random from the region of interest. In addition to the theoretical analyses, we conduct extensive experiments on real datasets that validate our proposal.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dc8"},"id":"arXiv:1804.10439v1","url":"http://arxiv.org/abs/1804.10439v1","updated":"2018-04-27T11:02:21Z","published":"2018-04-27T11:02:21Z","title":"Can we use Google Scholar to identify highly-cited documents?","summary":"The main objective of this paper is to empirically test whether the identification of highly-cited documents through Google Scholar is feasible and reliable. To this end, we carried out a longitudinal analysis (1950 to 2013), running a generic query (filtered only by year of publication) to minimise the effects of academic search engine optimisation. This gave us a final sample of 64,000 documents (1,000 per year). The strong correlation between a document's citations and its position in the search results (r= -0.67) led us to conclude that Google Scholar is able to identify highly-cited papers effectively. This, combined with Google Scholar's unique coverage (no restrictions on document type and source), makes the academic search engine an invaluable tool for bibliometric research relating to the identification of the most influential scientific documents. We find evidence, however, that Google Scholar ranks those documents whose language (or geographical web domain) matches with the user's interface language higher than could be expected based on citations. Nonetheless, this language effect and other factors related to the Google Scholar's operation, i.e. the proper identification of versions and the date of publication, only have an incidental impact. They do not compromise the ability of Google Scholar to identify the highly-cited papers.","author":["Alberto Martín-Martín","Enrique Orduna-Malea","Anne-Wil Harzing","Emilio Delgado López-Cózar"],"primaryCategory":"cs.DL","category":["cs.DL"],"timestamp_ms":{"$numberLong":"1524852141000"},"text":"The main objective of this paper is to empirically test whether the identification of highly-cited documents through Google Scholar is feasible and reliable. To this end, we carried out a longitudinal analysis (1950 to 2013), running a generic query (filtered only by year of publication) to minimise the effects of academic search engine optimisation. This gave us a final sample of 64,000 documents (1,000 per year). The strong correlation between a document's citations and its position in the search results (r= -0.67) led us to conclude that Google Scholar is able to identify highly-cited papers effectively. This, combined with Google Scholar's unique coverage (no restrictions on document type and source), makes the academic search engine an invaluable tool for bibliometric research relating to the identification of the most influential scientific documents. We find evidence, however, that Google Scholar ranks those documents whose language (or geographical web domain) matches with the user's interface language higher than could be expected based on citations. Nonetheless, this language effect and other factors related to the Google Scholar's operation, i.e. the proper identification of versions and the date of publication, only have an incidental impact. They do not compromise the ability of Google Scholar to identify the highly-cited papers.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dc7"},"id":"arXiv:1805.00889v1","url":"http://arxiv.org/abs/1805.00889v1","updated":"2018-05-02T16:07:39Z","published":"2018-05-02T16:07:39Z","title":"SONYC: A System for the Monitoring, Analysis and Mitigation of Urban\n  Noise Pollution","summary":"We present the Sounds of New York City (SONYC) project, a smart cities initiative focused on developing a cyber-physical system for the monitoring, analysis and mitigation of urban noise pollution. Noise pollution is one of the topmost quality of life issues for urban residents in the U.S. with proven effects on health, education, the economy, and the environment. Yet, most cities lack the resources to continuously monitor noise and understand the contribution of individual sources, the tools to analyze patterns of noise pollution at city-scale, and the means to empower city agencies to take effective, data-driven action for noise mitigation. The SONYC project advances novel technological and socio-technical solutions that help address these needs.   SONYC includes a distributed network of both sensors and people for large-scale noise monitoring. The sensors use low-cost, low-power technology, and cutting-edge machine listening techniques, to produce calibrated acoustic measurements and recognize individual sound sources in real time. Citizen science methods are used to help urban residents connect to city agencies and each other, understand their noise footprint, and facilitate reporting and self-regulation. Crucially, SONYC utilizes big data solutions to analyze, retrieve and visualize information from sensors and citizens, creating a comprehensive acoustic model of the city that can be used to identify significant patterns of noise pollution. These data can be used to drive the strategic application of noise code enforcement by city agencies to optimize the reduction of noise pollution. The entire system, integrating cyber, physical and social infrastructure, forms a closed loop of continuous sensing, analysis and actuation on the environment.   SONYC provides a blueprint for the mitigation of noise pollution that can potentially be applied to other cities in the US and abroad.","author":["Juan Pablo Bello","Claudio Silva","Oded Nov","R. Luke DuBois","Anish Arora","Justin Salamon","Charles Mydlarz","Harish Doraiswamy"],"primaryCategory":"cs.SD","category":["cs.SD","cs.CY","cs.HC","eess.AS"],"timestamp_ms":{"$numberLong":"1525302459000"},"text":"We present the Sounds of New York City (SONYC) project, a smart cities initiative focused on developing a cyber-physical system for the monitoring, analysis and mitigation of urban noise pollution. Noise pollution is one of the topmost quality of life issues for urban residents in the U.S. with proven effects on health, education, the economy, and the environment. Yet, most cities lack the resources to continuously monitor noise and understand the contribution of individual sources, the tools to analyze patterns of noise pollution at city-scale, and the means to empower city agencies to take effective, data-driven action for noise mitigation. The SONYC project advances novel technological and socio-technical solutions that help address these needs.   SONYC includes a distributed network of both sensors and people for large-scale noise monitoring. The sensors use low-cost, low-power technology, and cutting-edge machine listening techniques, to produce calibrated acoustic measurements and recognize individual sound sources in real time. Citizen science methods are used to help urban residents connect to city agencies and each other, understand their noise footprint, and facilitate reporting and self-regulation. Crucially, SONYC utilizes big data solutions to analyze, retrieve and visualize information from sensors and citizens, creating a comprehensive acoustic model of the city that can be used to identify significant patterns of noise pollution. These data can be used to drive the strategic application of noise code enforcement by city agencies to optimize the reduction of noise pollution. The entire system, integrating cyber, physical and social infrastructure, forms a closed loop of continuous sensing, analysis and actuation on the environment.   SONYC provides a blueprint for the mitigation of noise pollution that can potentially be applied to other cities in the US and abroad.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dc6"},"id":"arXiv:1804.10811v1","url":"http://arxiv.org/abs/1804.10811v1","updated":"2018-04-28T14:38:18Z","published":"2018-04-28T14:38:18Z","title":"Platform-as-a-Service (PaaS): The Next Hype of Cloud Computing","summary":"Cloud Computing is expected to become the driving force of information technology to revolutionize the future. Presently number of companies is trying to adopt this new technology either as service providers, enablers or vendors. In this way the cloud market is estimated be likely to emerge at a remarkable rate. Under the whole cloud umbrella, PaaS seems to have a relatively small market share. However, it is expected to offer much more as it is compared with its counterparts SaaS and IaaS. This paper is aimed to assess and analyze the future of PaaS technology. Year 2018 named as 'the year of PaaS'. It means that PaaS technology has established strong roots and ready to hit the market with better technology services. This research will discuss future PaaS market trends, growth and business competitors. In the current dynamic era, several companies in the market are offering PaaS services. This research will also outline some of the top service providers (proprietary \u0026 open source) to discuss their current technology status and present a futuristic look into their services and business strategies. Analysis of the present and future PaaS technology infrastructure will also be a major discussion in this paper.","author":["Robail Yasrab"],"primaryCategory":"cs.CY","category":["cs.CY","68Nxx, 68M01, 68M12, 68M14,","C.2.4; C.2.2; C.2.3; C.1.2; C.2.2; C.1.2; C.2.2; C.1.4; C.2.4;\n  C.3.3; C.3.2; C.3.2; C.3.3"],"timestamp_ms":{"$numberLong":"1524951498000"},"text":"Cloud Computing is expected to become the driving force of information technology to revolutionize the future. Presently number of companies is trying to adopt this new technology either as service providers, enablers or vendors. In this way the cloud market is estimated be likely to emerge at a remarkable rate. Under the whole cloud umbrella, PaaS seems to have a relatively small market share. However, it is expected to offer much more as it is compared with its counterparts SaaS and IaaS. This paper is aimed to assess and analyze the future of PaaS technology. Year 2018 named as 'the year of PaaS'. It means that PaaS technology has established strong roots and ready to hit the market with better technology services. This research will discuss future PaaS market trends, growth and business competitors. In the current dynamic era, several companies in the market are offering PaaS services. This research will also outline some of the top service providers (proprietary \u0026 open source) to discuss their current technology status and present a futuristic look into their services and business strategies. Analysis of the present and future PaaS technology infrastructure will also be a major discussion in this paper.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dc5"},"id":"arXiv:1805.00707v1","url":"http://arxiv.org/abs/1805.00707v1","updated":"2018-05-02T10:04:53Z","published":"2018-05-02T10:04:53Z","title":"Energy-Efficient Wireless Powered Secure Transmission with Cooperative\n  Jamming for Public Transportation","summary":"In this paper, wireless power transfer and cooperative jamming (CJ) are combined to enhance physical security in public transportation networks. First, a new secure system model with both fixed and mobile jammers is proposed to guarantee secrecy in the worst-case scenario. All jammers are endowed with energy harvesting (EH) capability. Following this, two CJ based schemes, namely B-CJ-SRM and B-CJ-TPM, are proposed, where SRM and TPM are short for secrecy rate maximization and transmit power minimization, respectively. They respectively maximize the secrecy rate (SR) with transmit power constraint and minimize the transmit power of the BS with SR constraint, by optimizing beamforming vector and artificial noise covariance matrix. To further reduce the complexity of our proposed optimal schemes, their low-complexity (LC) versions, called LC-B-CJ-SRM and LC-B-CJ-TPM are developed. Simulation results show that our proposed schemes, B-CJ-SRM and B-CJ-TPM, achieve significant SR performance improvement over existing zero-forcing and QoSD methods. Additionally, the SR performance of the proposed LC schemes are close to those of their original versions.","author":["Linqing Gui","Feifei Bao","Xiaobo Zhou","Chunhua Yu","Feng Shu","Jun Li"],"primaryCategory":"cs.CR","category":["cs.CR","eess.SP"],"timestamp_ms":{"$numberLong":"1525280693000"},"text":"In this paper, wireless power transfer and cooperative jamming (CJ) are combined to enhance physical security in public transportation networks. First, a new secure system model with both fixed and mobile jammers is proposed to guarantee secrecy in the worst-case scenario. All jammers are endowed with energy harvesting (EH) capability. Following this, two CJ based schemes, namely B-CJ-SRM and B-CJ-TPM, are proposed, where SRM and TPM are short for secrecy rate maximization and transmit power minimization, respectively. They respectively maximize the secrecy rate (SR) with transmit power constraint and minimize the transmit power of the BS with SR constraint, by optimizing beamforming vector and artificial noise covariance matrix. To further reduce the complexity of our proposed optimal schemes, their low-complexity (LC) versions, called LC-B-CJ-SRM and LC-B-CJ-TPM are developed. Simulation results show that our proposed schemes, B-CJ-SRM and B-CJ-TPM, achieve significant SR performance improvement over existing zero-forcing and QoSD methods. Additionally, the SR performance of the proposed LC schemes are close to those of their original versions.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dc4"},"id":"arXiv:1804.11312v2","url":"http://arxiv.org/abs/1804.11312v2","updated":"2018-05-01T14:52:13Z","published":"2018-04-30T16:38:38Z","title":"Performance Evaluation of an Algorithm-based Asynchronous\n  Checkpoint-Restart Fault Tolerant Application Using Mixed MPI/GPI-2","summary":"One of the hardest challenges of the current Big Data landscape is the lack of ability to process huge volumes of information in an acceptable time. The goal of this work, is to ascertain if it is useful to use typical Big Data tools to solve High Performance Computing problems, by exploring and comparing a distributed computing framework implemented on a commodity cluster architecture: the experiment will depend on the computational time required using tools such as Apache Spark. This will be compared to \"equivalent more traditional\" approaches such as using a distributed memory model with MPI on a distributed file system such as HDFS (Hadoop Distributed File System) and native C libraries that create an interface to encapsulate this file system functionalities, and using the GPI-2 implementation for the GASPI protocol and it's in-memory checkpointing library to provide an application with Fault Tolerance features. To be more precise, we've chosen the K-means algorithm as experiment, that will be ran on variable size datasets, and then we will compare the computational run time and time resilience of both approaches.","author":["Adrian Bazaga","Michal Pitonak"],"primaryCategory":"cs.DC","category":["cs.DC"],"timestamp_ms":{"$numberLong":"1525211533000"},"text":"One of the hardest challenges of the current Big Data landscape is the lack of ability to process huge volumes of information in an acceptable time. The goal of this work, is to ascertain if it is useful to use typical Big Data tools to solve High Performance Computing problems, by exploring and comparing a distributed computing framework implemented on a commodity cluster architecture: the experiment will depend on the computational time required using tools such as Apache Spark. This will be compared to \"equivalent more traditional\" approaches such as using a distributed memory model with MPI on a distributed file system such as HDFS (Hadoop Distributed File System) and native C libraries that create an interface to encapsulate this file system functionalities, and using the GPI-2 implementation for the GASPI protocol and it's in-memory checkpointing library to provide an application with Fault Tolerance features. To be more precise, we've chosen the K-means algorithm as experiment, that will be ran on variable size datasets, and then we will compare the computational run time and time resilience of both approaches.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dc3"},"id":"arXiv:1805.00385v1","url":"http://arxiv.org/abs/1805.00385v1","updated":"2018-05-01T15:08:30Z","published":"2018-05-01T15:08:30Z","title":"Boosting Self-Supervised Learning via Knowledge Transfer","summary":"In self-supervised learning, one trains a model to solve a so-called pretext task on a dataset without the need for human annotation. The main objective, however, is to transfer this model to a target domain and task. Currently, the most effective transfer strategy is fine-tuning, which restricts one to use the same model or parts thereof for both pretext and target tasks. In this paper, we present a novel framework for self-supervised learning that overcomes limitations in designing and comparing different tasks, models, and data domains. In particular, our framework decouples the structure of the self-supervised model from the final task-specific fine-tuned model. This allows us to: 1) quantitatively assess previously incompatible models including handcrafted features; 2) show that deeper neural network models can learn better representations from the same pretext task; 3) transfer knowledge learned with a deep model to a shallower one and thus boost its learning. We use this framework to design a novel self-supervised task, which achieves state-of-the-art performance on the common benchmarks in PASCAL VOC 2007, ILSVRC12 and Places by a significant margin. Our learned features shrink the mAP gap between models trained via self-supervised learning and supervised learning from 5.9% to 2.6% in object detection on PASCAL VOC 2007.","author":["Mehdi Noroozi","Ananth Vinjimoor","Paolo Favaro","Hamed Pirsiavash"],"primaryCategory":"cs.CV","category":["cs.CV"],"timestamp_ms":{"$numberLong":"1525212510000"},"text":"In self-supervised learning, one trains a model to solve a so-called pretext task on a dataset without the need for human annotation. The main objective, however, is to transfer this model to a target domain and task. Currently, the most effective transfer strategy is fine-tuning, which restricts one to use the same model or parts thereof for both pretext and target tasks. In this paper, we present a novel framework for self-supervised learning that overcomes limitations in designing and comparing different tasks, models, and data domains. In particular, our framework decouples the structure of the self-supervised model from the final task-specific fine-tuned model. This allows us to: 1) quantitatively assess previously incompatible models including handcrafted features; 2) show that deeper neural network models can learn better representations from the same pretext task; 3) transfer knowledge learned with a deep model to a shallower one and thus boost its learning. We use this framework to design a novel self-supervised task, which achieves state-of-the-art performance on the common benchmarks in PASCAL VOC 2007, ILSVRC12 and Places by a significant margin. Our learned features shrink the mAP gap between models trained via self-supervised learning and supervised learning from 5.9% to 2.6% in object detection on PASCAL VOC 2007.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dc2"},"id":"arXiv:1804.10246v1","url":"http://arxiv.org/abs/1804.10246v1","updated":"2018-04-26T19:05:23Z","published":"2018-04-26T19:05:23Z","title":"Application of Lowner-John's Ellipsoid in the Steganography of Lattice\n  Vectors and a Review of The Gentry's FHE","summary":"In this paper, first, we utilize the Lowner-John's ellipsoid of a convex set to hide the lattice data information. We also describe the algorithm of information recovery in polynomial time by employing the Todd-Khachyian algorithm. The importance of lattice data is generally due to their applications in the homomorphic encryption schemes. For this reason we also outline the general scheme of a homomorphic encryption provided by Gentry.","author":["Hossein Mohades","Mohamad Kadkhoda","Mohamad Mahdi Mohades"],"primaryCategory":"cs.CR","category":["cs.CR"],"timestamp_ms":{"$numberLong":"1524794723000"},"text":"In this paper, first, we utilize the Lowner-John's ellipsoid of a convex set to hide the lattice data information. We also describe the algorithm of information recovery in polynomial time by employing the Todd-Khachyian algorithm. The importance of lattice data is generally due to their applications in the homomorphic encryption schemes. For this reason we also outline the general scheme of a homomorphic encryption provided by Gentry.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dc1"},"id":"arXiv:1805.00710v1","url":"http://arxiv.org/abs/1805.00710v1","updated":"2018-05-02T10:23:39Z","published":"2018-05-02T10:23:39Z","title":"Differential passivity like properties for a class of nonlinear systems","summary":"In this paper, we derive new passive maps akin to incremental passive maps, for a class of nonlinear systems using dynamic feedback and Krasovskii's method. Further using the passive maps we present a control methodology for stabilization to a desired operating point. This work is illustrated by designing a controller for a nonlinear building heating ventilating and air conditioning (HVAC) subsystem.","author":["Krishna Chaitanya Kosaraju","Venkatesh Chinde","Ramkrishna Pasumarthy","N M Singh"],"primaryCategory":"cs.SY","category":["cs.SY"],"timestamp_ms":{"$numberLong":"1525281819000"},"text":"In this paper, we derive new passive maps akin to incremental passive maps, for a class of nonlinear systems using dynamic feedback and Krasovskii's method. Further using the passive maps we present a control methodology for stabilization to a desired operating point. This work is illustrated by designing a controller for a nonlinear building heating ventilating and air conditioning (HVAC) subsystem.","relevant":0},{"_id":{"$oid":"5aea850b5a59986e57287dc0"},"id":"arXiv:1805.00787v1","url":"http://arxiv.org/abs/1805.00787v1","updated":"2018-04-09T04:12:08Z","published":"2018-04-09T04:12:08Z","title":"Cognition in Dynamical Systems, Second Edition","summary":"Cognition is the process of knowing. As carried out by a dynamical system, it is the process by which the system absorbs information into its state. A complex network of agents cognizes knowledge about its environment, internal dynamics and initial state by forming emergent, macro-level patterns. Such patterns require each agent to find its place while partially aware of the whole pattern. Such partial awareness can be achieved by separating the system dynamics into two parts by timescale: the propagation dynamics and the pattern dynamics. The fast propagation dynamics describe the spread of signals across the network. If they converge to a fixed point for any quasi-static state of the slow pattern dynamics, that fixed point represents an aggregate of macro-level information. On longer timescales, agents coordinate via positive feedback to form patterns, which are defined using closed walks in the graph of agents. Patterns can be coherent, in that every part of the pattern depends on every other part for context. Coherent patterns are acausal, in that (a) they cannot be predicted and (b) no part of the stored knowledge can be mapped to any part of the pattern, or vice versa. A cognitive network's knowledge is encoded or embodied by the selection of patterns which emerge. The theory of cognition summarized here can model autocatalytic reaction-diffusion systems, artificial neural networks, market economies and ant colony optimization, among many other real and virtual systems. This theory suggests a new understanding of complexity as a lattice of contexts rather than a single measure.","author":["Jack Hall"],"primaryCategory":"cs.MA","category":["cs.MA","cs.AI","cs.GT","cs.LG"],"timestamp_ms":{"$numberLong":"1523272328000"},"text":"Cognition is the process of knowing. As carried out by a dynamical system, it is the process by which the system absorbs information into its state. A complex network of agents cognizes knowledge about its environment, internal dynamics and initial state by forming emergent, macro-level patterns. Such patterns require each agent to find its place while partially aware of the whole pattern. Such partial awareness can be achieved by separating the system dynamics into two parts by timescale: the propagation dynamics and the pattern dynamics. The fast propagation dynamics describe the spread of signals across the network. If they converge to a fixed point for any quasi-static state of the slow pattern dynamics, that fixed point represents an aggregate of macro-level information. On longer timescales, agents coordinate via positive feedback to form patterns, which are defined using closed walks in the graph of agents. Patterns can be coherent, in that every part of the pattern depends on every other part for context. Coherent patterns are acausal, in that (a) they cannot be predicted and (b) no part of the stored knowledge can be mapped to any part of the pattern, or vice versa. A cognitive network's knowledge is encoded or embodied by the selection of patterns which emerge. The theory of cognition summarized here can model autocatalytic reaction-diffusion systems, artificial neural networks, market economies and ant colony optimization, among many other real and virtual systems. This theory suggests a new understanding of complexity as a lattice of contexts rather than a single measure.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287dbf"},"id":"arXiv:1805.00873v1","url":"http://arxiv.org/abs/1805.00873v1","updated":"2018-04-27T12:44:04Z","published":"2018-04-27T12:44:04Z","title":"A Hybrid Q-Learning Sine-Cosine-based Strategy for Addressing the\n  Combinatorial Test Suite Minimization Problem","summary":"The sine-cosine algorithm (SCA) is a new population-based meta-heuristic algorithm. In addition to exploiting sine and cosine functions to perform local and global searches (hence the name sine-cosine), the SCA introduces several random and adaptive parameters to facilitate the search process. Although it shows promising results, the search process of the SCA is vulnerable to local minima/maxima due to the adoption of a fixed switch probability and the bounded magnitude of the sine and cosine functions (from -1 to 1). In this paper, we propose a new hybrid Q-learning sine-cosine- based strategy, called the Q-learning sine-cosine algorithm (QLSCA). Within the QLSCA, we eliminate the switching probability. Instead, we rely on the Q-learning algorithm (based on the penalty and reward mechanism) to dynamically identify the best operation during runtime. Additionally, we integrate two new operations (L\\'evy flight motion and crossover) into the QLSCA to facilitate jumping out of local minima/maxima and enhance the solution diversity. To assess its performance, we adopt the QLSCA for the combinatorial test suite minimization problem. Experimental results reveal that the QLSCA is statistically superior with regard to test suite size reduction compared to recent state-of-the-art strategies, including the original SCA, the particle swarm test generator (PSTG), adaptive particle swarm optimization (APSO) and the cuckoo search strategy (CS) at the 95% confidence level. However, concerning the comparison with discrete particle swarm optimization (DPSO), there is no significant difference in performance at the 95% confidence level. On a positive note, the QLSCA statistically outperforms the DPSO in certain configurations at the 90% confidence level.","author":["Kamal Z. Zamli","Fakhrud Din","Bestoun S. Ahmed","Miroslav Bures"],"primaryCategory":"cs.AI","category":["cs.AI","cs.SE"],"timestamp_ms":{"$numberLong":"1524858244000"},"text":"The sine-cosine algorithm (SCA) is a new population-based meta-heuristic algorithm. In addition to exploiting sine and cosine functions to perform local and global searches (hence the name sine-cosine), the SCA introduces several random and adaptive parameters to facilitate the search process. Although it shows promising results, the search process of the SCA is vulnerable to local minima/maxima due to the adoption of a fixed switch probability and the bounded magnitude of the sine and cosine functions (from -1 to 1). In this paper, we propose a new hybrid Q-learning sine-cosine- based strategy, called the Q-learning sine-cosine algorithm (QLSCA). Within the QLSCA, we eliminate the switching probability. Instead, we rely on the Q-learning algorithm (based on the penalty and reward mechanism) to dynamically identify the best operation during runtime. Additionally, we integrate two new operations (L\\'evy flight motion and crossover) into the QLSCA to facilitate jumping out of local minima/maxima and enhance the solution diversity. To assess its performance, we adopt the QLSCA for the combinatorial test suite minimization problem. Experimental results reveal that the QLSCA is statistically superior with regard to test suite size reduction compared to recent state-of-the-art strategies, including the original SCA, the particle swarm test generator (PSTG), adaptive particle swarm optimization (APSO) and the cuckoo search strategy (CS) at the 95% confidence level. However, concerning the comparison with discrete particle swarm optimization (DPSO), there is no significant difference in performance at the 95% confidence level. On a positive note, the QLSCA statistically outperforms the DPSO in certain configurations at the 90% confidence level.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287dbe"},"id":"arXiv:1804.09997v1","url":"http://arxiv.org/abs/1804.09997v1","updated":"2018-04-26T11:37:03Z","published":"2018-04-26T11:37:03Z","title":"PANDA: Facilitating Usable AI Development","summary":"Recent advances in artificial intelligence (AI) and machine learning have created a general perception that AI could be used to solve complex problems, and in some situations over-hyped as a tool that can be so easily used. Unfortunately, the barrier to realization of mass adoption of AI on various business domains is too high because most domain experts have no background in AI. Developing AI applications involves multiple phases, namely data preparation, application modeling, and product deployment. The effort of AI research has been spent mostly on new AI models (in the model training stage) to improve the performance of benchmark tasks such as image recognition. Many other factors such as usability, efficiency and security of AI have not been well addressed, and therefore form a barrier to democratizing AI. Further, for many real world applications such as healthcare and autonomous driving, learning via huge amounts of possibility exploration is not feasible since humans are involved. In many complex applications such as healthcare, subject matter experts (e.g. Clinicians) are the ones who appreciate the importance of features that affect health, and their knowledge together with existing knowledge bases are critical to the end results. In this paper, we take a new perspective on developing AI solutions, and present a solution for making AI usable. We hope that this resolution will enable all subject matter experts (eg. Clinicians) to exploit AI like data scientists.","author":["Jinyang Gao","Wei Wang","Meihui Zhang","Gang Chen","H. V. Jagadish","Guoliang Li","Teck Khim Ng","Beng Chin Ooi","Sheng Wang","Jingren Zhou"],"primaryCategory":"cs.AI","category":["cs.AI","cs.DB"],"timestamp_ms":{"$numberLong":"1524767823000"},"text":"Recent advances in artificial intelligence (AI) and machine learning have created a general perception that AI could be used to solve complex problems, and in some situations over-hyped as a tool that can be so easily used. Unfortunately, the barrier to realization of mass adoption of AI on various business domains is too high because most domain experts have no background in AI. Developing AI applications involves multiple phases, namely data preparation, application modeling, and product deployment. The effort of AI research has been spent mostly on new AI models (in the model training stage) to improve the performance of benchmark tasks such as image recognition. Many other factors such as usability, efficiency and security of AI have not been well addressed, and therefore form a barrier to democratizing AI. Further, for many real world applications such as healthcare and autonomous driving, learning via huge amounts of possibility exploration is not feasible since humans are involved. In many complex applications such as healthcare, subject matter experts (e.g. Clinicians) are the ones who appreciate the importance of features that affect health, and their knowledge together with existing knowledge bases are critical to the end results. In this paper, we take a new perspective on developing AI solutions, and present a solution for making AI usable. We hope that this resolution will enable all subject matter experts (eg. Clinicians) to exploit AI like data scientists.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287dbd"},"id":"arXiv:1804.09841v2","url":"http://arxiv.org/abs/1804.09841v2","updated":"2018-04-27T06:37:48Z","published":"2018-04-26T00:27:50Z","title":"Location-Aware Pilot Allocation in Multi-Cell Multi-User Massive MIMO\n  Networks","summary":"We propose a location-aware pilot allocation algorithm for a massive multiple-input multiple-output (MIMO) network with high-mobility users, where the wireless channels are subject to Rician fading. Pilot allocation in massive MIMO is a hard combinatorial problem and depends on the locations of users. As such, it is highly complex to achieve the optimal pilot allocation in real-time for a network with high-mobility users. Against this background, we propose a low-complexity pilot allocation algorithm, which exploits the behavior of line-of-sight (LOS) interference among the users and allocate the same pilot sequence to the users with small LOS interference. Our examination demonstrates that our proposed algorithm significantly outperforms the existing algorithms, even with localization errors. Specifically, for the system considered in this work, our proposed algorithm provides up to 37.26% improvement in sum spectral efficiency (SE) and improves the sum SE of the worst interference-affected users by up to 2.57 bits/sec/Hz, as compared to the existing algorithms.","author":["Noman Akbar","Shihao Yan","Nan Yang","Jinhong Yuan"],"primaryCategory":"cs.IT","category":["cs.IT","math.IT"],"timestamp_ms":{"$numberLong":"1524836268000"},"text":"We propose a location-aware pilot allocation algorithm for a massive multiple-input multiple-output (MIMO) network with high-mobility users, where the wireless channels are subject to Rician fading. Pilot allocation in massive MIMO is a hard combinatorial problem and depends on the locations of users. As such, it is highly complex to achieve the optimal pilot allocation in real-time for a network with high-mobility users. Against this background, we propose a low-complexity pilot allocation algorithm, which exploits the behavior of line-of-sight (LOS) interference among the users and allocate the same pilot sequence to the users with small LOS interference. Our examination demonstrates that our proposed algorithm significantly outperforms the existing algorithms, even with localization errors. Specifically, for the system considered in this work, our proposed algorithm provides up to 37.26% improvement in sum spectral efficiency (SE) and improves the sum SE of the worst interference-affected users by up to 2.57 bits/sec/Hz, as compared to the existing algorithms.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287dbc"},"id":"arXiv:1804.10827v1","url":"http://arxiv.org/abs/1804.10827v1","updated":"2018-04-28T16:17:15Z","published":"2018-04-28T16:17:15Z","title":"Clustering Perturbation Resilient Instances","summary":"Euclidean $k$-means is a problem that is NP-hard in the worst-case but often solved efficiently by simple heuristics in practice. This has lead researchers to study various properties of real-world data sets that allow stable optimal clusters and provably efficient, simple algorithms to recover them. We consider stable instances of Euclidean $k$-means that have provable polynomial time algorithms for recovering optimal cluster. These results often have assumptions about the data that either do not hold in practice or the algorithms are not practical or stable enough with running time quadratic or more in the number of points. We propose simple algorithms with running time linear in the number of points and the dimension that provably recover the optimal clustering on $\\alpha$-metric perturbation resilient instances of Euclidean $k$-means. Our results hold even when the instances satisfy $\\alpha$-center proximity, a weaker property that is implied by $\\alpha$-metric perturbation resilience. In the case when the data contains a certain class of outliers (and only the inliers satisfy $\\alpha$-center proximity property), we give an algorithm that outputs a small list of clusterings, one of which is guaranteed to recover the optimal clustering.","author":["Amit Deshpande","Anand Louis","Apoorv Vikram Singh"],"primaryCategory":"cs.DS","category":["cs.DS","cs.LG"],"timestamp_ms":{"$numberLong":"1524957435000"},"text":"Euclidean $k$-means is a problem that is NP-hard in the worst-case but often solved efficiently by simple heuristics in practice. This has lead researchers to study various properties of real-world data sets that allow stable optimal clusters and provably efficient, simple algorithms to recover them. We consider stable instances of Euclidean $k$-means that have provable polynomial time algorithms for recovering optimal cluster. These results often have assumptions about the data that either do not hold in practice or the algorithms are not practical or stable enough with running time quadratic or more in the number of points. We propose simple algorithms with running time linear in the number of points and the dimension that provably recover the optimal clustering on $\\alpha$-metric perturbation resilient instances of Euclidean $k$-means. Our results hold even when the instances satisfy $\\alpha$-center proximity, a weaker property that is implied by $\\alpha$-metric perturbation resilience. In the case when the data contains a certain class of outliers (and only the inliers satisfy $\\alpha$-center proximity property), we give an algorithm that outputs a small list of clusterings, one of which is guaranteed to recover the optimal clustering.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287dbb"},"id":"arXiv:1804.11128v1","url":"http://arxiv.org/abs/1804.11128v1","updated":"2018-04-30T11:29:33Z","published":"2018-04-30T11:29:33Z","title":"Generalizing the Hypergraph Laplacian via a Diffusion Process with\n  Mediators","summary":"In a recent breakthrough STOC~2015 paper, a continuous diffusion process was considered on hypergraphs (which has been refined in a recent JACM 2018 paper) to define a Laplacian operator, whose spectral properties satisfy the celebrated Cheeger's inequality. However, one peculiar aspect of this diffusion process is that each hyperedge directs flow only from vertices with the maximum density to those with the minimum density, while ignoring vertices having strict in-beween densities. In this work, we consider a generalized diffusion process, in which vertices in a hyperedge can act as mediators to receive flow from vertices with maximum density and deliver flow to those with minimum density. We show that the resulting Laplacian operator still has a second eigenvalue satsifying the Cheeger's inequality. Our generalized diffusion model shows that there is a family of operators whose spectral properties are related to hypergraph conductance, and provides a powerful tool to enhance the development of spectral hypergraph theory. Moreover, since every vertex can participate in the new diffusion model at every instant, this can potentially have wider practical applications.","author":["T-H. Hubert Chan","Zhibin Liang"],"primaryCategory":"cs.DM","category":["cs.DM"],"timestamp_ms":{"$numberLong":"1525112973000"},"text":"In a recent breakthrough STOC~2015 paper, a continuous diffusion process was considered on hypergraphs (which has been refined in a recent JACM 2018 paper) to define a Laplacian operator, whose spectral properties satisfy the celebrated Cheeger's inequality. However, one peculiar aspect of this diffusion process is that each hyperedge directs flow only from vertices with the maximum density to those with the minimum density, while ignoring vertices having strict in-beween densities. In this work, we consider a generalized diffusion process, in which vertices in a hyperedge can act as mediators to receive flow from vertices with maximum density and deliver flow to those with minimum density. We show that the resulting Laplacian operator still has a second eigenvalue satsifying the Cheeger's inequality. Our generalized diffusion model shows that there is a family of operators whose spectral properties are related to hypergraph conductance, and provides a powerful tool to enhance the development of spectral hypergraph theory. Moreover, since every vertex can participate in the new diffusion model at every instant, this can potentially have wider practical applications.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287dba"},"id":"arXiv:1804.10849v1","url":"http://arxiv.org/abs/1804.10849v1","updated":"2018-04-28T21:28:22Z","published":"2018-04-28T21:28:22Z","title":"Ray-of-Arrival Passing for Indirect Beam Training in Cooperative\n  Millimeter Wave MIMO Networks","summary":"This paper is concerned with the channel estimation problem in multi-cell Millimeter (mmWave) wireless systems. We develop a novel Ray-of-Arrival Passing for Indirect (RAPID) Beam Training framework, in which a network consisting of multiple BS are able to work cooperatively to estimate jointly the UE channels. To achieve this aim, we consider the spatial geometry of the mmWave environment and transform conventional angular domain beamforming concepts into the more euclidean, Ray-based domain. Leveraging this model, we then consider the conditional probabilities that pilot signals are received in each direction, given that the deployment of each BS is known to the network. Simulation results show that RAPID is able to improve the average estimation of the network and significantly increase the rate of poorer quality links. Furthermore, we also show that, when a coverage rate threshold is considered, RAPID is able to improve greatly the probability that multiple link options will be available to a user at any given time.","author":["Matthew Kokshoorn","He Chen","Yonghui Li","Branka Vucetic"],"primaryCategory":"cs.IT","category":["cs.IT","math.IT"],"timestamp_ms":{"$numberLong":"1524976102000"},"text":"This paper is concerned with the channel estimation problem in multi-cell Millimeter (mmWave) wireless systems. We develop a novel Ray-of-Arrival Passing for Indirect (RAPID) Beam Training framework, in which a network consisting of multiple BS are able to work cooperatively to estimate jointly the UE channels. To achieve this aim, we consider the spatial geometry of the mmWave environment and transform conventional angular domain beamforming concepts into the more euclidean, Ray-based domain. Leveraging this model, we then consider the conditional probabilities that pilot signals are received in each direction, given that the deployment of each BS is known to the network. Simulation results show that RAPID is able to improve the average estimation of the network and significantly increase the rate of poorer quality links. Furthermore, we also show that, when a coverage rate threshold is considered, RAPID is able to improve greatly the probability that multiple link options will be available to a user at any given time.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287db9"},"id":"arXiv:1804.11294v1","url":"http://arxiv.org/abs/1804.11294v1","updated":"2018-04-30T16:15:36Z","published":"2018-04-30T16:15:36Z","title":"Stack-U-Net: Refinement Network for Image Segmentation on the Example of\n  Optic Disc and Cup","summary":"In this work, we propose a special cascade network for image segmentation, which is based on the U-Net networks as building blocks and the idea of the iterative refinement. The model was mainly applied to achieve higher recognition quality for the task of finding borders of the optic disc and cup, which are relevant to the presence of glaucoma. Compared to a single U-Net and the state-of-the-art methods for the investigated tasks, very high segmentation quality has been achieved without a need for increasing the volume of datasets. Our experiments include comparison with the best-known methods on publicly available databases DRIONS-DB, RIM-ONE v.3, DRISHTI-GS, and evaluation on a private data set collected in collaboration with University of California San Francisco Medical School. The analysis of the architecture details is presented, and it is argued that the model can be employed for a broad scope of image segmentation problems of similar nature.","author":["Artem Sevastopolsky","Stepan Drapak","Konstantin Kiselev","Blake M. Snyder","Anastasia Georgievskaya"],"primaryCategory":"cs.CV","category":["cs.CV"],"timestamp_ms":{"$numberLong":"1525130136000"},"text":"In this work, we propose a special cascade network for image segmentation, which is based on the U-Net networks as building blocks and the idea of the iterative refinement. The model was mainly applied to achieve higher recognition quality for the task of finding borders of the optic disc and cup, which are relevant to the presence of glaucoma. Compared to a single U-Net and the state-of-the-art methods for the investigated tasks, very high segmentation quality has been achieved without a need for increasing the volume of datasets. Our experiments include comparison with the best-known methods on publicly available databases DRIONS-DB, RIM-ONE v.3, DRISHTI-GS, and evaluation on a private data set collected in collaboration with University of California San Francisco Medical School. The analysis of the architecture details is presented, and it is argued that the model can be employed for a broad scope of image segmentation problems of similar nature.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287db8"},"id":"arXiv:1805.00334v1","url":"http://arxiv.org/abs/1805.00334v1","updated":"2018-04-27T02:53:25Z","published":"2018-04-27T02:53:25Z","title":"Convolutional neural network for Fourier ptychography video\n  reconstruction: learning temporal dynamics from spatial ensembles","summary":"Convolutional neural networks (CNNs) have gained tremendous success in solving complex inverse problems for both problems involving independent datasets from input-output pairs of static objects, as well as sequential datasets from dynamic objects. In order to learn the underlying temporal statistics, a video sequence is typically used at the cost of network complexity and computation. The aim of this work is to develop a novel CNN framework to reconstruct video sequence of dynamic live cells captured using a computational microscopy technique, Fourier ptychographic microscopy (FPM). The unique feature of the FPM is its capability to reconstruct images with both wide field-of-view (FOV) and high resolution, i.e. a large space-bandwidth-product (SBP), by taking a series of low resolution intensity images. For live cell imaging, a single FPM frame contains thousands of cell samples with different morphological features. Our idea is to fully exploit the statistical information provided by this large spatial ensembles so as to learn temporal information in a sequential measurement, without using any additional temporal dataset. Specifically, we show that it is possible to reconstruct high-SBP dynamic cell videos by a CNN trained only on the first FPM dataset captured at the beginning of a time-series experiment. Our CNN approach reconstructs a 12800x10800 pixels phase image using only ~25 seconds, a 50x speedup compared to the model-based FPM algorithm. In addition, the CNN further reduces the required number of images in each time frame by ~6x. Overall, this significantly improves the imaging throughput by reducing both the acquisition and computational times. Our technique demonstrates a promising deep learning approach to continuously monitor large live-cell populations over an extended time and gather useful spatial and temporal information with sub-cellular resolution.","author":["Thanh Nguyen","Yujia Xue","Yunzhe Li","Lei Tian","George Nehmetallah"],"primaryCategory":"cs.CV","category":["cs.CV","physics.optics"],"timestamp_ms":{"$numberLong":"1524822805000"},"text":"Convolutional neural networks (CNNs) have gained tremendous success in solving complex inverse problems for both problems involving independent datasets from input-output pairs of static objects, as well as sequential datasets from dynamic objects. In order to learn the underlying temporal statistics, a video sequence is typically used at the cost of network complexity and computation. The aim of this work is to develop a novel CNN framework to reconstruct video sequence of dynamic live cells captured using a computational microscopy technique, Fourier ptychographic microscopy (FPM). The unique feature of the FPM is its capability to reconstruct images with both wide field-of-view (FOV) and high resolution, i.e. a large space-bandwidth-product (SBP), by taking a series of low resolution intensity images. For live cell imaging, a single FPM frame contains thousands of cell samples with different morphological features. Our idea is to fully exploit the statistical information provided by this large spatial ensembles so as to learn temporal information in a sequential measurement, without using any additional temporal dataset. Specifically, we show that it is possible to reconstruct high-SBP dynamic cell videos by a CNN trained only on the first FPM dataset captured at the beginning of a time-series experiment. Our CNN approach reconstructs a 12800x10800 pixels phase image using only ~25 seconds, a 50x speedup compared to the model-based FPM algorithm. In addition, the CNN further reduces the required number of images in each time frame by ~6x. Overall, this significantly improves the imaging throughput by reducing both the acquisition and computational times. Our technique demonstrates a promising deep learning approach to continuously monitor large live-cell populations over an extended time and gather useful spatial and temporal information with sub-cellular resolution.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287db7"},"id":"arXiv:1805.00258v1","url":"http://arxiv.org/abs/1805.00258v1","updated":"2018-05-01T09:56:43Z","published":"2018-05-01T09:56:43Z","title":"Object Activity Scene Description, Construction and Recognition","summary":"Action recognition is a critical task for social robots to meaningfully engage with their environment. 3D human skeleton-based action recognition is an attractive research area in recent years. Although, the existing approaches are good at action recognition, it is a great challenge to recognize a group of actions in an activity scene. To tackle this problem, at first, we partition the scene into several primitive actions (PAs) based upon motion attention mechanism. Then, the primitive actions are described by the trajectory vectors of corresponding joints. After that, motivated by text classification based on word embedding, we employ convolution neural network (CNN) to recognize activity scenes by considering motion of joints as \"word\" of activity. The experimental results on the scenes of human activity dataset show the efficiency of the proposed approach.","author":["Hui Feng","Shanshan Wang","Shuzhi Sam Ge"],"primaryCategory":"cs.CV","category":["cs.CV"],"timestamp_ms":{"$numberLong":"1525193803000"},"text":"Action recognition is a critical task for social robots to meaningfully engage with their environment. 3D human skeleton-based action recognition is an attractive research area in recent years. Although, the existing approaches are good at action recognition, it is a great challenge to recognize a group of actions in an activity scene. To tackle this problem, at first, we partition the scene into several primitive actions (PAs) based upon motion attention mechanism. Then, the primitive actions are described by the trajectory vectors of corresponding joints. After that, motivated by text classification based on word embedding, we employ convolution neural network (CNN) to recognize activity scenes by considering motion of joints as \"word\" of activity. The experimental results on the scenes of human activity dataset show the efficiency of the proposed approach.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287db6"},"id":"arXiv:1804.10745v2","url":"http://arxiv.org/abs/1804.10745v2","updated":"2018-05-01T05:51:52Z","published":"2018-04-28T05:20:53Z","title":"Generalizing Across Domains via Cross-Gradient Training","summary":"We present CROSSGRAD, a method to use multi-domain training data to learn a classifier that generalizes to new domains. CROSSGRAD does not need an adaptation phase via labeled or unlabeled data, or domain features in the new domain. Most existing domain adaptation methods attempt to erase domain signals using techniques like domain adversarial training. In contrast, CROSSGRAD is free to use domain signals for predicting labels, if it can prevent overfitting on training domains. We conceptualize the task in a Bayesian setting, in which a sampling step is implemented as data augmentation, based on domain-guided perturbations of input instances. CROSSGRAD parallelly trains a label and a domain classifier on examples perturbed by loss gradients of each other's objectives. This enables us to directly perturb inputs, without separating and re-mixing domain signals while making various distributional assumptions. Empirical evaluation on three different applications where this setting is natural establishes that (1) domain-guided perturbation provides consistently better generalization to unseen domains, compared to generic instance perturbation methods, and that (2) data augmentation is a more stable and accurate method than domain adversarial training.","author":["Shiv Shankar","Vihari Piratla","Soumen Chakrabarti","Siddhartha Chaudhuri","Preethi Jyothi","Sunita Sarawagi"],"primaryCategory":"cs.LG","category":["cs.LG","stat.ML"],"timestamp_ms":{"$numberLong":"1525179112000"},"text":"We present CROSSGRAD, a method to use multi-domain training data to learn a classifier that generalizes to new domains. CROSSGRAD does not need an adaptation phase via labeled or unlabeled data, or domain features in the new domain. Most existing domain adaptation methods attempt to erase domain signals using techniques like domain adversarial training. In contrast, CROSSGRAD is free to use domain signals for predicting labels, if it can prevent overfitting on training domains. We conceptualize the task in a Bayesian setting, in which a sampling step is implemented as data augmentation, based on domain-guided perturbations of input instances. CROSSGRAD parallelly trains a label and a domain classifier on examples perturbed by loss gradients of each other's objectives. This enables us to directly perturb inputs, without separating and re-mixing domain signals while making various distributional assumptions. Empirical evaluation on three different applications where this setting is natural establishes that (1) domain-guided perturbation provides consistently better generalization to unseen domains, compared to generic instance perturbation methods, and that (2) data augmentation is a more stable and accurate method than domain adversarial training.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287db5"},"id":"arXiv:1805.00185v1","url":"http://arxiv.org/abs/1805.00185v1","updated":"2018-05-01T04:54:45Z","published":"2018-05-01T04:54:45Z","title":"Phylotastic: An Experiment in Creating, Manipulating, and Evolving\n  Phylogenetic Biology Workflows Using Logic Programming","summary":"Evolutionary Biologists have long struggled with the challenge of developing analysis workflows in a flexible manner, thus facilitating the reuse of phylogenetic knowledge. An evolutionary biology workflow can be viewed as a plan which composes web services that can retrieve, manipulate, and produce phylogenetic trees. The Phylotastic project was launched two years ago as a collaboration between evolutionary biologists and computer scientists, with the goal of developing an open architecture to facilitate the creation of such analysis workflows. While composition of web services is a problem that has been extensively explored in the literature, including within the logic programming domain, the incarnation of the problem in Phylotastic provides a number of additional challenges. Along with the need to integrate preferences and formal ontologies in the description of the desired workflow, evolutionary biologists tend to construct workflows in an incremental manner, by successively refining the workflow, by indicating desired changes (e.g., exclusion of certain services, modifications of the desired output). This leads to the need of successive iterations of incremental replanning, to develop a new workflow that integrates the requested changes while minimizing the changes to the original workflow. This paper illustrates how Phylotastic has addressed the challenges of creating and refining phylogenetic analysis workflows using logic programming technology and how such solutions have been used within the general framework of the Phylotastic project. Under consideration in Theory and Practice of Logic Programming (TPLP).","author":["Thanh Hai Nguyen","Enrico Pontelli","Tran Cao Son"],"primaryCategory":"cs.AI","category":["cs.AI","cs.LO","cs.PL"],"timestamp_ms":{"$numberLong":"1525175685000"},"text":"Evolutionary Biologists have long struggled with the challenge of developing analysis workflows in a flexible manner, thus facilitating the reuse of phylogenetic knowledge. An evolutionary biology workflow can be viewed as a plan which composes web services that can retrieve, manipulate, and produce phylogenetic trees. The Phylotastic project was launched two years ago as a collaboration between evolutionary biologists and computer scientists, with the goal of developing an open architecture to facilitate the creation of such analysis workflows. While composition of web services is a problem that has been extensively explored in the literature, including within the logic programming domain, the incarnation of the problem in Phylotastic provides a number of additional challenges. Along with the need to integrate preferences and formal ontologies in the description of the desired workflow, evolutionary biologists tend to construct workflows in an incremental manner, by successively refining the workflow, by indicating desired changes (e.g., exclusion of certain services, modifications of the desired output). This leads to the need of successive iterations of incremental replanning, to develop a new workflow that integrates the requested changes while minimizing the changes to the original workflow. This paper illustrates how Phylotastic has addressed the challenges of creating and refining phylogenetic analysis workflows using logic programming technology and how such solutions have been used within the general framework of the Phylotastic project. Under consideration in Theory and Practice of Logic Programming (TPLP).","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287db4"},"id":"arXiv:1804.11287v1","url":"http://arxiv.org/abs/1804.11287v1","updated":"2018-04-30T16:02:12Z","published":"2018-04-30T16:02:12Z","title":"Computing Approximate Statistical Discrepancy","summary":"Consider a geometric range space $(X,\\c{A})$ where each data point $x \\in X$ has two or more values (say $r(x)$ and $b(x)$). Also consider a function $\\Phi(A)$ defined on any subset $A \\in (X,\\c{A})$ on the sum of values in that range e.g., $r_A = \\sum_{x \\in A} r(x)$ and $b_A = \\sum_{x \\in A} b(x)$. The $\\Phi$-maximum range is $A^* = \\arg \\max_{A \\in (X,\\c{A})} \\Phi(A)$. Our goal is to find some $\\hat{A}$ such that $|\\Phi(\\hat{A}) - \\Phi(A^*)| \\leq \\varepsilon.$ We develop algorithms for this problem for range spaces with bounded VC-dimension, as well as significant improvements for those defined by balls, halfspaces, and axis-aligned rectangles. This problem has many applications in many areas including discrepancy evaluation, classification, and spatial scan statistics.","author":["Michael Matheny","Jeff M. Phillips"],"primaryCategory":"cs.CG","category":["cs.CG"],"timestamp_ms":{"$numberLong":"1525129332000"},"text":"Consider a geometric range space $(X,\\c{A})$ where each data point $x \\in X$ has two or more values (say $r(x)$ and $b(x)$). Also consider a function $\\Phi(A)$ defined on any subset $A \\in (X,\\c{A})$ on the sum of values in that range e.g., $r_A = \\sum_{x \\in A} r(x)$ and $b_A = \\sum_{x \\in A} b(x)$. The $\\Phi$-maximum range is $A^* = \\arg \\max_{A \\in (X,\\c{A})} \\Phi(A)$. Our goal is to find some $\\hat{A}$ such that $|\\Phi(\\hat{A}) - \\Phi(A^*)| \\leq \\varepsilon.$ We develop algorithms for this problem for range spaces with bounded VC-dimension, as well as significant improvements for those defined by balls, halfspaces, and axis-aligned rectangles. This problem has many applications in many areas including discrepancy evaluation, classification, and spatial scan statistics.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287db3"},"id":"arXiv:1805.00528v1","url":"http://arxiv.org/abs/1805.00528v1","updated":"2018-04-19T08:17:08Z","published":"2018-04-19T08:17:08Z","title":"Reconstruction of Simulation-Based Physical Field with Limited Samples\n  by ReConNN","summary":"A variety of modeling techniques have been developed in the past decade to reduce the computational expense and increase the calculation accuracy. In this study, the distinctive characteristic compared to classical modeling models is \"from image based model to mechanical based model (e.g. stress, strain, and deformation)\". In such framework, a neural network architecture named ReConNN is proposed and the ReConNN mainly contains two neural networks that are CNN and GAN. A classical topology optimization is considered as an experimental example, and the CNN is employed to construct the mapping between contour images during topology optimization and compliance. Subsequently, the GAN is utilized to generate more contour images to improve the reconstructed model. Finally, the Lagrange polynomial is applied to complete the reconstruction. However, typical CNN architectures are commonly applied to classification problems, which appear powerless handling with regression of images for simulation problems. Meanwhile, the existing GAN architectures are insufficient to generate high-accuracy \"pseudo contour images\". Therefore, a Convolution in Convolution (CIC) architecture and a Convolutional AutoEncoder based on Wasserstein Generative Adversarial Network (WGAN-CAE) architecture are suggested. Specially, extensive experiments and comparisons with existing architectures of CNN and GAN demonstrate that the CIC is highly accurate and corresponding computational cost also can be significantly reduced when handling the regression problem of contour images, and the WGAN-CAE achieves significant improvements on generating contour images. The results demonstrate that the proposed ReConNN has a potential capability to reconstruct physical field for further researches, e.g. optimization.","author":["Yu Li","Hu Wang","Kangjia Mo","Tao Zeng"],"primaryCategory":"cs.LG","category":["cs.LG","cs.CV"],"timestamp_ms":{"$numberLong":"1524151028000"},"text":"A variety of modeling techniques have been developed in the past decade to reduce the computational expense and increase the calculation accuracy. In this study, the distinctive characteristic compared to classical modeling models is \"from image based model to mechanical based model (e.g. stress, strain, and deformation)\". In such framework, a neural network architecture named ReConNN is proposed and the ReConNN mainly contains two neural networks that are CNN and GAN. A classical topology optimization is considered as an experimental example, and the CNN is employed to construct the mapping between contour images during topology optimization and compliance. Subsequently, the GAN is utilized to generate more contour images to improve the reconstructed model. Finally, the Lagrange polynomial is applied to complete the reconstruction. However, typical CNN architectures are commonly applied to classification problems, which appear powerless handling with regression of images for simulation problems. Meanwhile, the existing GAN architectures are insufficient to generate high-accuracy \"pseudo contour images\". Therefore, a Convolution in Convolution (CIC) architecture and a Convolutional AutoEncoder based on Wasserstein Generative Adversarial Network (WGAN-CAE) architecture are suggested. Specially, extensive experiments and comparisons with existing architectures of CNN and GAN demonstrate that the CIC is highly accurate and corresponding computational cost also can be significantly reduced when handling the regression problem of contour images, and the WGAN-CAE achieves significant improvements on generating contour images. The results demonstrate that the proposed ReConNN has a potential capability to reconstruct physical field for further researches, e.g. optimization.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287db2"},"id":"arXiv:1805.00059v1","url":"http://arxiv.org/abs/1805.00059v1","updated":"2018-04-20T11:49:33Z","published":"2018-04-20T11:49:33Z","title":"Universality in Freezing Cellular Automata","summary":"Cellular Automata have been used since their introduction as a discrete tool of modelization. In many of the physical processes one may modelize thus (such as bootstrap percolation, forest fire or epidemic propagation models, life without death, etc), each local change is irreversible. The class of freezing Cellular Automata (FCA) captures this feature. In a freezing cellular automaton the states are ordered and the cells can only decrease their state according to this \"freezing-order\". We investigate the dynamics of such systems through the questions of simulation and universality in this class: is there a Freezing Cellular Automaton (FCA) that is able to simulate any Freezing Cellular Automata, i.e. an intrinsically universal FCA? We show that the answer to that question is sensitive to both the number of changes cells are allowed to make, and geometric features of the space. In dimension 1, there is no universal FCA. In dimension 2, if either the number of changes is at least 2, or the neighborhood is Moore, then there are universal FCA. On the other hand, there is no universal FCA with one change and Von Neumann neighborhood. We also show that monotonicity of the local rule with respect to the freezing-order (a common feature of bootstrap percolation) is also an obstacle to universality.","author":["Florent Becker","Diego Maldonado","Nicolas Ollinger","Guillaume Theyssier"],"primaryCategory":"cs.CC","category":["cs.CC","cs.DM","math.DS","nlin.CG"],"timestamp_ms":{"$numberLong":"1524250173000"},"text":"Cellular Automata have been used since their introduction as a discrete tool of modelization. In many of the physical processes one may modelize thus (such as bootstrap percolation, forest fire or epidemic propagation models, life without death, etc), each local change is irreversible. The class of freezing Cellular Automata (FCA) captures this feature. In a freezing cellular automaton the states are ordered and the cells can only decrease their state according to this \"freezing-order\". We investigate the dynamics of such systems through the questions of simulation and universality in this class: is there a Freezing Cellular Automaton (FCA) that is able to simulate any Freezing Cellular Automata, i.e. an intrinsically universal FCA? We show that the answer to that question is sensitive to both the number of changes cells are allowed to make, and geometric features of the space. In dimension 1, there is no universal FCA. In dimension 2, if either the number of changes is at least 2, or the neighborhood is Moore, then there are universal FCA. On the other hand, there is no universal FCA with one change and Von Neumann neighborhood. We also show that monotonicity of the local rule with respect to the freezing-order (a common feature of bootstrap percolation) is also an obstacle to universality.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287db1"},"id":"arXiv:1804.11240v1","url":"http://arxiv.org/abs/1804.11240v1","updated":"2018-03-30T13:13:04Z","published":"2018-03-30T13:13:04Z","title":"A blind robust watermarking method based on Arnold Cat map and amplified\n  pseudo-noise strings with weak correlation","summary":"In this paper, a robust and blind watermarking method is proposed, which is highly resistant to the common image watermarking attacks, such as noises, compression, and image quality enhancement processing. In this method, Arnold Cat map is used as a pre-processing on the host image, which increases the security and imperceptibility of embedding watermark bits with a strong gain factor. Moreover, two pseudo-noise strings with weak correlation are used as the symbol of each 0 or 1 bit of the watermark, which increases the accuracy in detecting the state of watermark bits at extraction phase in comparison to using two random pseudo-noise strings. In this method, to increase the robustness and further imperceptibility of the embedding, the Arnold Cat mapped image is subjected to non-overlapping blocking, and then the high frequency coefficients of the approximation sub-band of the FDCuT transform are used as the embedding location for each block. Comparison of the proposed method with recent robust methods under the same experimental conditions indicates the superiority of the proposed method.","author":["Seyyed Hossein Soleymani","Amir Hossein Taherinia","Amir Hossein Mohajerzadeh"],"primaryCategory":"cs.MM","category":["cs.MM"],"timestamp_ms":{"$numberLong":"1522440784000"},"text":"In this paper, a robust and blind watermarking method is proposed, which is highly resistant to the common image watermarking attacks, such as noises, compression, and image quality enhancement processing. In this method, Arnold Cat map is used as a pre-processing on the host image, which increases the security and imperceptibility of embedding watermark bits with a strong gain factor. Moreover, two pseudo-noise strings with weak correlation are used as the symbol of each 0 or 1 bit of the watermark, which increases the accuracy in detecting the state of watermark bits at extraction phase in comparison to using two random pseudo-noise strings. In this method, to increase the robustness and further imperceptibility of the embedding, the Arnold Cat mapped image is subjected to non-overlapping blocking, and then the high frequency coefficients of the approximation sub-band of the FDCuT transform are used as the embedding location for each block. Comparison of the proposed method with recent robust methods under the same experimental conditions indicates the superiority of the proposed method.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287db0"},"id":"arXiv:1804.09790v1","url":"http://arxiv.org/abs/1804.09790v1","updated":"2018-04-25T20:26:46Z","published":"2018-04-25T20:26:46Z","title":"Adaptive MPC with Chance Constraints for FIR Systems","summary":"This paper proposes an adaptive stochastic Model Predictive Control (MPC) strategy for stable linear time invariant systems in the presence of bounded disturbances. We consider multi-input multi-output systems that can be expressed by a finite impulse response model, whose parameters we estimate using a linear Recursive Least Squares algorithm. Building on the work of [1],[2], our approach is able to handle hard input constraints and probabilistic output constraints. By using tools from distributionally robust optimization, we formulate our MPC design task as a convex optimization problem that can be solved using existing tools. Furthermore, we show that our adaptive stochastic MPC algorithm is persistently feasible. The efficacy of the developed algorithm is demonstrated in a numerical example and the results are compared with the adaptive robust MPC algorithm of [2].","author":["Monimoy Bujarbaruah","Xiaojing Zhang","Francesco Borrelli"],"primaryCategory":"cs.SY","category":["cs.SY"],"timestamp_ms":{"$numberLong":"1524713206000"},"text":"This paper proposes an adaptive stochastic Model Predictive Control (MPC) strategy for stable linear time invariant systems in the presence of bounded disturbances. We consider multi-input multi-output systems that can be expressed by a finite impulse response model, whose parameters we estimate using a linear Recursive Least Squares algorithm. Building on the work of [1],[2], our approach is able to handle hard input constraints and probabilistic output constraints. By using tools from distributionally robust optimization, we formulate our MPC design task as a convex optimization problem that can be solved using existing tools. Furthermore, we show that our adaptive stochastic MPC algorithm is persistently feasible. The efficacy of the developed algorithm is demonstrated in a numerical example and the results are compared with the adaptive robust MPC algorithm of [2].","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287daf"},"id":"arXiv:1804.10334v1","url":"http://arxiv.org/abs/1804.10334v1","updated":"2018-04-27T04:07:49Z","published":"2018-04-27T04:07:49Z","title":"Deep Learning Coordinated Beamforming for Highly-Mobile Millimeter Wave\n  Systems","summary":"Supporting high mobility in millimeter wave (mmWave) systems enables a wide range of important applications such as vehicular communications and wireless virtual/augmented reality. Realizing this in practice, though, requires overcoming several challenges. First, the use of narrow beams and the sensitivity of mmWave signals to blockage greatly impact the coverage and reliability of highly-mobile links. Second, highly-mobile users in dense mmWave deployments need to frequently hand-off between base stations (BSs), which is associated with critical control and latency overhead. Further, identifying the optimal beamforming vectors in large antenna array mmWave systems requires considerable training overhead, which significantly affects the efficiency of these mobile systems. In this paper, a novel integrated machine learning and coordinated beamforming solution is developed to overcome these challenges and enable highly-mobile mmWave applications. In the proposed solution, a number of distributed yet coordinating BSs simultaneously serve a mobile user. This user ideally needs to transmit only one uplink training pilot sequence that will be jointly received at the coordinating BSs using omni or quasi-omni beam patterns. These received signals draw a defining signature not only for the user location, but also for its interaction with the surrounding environment. The developed solution then leverages a deep learning model that learns how to use these signatures to predict the beamforming vectors at the BSs. This renders a comprehensive solution that supports highly-mobile mmWave applications with reliable coverage, low latency, and negligible training overhead. Simulation results show that the proposed deep-learning coordinated beamforming strategy approaches the achievable rate of the genie-aided solution that knows the optimal beamforming vectors with no training overhead.","author":["Ahmed Alkhateeb","Sam Alex","Paul Varkey","Ying Li","Qi Qu","Djordje Tujkovic"],"primaryCategory":"cs.IT","category":["cs.IT","math.IT"],"timestamp_ms":{"$numberLong":"1524827269000"},"text":"Supporting high mobility in millimeter wave (mmWave) systems enables a wide range of important applications such as vehicular communications and wireless virtual/augmented reality. Realizing this in practice, though, requires overcoming several challenges. First, the use of narrow beams and the sensitivity of mmWave signals to blockage greatly impact the coverage and reliability of highly-mobile links. Second, highly-mobile users in dense mmWave deployments need to frequently hand-off between base stations (BSs), which is associated with critical control and latency overhead. Further, identifying the optimal beamforming vectors in large antenna array mmWave systems requires considerable training overhead, which significantly affects the efficiency of these mobile systems. In this paper, a novel integrated machine learning and coordinated beamforming solution is developed to overcome these challenges and enable highly-mobile mmWave applications. In the proposed solution, a number of distributed yet coordinating BSs simultaneously serve a mobile user. This user ideally needs to transmit only one uplink training pilot sequence that will be jointly received at the coordinating BSs using omni or quasi-omni beam patterns. These received signals draw a defining signature not only for the user location, but also for its interaction with the surrounding environment. The developed solution then leverages a deep learning model that learns how to use these signatures to predict the beamforming vectors at the BSs. This renders a comprehensive solution that supports highly-mobile mmWave applications with reliable coverage, low latency, and negligible training overhead. Simulation results show that the proposed deep-learning coordinated beamforming strategy approaches the achievable rate of the genie-aided solution that knows the optimal beamforming vectors with no training overhead.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287dae"},"id":"arXiv:1805.00322v1","url":"http://arxiv.org/abs/1805.00322v1","updated":"2018-04-20T23:56:10Z","published":"2018-04-20T23:56:10Z","title":"Occluded object reconstruction for first responders with augmented\n  reality glasses using conditional generative adversarial networks","summary":"Firefighters suffer a variety of life-threatening risks, including line-of-duty deaths, injuries, and exposures to hazardous substances. Support for reducing these risks is important. We built a partially occluded object reconstruction method on augmented reality glasses for first responders. We used a deep learning based on conditional generative adversarial networks to train associations between the various images of flammable and hazardous objects and their partially occluded counterparts. Our system then reconstructed an image of a new flammable object. Finally, the reconstructed image was superimposed on the input image to provide \"transparency\". The system imitates human learning about the laws of physics through experience by learning the shape of flammable objects and the flame characteristics.","author":["Kyongsik Yun","Thomas Lu","Edward Chow"],"primaryCategory":"cs.CV","category":["cs.CV","cs.LG"],"timestamp_ms":{"$numberLong":"1524293770000"},"text":"Firefighters suffer a variety of life-threatening risks, including line-of-duty deaths, injuries, and exposures to hazardous substances. Support for reducing these risks is important. We built a partially occluded object reconstruction method on augmented reality glasses for first responders. We used a deep learning based on conditional generative adversarial networks to train associations between the various images of flammable and hazardous objects and their partially occluded counterparts. Our system then reconstructed an image of a new flammable object. Finally, the reconstructed image was superimposed on the input image to provide \"transparency\". The system imitates human learning about the laws of physics through experience by learning the shape of flammable objects and the flame characteristics.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287dad"},"id":"arXiv:1804.10819v1","url":"http://arxiv.org/abs/1804.10819v1","updated":"2018-04-28T15:23:25Z","published":"2018-04-28T15:23:25Z","title":"Learning Cross-Modal Deep Embeddings for Multi-Object Image Retrieval\n  using Text and Sketch","summary":"In this work we introduce a cross modal image retrieval system that allows both text and sketch as input modalities for the query. A cross-modal deep network architecture is formulated to jointly model the sketch and text input modalities as well as the the image output modality, learning a common embedding between text and images and between sketches and images. In addition, an attention model is used to selectively focus the attention on the different objects of the image, allowing for retrieval with multiple objects in the query. Experiments show that the proposed method performs the best in both single and multiple object image retrieval in standard datasets.","author":["Sounak Dey","Anjan Dutta","Suman K. Ghosh","Ernest Valveny","Josep Lladós","Umapada Pal"],"primaryCategory":"cs.CV","category":["cs.CV"],"timestamp_ms":{"$numberLong":"1524954205000"},"text":"In this work we introduce a cross modal image retrieval system that allows both text and sketch as input modalities for the query. A cross-modal deep network architecture is formulated to jointly model the sketch and text input modalities as well as the the image output modality, learning a common embedding between text and images and between sketches and images. In addition, an attention model is used to selectively focus the attention on the different objects of the image, allowing for retrieval with multiple objects in the query. Experiments show that the proposed method performs the best in both single and multiple object image retrieval in standard datasets.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287dac"},"id":"arXiv:1804.10167v1","url":"http://arxiv.org/abs/1804.10167v1","updated":"2018-04-26T16:48:52Z","published":"2018-04-26T16:48:52Z","title":"fMRI: preprocessing, classification and pattern recognition","summary":"As machine learning continues to gain momentum in the neuroscience community, we witness the emergence of novel applications such as diagnostics, characterization, and treatment outcome prediction for psychiatric and neurological disorders, for instance, epilepsy and depression. Systematic research into these mental disorders increasingly involves drawing clinical conclusions on the basis of data-driven approaches; to this end, structural and functional neuroimaging serve as key source modalities. Identification of informative neuroimaging markers requires establishing a comprehensive preparation pipeline for data which may be severely corrupted by artifactual signal fluctuations. In this work, we review a large body of literature to provide ample evidence for the advantages of pattern recognition approaches in clinical applications, overview advanced graph-based pattern recognition approaches, and propose a noise-aware neuroimaging data processing pipeline. To demonstrate the effectiveness of our approach, we provide results from a pilot study, which show a significant improvement in classification accuracy, indicating a promising research direction.","author":["Maxim Sharaev","Alexander Andreev","Alexey Artemov","Alexander Bernstein","Evgeny Burnaev","Ekaterina Kondratyeva","Svetlana Sushchinskaya","Renat Akzhigitov"],"primaryCategory":"cs.CV","category":["cs.CV","stat.AP"],"timestamp_ms":{"$numberLong":"1524786532000"},"text":"As machine learning continues to gain momentum in the neuroscience community, we witness the emergence of novel applications such as diagnostics, characterization, and treatment outcome prediction for psychiatric and neurological disorders, for instance, epilepsy and depression. Systematic research into these mental disorders increasingly involves drawing clinical conclusions on the basis of data-driven approaches; to this end, structural and functional neuroimaging serve as key source modalities. Identification of informative neuroimaging markers requires establishing a comprehensive preparation pipeline for data which may be severely corrupted by artifactual signal fluctuations. In this work, we review a large body of literature to provide ample evidence for the advantages of pattern recognition approaches in clinical applications, overview advanced graph-based pattern recognition approaches, and propose a noise-aware neuroimaging data processing pipeline. To demonstrate the effectiveness of our approach, we provide results from a pilot study, which show a significant improvement in classification accuracy, indicating a promising research direction.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287dab"},"id":"arXiv:1805.00634v1","url":"http://arxiv.org/abs/1805.00634v1","updated":"2018-05-02T05:37:42Z","published":"2018-05-02T05:37:42Z","title":"A Probabilistic Extension of Action Language BC+","summary":"We present a probabilistic extension of action language BC+. Just like BC+ is defined as a high-level notation of answer set programs for describing transition systems, the proposed language, which we call pBC+, is defined as a high-level notation of LPMLN programs---a probabilistic extension of answer set programs. We show how probabilistic reasoning about transition systems, such as prediction, postdiction, and planning problems, as well as probabilistic diagnosis for dynamic domains, can be modeled in pBC+ and computed using an implementation of LPMLN.","author":["Joohyung Lee","Yi Wang"],"primaryCategory":"cs.AI","category":["cs.AI"],"timestamp_ms":{"$numberLong":"1525264662000"},"text":"We present a probabilistic extension of action language BC+. Just like BC+ is defined as a high-level notation of answer set programs for describing transition systems, the proposed language, which we call pBC+, is defined as a high-level notation of LPMLN programs---a probabilistic extension of answer set programs. We show how probabilistic reasoning about transition systems, such as prediction, postdiction, and planning problems, as well as probabilistic diagnosis for dynamic domains, can be modeled in pBC+ and computed using an implementation of LPMLN.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287daa"},"id":"arXiv:1804.10481v1","url":"http://arxiv.org/abs/1804.10481v1","updated":"2018-04-27T13:03:42Z","published":"2018-04-27T13:03:42Z","title":"Interactive Medical Image Segmentation via Point-Based Interaction and\n  Sequential Patch Learning","summary":"Due to low tissue contrast, irregular object appearance, and unpredictable location variation, segmenting the objects from different medical imaging modalities (e.g., CT, MR) is considered as an important yet challenging task. In this paper, we present a novel method for interactive medical image segmentation with the following merits. (1) Our design is fundamentally different from previous pure patch-based and image-based segmentation methods. We observe that during delineation, the physician repeatedly check the inside-outside intensity changing to determine the boundary, which indicates that comparison in an inside-outside manner is extremely important. Thus, we innovatively model our segmentation task as learning the representation of the bi-directional sequential patches, starting from (or ending in) the given central point of the object. This can be realized by our proposed ConvRNN network embedded with a gated memory propagation unit. (2) Unlike previous interactive methods (requiring bounding box or seed points), we only ask the physician to merely click on the rough central point of the object before segmentation, which could simultaneously enhance the performance and reduce the segmentation time. (3) We utilize our method in a multi-level framework for better performance. We systematically evaluate our method in three different segmentation tasks including CT kidney tumor, MR prostate, and PROMISE12 challenge, showing promising results compared with state-of-the-art methods. The code is available here: \\href{https://github.com/sunalbert/Sequential-patch-based-segmentation}{Sequential-patch-based-segmentation}.","author":["Jinquan Sun","Yinghuan Shi","Yang Gao","Lei Wang","Luping Zhou","Wanqi Yang","Dinggang Shen"],"primaryCategory":"cs.CV","category":["cs.CV"],"timestamp_ms":{"$numberLong":"1524859422000"},"text":"Due to low tissue contrast, irregular object appearance, and unpredictable location variation, segmenting the objects from different medical imaging modalities (e.g., CT, MR) is considered as an important yet challenging task. In this paper, we present a novel method for interactive medical image segmentation with the following merits. (1) Our design is fundamentally different from previous pure patch-based and image-based segmentation methods. We observe that during delineation, the physician repeatedly check the inside-outside intensity changing to determine the boundary, which indicates that comparison in an inside-outside manner is extremely important. Thus, we innovatively model our segmentation task as learning the representation of the bi-directional sequential patches, starting from (or ending in) the given central point of the object. This can be realized by our proposed ConvRNN network embedded with a gated memory propagation unit. (2) Unlike previous interactive methods (requiring bounding box or seed points), we only ask the physician to merely click on the rough central point of the object before segmentation, which could simultaneously enhance the performance and reduce the segmentation time. (3) We utilize our method in a multi-level framework for better performance. We systematically evaluate our method in three different segmentation tasks including CT kidney tumor, MR prostate, and PROMISE12 challenge, showing promising results compared with state-of-the-art methods. The code is available here: \\href{https://github.com/sunalbert/Sequential-patch-based-segmentation}{Sequential-patch-based-segmentation}.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287da9"},"id":"arXiv:1804.10692v1","url":"http://arxiv.org/abs/1804.10692v1","updated":"2018-04-27T21:26:08Z","published":"2018-04-27T21:26:08Z","title":"Reward Learning from Narrated Demonstrations","summary":"Humans effortlessly \"program\" one another by communicating goals and desires in natural language. In contrast, humans program robotic behaviours by indicating desired object locations and poses to be achieved, by providing RGB images of goal configurations, or supplying a demonstration to be imitated. None of these methods generalize across environment variations, and they convey the goal in awkward technical terms. This work proposes joint learning of natural language grounding and instructable behavioural policies reinforced by perceptual detectors of natural language expressions, grounded to the sensory inputs of the robotic agent. Our supervision is narrated visual demonstrations(NVD), which are visual demonstrations paired with verbal narration (as opposed to being silent). We introduce a dataset of NVD where teachers perform activities while describing them in detail. We map the teachers' descriptions to perceptual reward detectors, and use them to train corresponding behavioural policies in simulation.We empirically show that our instructable agents (i) learn visual reward detectors using a small number of examples by exploiting hard negative mined configurations from demonstration dynamics, (ii) develop pick-and place policies using learned visual reward detectors, (iii) benefit from object-factorized state representations that mimic the syntactic structure of natural language goal expressions, and (iv) can execute behaviours that involve novel objects in novel locations at test time, instructed by natural language.","author":["Hsiao-Yu Fish Tung","Adam W. Harley","Liang-Kang Huang","Katerina Fragkiadaki"],"primaryCategory":"cs.CV","category":["cs.CV","cs.RO"],"timestamp_ms":{"$numberLong":"1524889568000"},"text":"Humans effortlessly \"program\" one another by communicating goals and desires in natural language. In contrast, humans program robotic behaviours by indicating desired object locations and poses to be achieved, by providing RGB images of goal configurations, or supplying a demonstration to be imitated. None of these methods generalize across environment variations, and they convey the goal in awkward technical terms. This work proposes joint learning of natural language grounding and instructable behavioural policies reinforced by perceptual detectors of natural language expressions, grounded to the sensory inputs of the robotic agent. Our supervision is narrated visual demonstrations(NVD), which are visual demonstrations paired with verbal narration (as opposed to being silent). We introduce a dataset of NVD where teachers perform activities while describing them in detail. We map the teachers' descriptions to perceptual reward detectors, and use them to train corresponding behavioural policies in simulation.We empirically show that our instructable agents (i) learn visual reward detectors using a small number of examples by exploiting hard negative mined configurations from demonstration dynamics, (ii) develop pick-and place policies using learned visual reward detectors, (iii) benefit from object-factorized state representations that mimic the syntactic structure of natural language goal expressions, and (iv) can execute behaviours that involve novel objects in novel locations at test time, instructed by natural language.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287da8"},"id":"arXiv:1805.00178v1","url":"http://arxiv.org/abs/1805.00178v1","updated":"2018-05-01T04:09:09Z","published":"2018-05-01T04:09:09Z","title":"Dynamic Sentence Sampling for Efficient Training of Neural Machine\n  Translation","summary":"Traditional Neural machine translation (NMT) involves a fixed training procedure where each sentence is sampled once during each epoch. In reality, some sentences are well-learned during the initial few epochs; however, using this approach, the well-learned sentences would continue to be trained along with those sentences that were not well learned for 10-30 epochs, which results in a wastage of time. Here, we propose an efficient method to dynamically sample the sentences in order to accelerate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks depict that the proposed method can significantly accelerate the NMT training and improve the NMT performance.","author":["Rui Wang","Masao Utiyama","Eiichiro Sumita"],"primaryCategory":"cs.CL","category":["cs.CL"],"timestamp_ms":{"$numberLong":"1525172949000"},"text":"Traditional Neural machine translation (NMT) involves a fixed training procedure where each sentence is sampled once during each epoch. In reality, some sentences are well-learned during the initial few epochs; however, using this approach, the well-learned sentences would continue to be trained along with those sentences that were not well learned for 10-30 epochs, which results in a wastage of time. Here, we propose an efficient method to dynamically sample the sentences in order to accelerate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks depict that the proposed method can significantly accelerate the NMT training and improve the NMT performance.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287da7"},"id":"arXiv:1805.00585v1","url":"http://arxiv.org/abs/1805.00585v1","updated":"2018-05-02T00:24:36Z","published":"2018-05-02T00:24:36Z","title":"Dynamically Improving Branch Prediction Accuracy Between Contexts","summary":"Branch prediction is a standard feature in most processors, significantly improving the run time of programs by allowing a processor to predict the direction of a branch before it has been evaluated. Current branch prediction methods can achieve excellent prediction accuracy through global tables, various hashing methods, and even machine learning techniques such as SVMs or neural networks. Such designs, however, may lose effectiveness when attempting to predict across context switches in the operating system. Such a scenario may lead to destructive interference between contexts, therefore reducing overall predictor accuracy. To solve this problem, we propose a novel scheme for deciding whether a context switch produces destructive or constructive interference. First, we present evidence that shows that destructive interference can have a significant negative impact on prediction accuracy. Second, we present an extensible framework that keeps track of context switches and prediction accuracy to improve overall accuracy. Experimental results show that this framework effectively reduces the effect of destructive interference on branch prediction.","author":["Adam Auten","Tanishq Dubey","Rohan Mathur"],"primaryCategory":"cs.AR","category":["cs.AR"],"timestamp_ms":{"$numberLong":"1525245876000"},"text":"Branch prediction is a standard feature in most processors, significantly improving the run time of programs by allowing a processor to predict the direction of a branch before it has been evaluated. Current branch prediction methods can achieve excellent prediction accuracy through global tables, various hashing methods, and even machine learning techniques such as SVMs or neural networks. Such designs, however, may lose effectiveness when attempting to predict across context switches in the operating system. Such a scenario may lead to destructive interference between contexts, therefore reducing overall predictor accuracy. To solve this problem, we propose a novel scheme for deciding whether a context switch produces destructive or constructive interference. First, we present evidence that shows that destructive interference can have a significant negative impact on prediction accuracy. Second, we present an extensible framework that keeps track of context switches and prediction accuracy to improve overall accuracy. Experimental results show that this framework effectively reduces the effect of destructive interference on branch prediction.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287da6"},"id":"arXiv:1804.10652v1","url":"http://arxiv.org/abs/1804.10652v1","updated":"2018-04-27T19:13:34Z","published":"2018-04-27T19:13:34Z","title":"Human Motion Modeling using DVGANs","summary":"We present a novel generative model for human motion modeling using Generative Adversarial Networks (GANs). We formulate the GAN discriminator using dense validation at each time-scale and perturb the discriminator input to make it translation invariant. Our model is capable of motion generation and completion. We show through our evaluations the resiliency to noise, generalization over actions, and generation of long diverse sequences. We evaluate our approach on Human 3.6M and CMU motion capture datasets using inception scores.","author":["Xiao Lin","Mohamed R. Amer"],"primaryCategory":"cs.CV","category":["cs.CV"],"timestamp_ms":{"$numberLong":"1524881614000"},"text":"We present a novel generative model for human motion modeling using Generative Adversarial Networks (GANs). We formulate the GAN discriminator using dense validation at each time-scale and perturb the discriminator input to make it translation invariant. Our model is capable of motion generation and completion. We show through our evaluations the resiliency to noise, generalization over actions, and generation of long diverse sequences. We evaluate our approach on Human 3.6M and CMU motion capture datasets using inception scores.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287da5"},"id":"arXiv:1804.09773v1","url":"http://arxiv.org/abs/1804.09773v1","updated":"2018-04-25T19:45:36Z","published":"2018-04-25T19:45:36Z","title":"Optimal measurement selection algorithm and estimator for ultra-wideband\n  symmetric ranging localization","summary":"A state estimator is derived for an agent with the ability to measure single ranges to fixed points in its environment, and equipped with an accelerometer and a rate gyroscope. The state estimator makes no agent-specific assumptions, and can be immediately applied to any rigid body with these sensors. Also, the state estimator doesn't use any trilateration-based method to calculate position from range measurements. As the considered system can only make a single range measurement at a time, we present a greedy optimization algorithm for selecting the best measurement. Experiments in an indoor testbed using an externally controlled multicopter demonstrate the efficacy of the algorithm, specifically showing an improvement over a na\\\"ive strategy of a fixed sequence of measurements. In separate experiments, the algorithm is also used in feedback control, to control the position of the multicopter.","author":["Saman Fahandezh-Saadi","Mark W. Mueller"],"primaryCategory":"cs.RO","category":["cs.RO"],"timestamp_ms":{"$numberLong":"1524710736000"},"text":"A state estimator is derived for an agent with the ability to measure single ranges to fixed points in its environment, and equipped with an accelerometer and a rate gyroscope. The state estimator makes no agent-specific assumptions, and can be immediately applied to any rigid body with these sensors. Also, the state estimator doesn't use any trilateration-based method to calculate position from range measurements. As the considered system can only make a single range measurement at a time, we present a greedy optimization algorithm for selecting the best measurement. Experiments in an indoor testbed using an externally controlled multicopter demonstrate the efficacy of the algorithm, specifically showing an improvement over a na\\\"ive strategy of a fixed sequence of measurements. In separate experiments, the algorithm is also used in feedback control, to control the position of the multicopter.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287da4"},"id":"arXiv:1804.10318v2","url":"http://arxiv.org/abs/1804.10318v2","updated":"2018-04-30T01:07:07Z","published":"2018-04-27T01:25:41Z","title":"Efficiently Learning Nonstationary Gaussian Processes for Real World\n  Impact","summary":"Most real world phenomena such as sunlight distribution under a forest canopy, minerals concentration, stock valuation, exhibit nonstationary dynamics i.e. phenomenon variation changes depending on the locality. Nonstationary dynamics pose both theoretical and practical challenges to statistical machine learning algorithms that aim to accurately capture the complexities governing the evolution of such processes. Typically the nonstationary dynamics are modeled using nonstationary Gaussian Process models (NGPS) that employ local latent dynamics parameterization to correspondingly model the nonstationary real observable dynamics. Recently, an approach based on most likely induced latent dynamics representation attracted research community's attention for a while. The approach could not be employed for large scale real world applications because learning a most likely latent dynamics representation involves maximization of marginal likelihood of the observed real dynamics that becomes intractable as the number of induced latent points grows with problem size. We have established a direct relationship between informativeness of the induced latent dynamics and the marginal likelihood of the observed real dynamics. This opens up the possibility of maximizing marginal likelihood of observed real dynamics indirectly by near optimally maximizing entropy or mutual information gain on the induced latent dynamics using greedy algorithms. Therefore, for an efficient yet accurate inference, we propose to build an induced latent dynamics representation using a novel algorithm LISAL that adaptively maximizes entropy or mutual information on the induced latent dynamics and marginal likelihood of observed real dynamics in an iterative manner. The relevance of LISAL is validated using real world datasets.","author":["Sahil Garg"],"primaryCategory":"cs.LG","category":["cs.LG","stat.ML"],"timestamp_ms":{"$numberLong":"1525075627000"},"text":"Most real world phenomena such as sunlight distribution under a forest canopy, minerals concentration, stock valuation, exhibit nonstationary dynamics i.e. phenomenon variation changes depending on the locality. Nonstationary dynamics pose both theoretical and practical challenges to statistical machine learning algorithms that aim to accurately capture the complexities governing the evolution of such processes. Typically the nonstationary dynamics are modeled using nonstationary Gaussian Process models (NGPS) that employ local latent dynamics parameterization to correspondingly model the nonstationary real observable dynamics. Recently, an approach based on most likely induced latent dynamics representation attracted research community's attention for a while. The approach could not be employed for large scale real world applications because learning a most likely latent dynamics representation involves maximization of marginal likelihood of the observed real dynamics that becomes intractable as the number of induced latent points grows with problem size. We have established a direct relationship between informativeness of the induced latent dynamics and the marginal likelihood of the observed real dynamics. This opens up the possibility of maximizing marginal likelihood of observed real dynamics indirectly by near optimally maximizing entropy or mutual information gain on the induced latent dynamics using greedy algorithms. Therefore, for an efficient yet accurate inference, we propose to build an induced latent dynamics representation using a novel algorithm LISAL that adaptively maximizes entropy or mutual information on the induced latent dynamics and marginal likelihood of observed real dynamics in an iterative manner. The relevance of LISAL is validated using real world datasets.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287da3"},"id":"arXiv:1805.00270v1","url":"http://arxiv.org/abs/1805.00270v1","updated":"2018-05-01T11:08:00Z","published":"2018-05-01T11:08:00Z","title":"Capturing Ambiguity in Crowdsourcing Frame Disambiguation","summary":"FrameNet is a computational linguistics resource composed of semantic frames, high-level concepts that represent the meanings of words. In this paper, we present an approach to gather frame disambiguation annotations in sentences using a crowdsourcing approach with multiple workers per sentence to capture inter-annotator disagreement. We perform an experiment over a set of 433 sentences annotated with frames from the FrameNet corpus, and show that the aggregated crowd annotations achieve an F1 score greater than 0.67 as compared to expert linguists. We highlight cases where the crowd annotation was correct even though the expert is in disagreement, arguing for the need to have multiple annotators per sentence. Most importantly, we examine cases in which crowd workers could not agree, and demonstrate that these cases exhibit ambiguity, either in the sentence, frame, or the task itself, and argue that collapsing such cases to a single, discrete truth value (i.e. correct or incorrect) is inappropriate, creating arbitrary targets for machine learning.","author":["Anca Dumitrache","Lora Aroyo","Chris Welty"],"primaryCategory":"cs.CL","category":["cs.CL"],"timestamp_ms":{"$numberLong":"1525198080000"},"text":"FrameNet is a computational linguistics resource composed of semantic frames, high-level concepts that represent the meanings of words. In this paper, we present an approach to gather frame disambiguation annotations in sentences using a crowdsourcing approach with multiple workers per sentence to capture inter-annotator disagreement. We perform an experiment over a set of 433 sentences annotated with frames from the FrameNet corpus, and show that the aggregated crowd annotations achieve an F1 score greater than 0.67 as compared to expert linguists. We highlight cases where the crowd annotation was correct even though the expert is in disagreement, arguing for the need to have multiple annotators per sentence. Most importantly, we examine cases in which crowd workers could not agree, and demonstrate that these cases exhibit ambiguity, either in the sentence, frame, or the task itself, and argue that collapsing such cases to a single, discrete truth value (i.e. correct or incorrect) is inappropriate, creating arbitrary targets for machine learning.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287da2"},"id":"arXiv:1804.10916v1","url":"http://arxiv.org/abs/1804.10916v1","updated":"2018-04-29T12:19:53Z","published":"2018-04-29T12:19:53Z","title":"Unsupervised Cross-Modality Domain Adaptation of ConvNets for Biomedical\n  Image Segmentations with Adversarial Loss","summary":"Convolutional networks (ConvNets) have achieved great successes in various challenging vision tasks. However, the performance of ConvNets would degrade when encountering the domain shift. The domain adaptation is more significant while challenging in the field of biomedical image analysis, where cross-modality data have largely different distributions. Given that annotating the medical data is especially expensive, the supervised transfer learning approaches are not quite optimal. In this paper, we propose an unsupervised domain adaptation framework with adversarial learning for cross-modality biomedical image segmentations. Specifically, our model is based on a dilated fully convolutional network for pixel-wise prediction. Moreover, we build a plug-and-play domain adaptation module (DAM) to map the target input to features which are aligned with source domain feature space. A domain critic module (DCM) is set up for discriminating the feature space of both domains. We optimize the DAM and DCM via an adversarial loss without using any target domain label. Our proposed method is validated by adapting a ConvNet trained with MRI images to unpaired CT data for cardiac structures segmentations, and achieved very promising results.","author":["Qi Dou","Cheng Ouyang","Cheng Chen","Hao Chen","Pheng-Ann Heng"],"primaryCategory":"cs.CV","category":["cs.CV"],"timestamp_ms":{"$numberLong":"1525029593000"},"text":"Convolutional networks (ConvNets) have achieved great successes in various challenging vision tasks. However, the performance of ConvNets would degrade when encountering the domain shift. The domain adaptation is more significant while challenging in the field of biomedical image analysis, where cross-modality data have largely different distributions. Given that annotating the medical data is especially expensive, the supervised transfer learning approaches are not quite optimal. In this paper, we propose an unsupervised domain adaptation framework with adversarial learning for cross-modality biomedical image segmentations. Specifically, our model is based on a dilated fully convolutional network for pixel-wise prediction. Moreover, we build a plug-and-play domain adaptation module (DAM) to map the target input to features which are aligned with source domain feature space. A domain critic module (DCM) is set up for discriminating the feature space of both domains. We optimize the DAM and DCM via an adversarial loss without using any target domain label. Our proposed method is validated by adapting a ConvNet trained with MRI images to unpaired CT data for cardiac structures segmentations, and achieved very promising results.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287da1"},"id":"arXiv:1805.00650v1","url":"http://arxiv.org/abs/1805.00650v1","updated":"2018-05-02T07:08:19Z","published":"2018-05-02T07:08:19Z","title":"Efficient Membership Testing for Pseudovarieties of Finite Semigroups","summary":"We consider the complexity of deciding membership of a given finite semigroup to a fixed pseudovariety. While it is known that there exist pseudovarieties with NP-complete or even undecidable membership problems, for many well-known pseudovarieties the problem is known to be decidable in polynomial time. We show that for many of these pseudovarieties, the membership problem is actually in AC^0. To this end, we show that these pseudovarieties can be characterized by first-order sentences with multiplication as the only predicate. We prove closure properties of the class of pseudovarieties with such first-order descriptions under various well-known operations; in particular, if V can be described by a first-order sentence, then DV, LV, and the Mal'cev products of K, D, N, LI, and LG with V are first-order definable as well. Moreover, if H is a first-order definable pseudovariety of finite groups, then the pseudovariety of all finite semigroups whose subgroups are in H is first-order definable. Our logical formalism can be extended by a predicate corresponding to the omega-operator in semigroups. The extended logic is powerful enough to capture all finitely based pseudovarieties, i.e., pseudovarieties characterized by some finite set of pseudoidentities, and yields decidability of membership in L, in FOLL (polynomial-size Boolean circuits of O(log log n) depth) and in qAC^0 (Boolean circuits of constant depth and quasi-polynomial size). In view of lower bounds from circuit complexity, we obtain a new technique to prove that a pseudovariety V is not finitely based: if membership in V is hard for PARITY, it cannot be defined in this extended logic and thus does not admit a finite basis. We show that membership to EA is L-complete, thereby improving previous complexity results and obtaining a new proof that the pseudovariety is not finitely based at the same time.","author":["Lukas Fleischer"],"primaryCategory":"cs.FL","category":["cs.FL","F.2.2; F.4.3"],"timestamp_ms":{"$numberLong":"1525270099000"},"text":"We consider the complexity of deciding membership of a given finite semigroup to a fixed pseudovariety. While it is known that there exist pseudovarieties with NP-complete or even undecidable membership problems, for many well-known pseudovarieties the problem is known to be decidable in polynomial time. We show that for many of these pseudovarieties, the membership problem is actually in AC^0. To this end, we show that these pseudovarieties can be characterized by first-order sentences with multiplication as the only predicate. We prove closure properties of the class of pseudovarieties with such first-order descriptions under various well-known operations; in particular, if V can be described by a first-order sentence, then DV, LV, and the Mal'cev products of K, D, N, LI, and LG with V are first-order definable as well. Moreover, if H is a first-order definable pseudovariety of finite groups, then the pseudovariety of all finite semigroups whose subgroups are in H is first-order definable. Our logical formalism can be extended by a predicate corresponding to the omega-operator in semigroups. The extended logic is powerful enough to capture all finitely based pseudovarieties, i.e., pseudovarieties characterized by some finite set of pseudoidentities, and yields decidability of membership in L, in FOLL (polynomial-size Boolean circuits of O(log log n) depth) and in qAC^0 (Boolean circuits of constant depth and quasi-polynomial size). In view of lower bounds from circuit complexity, we obtain a new technique to prove that a pseudovariety V is not finitely based: if membership in V is hard for PARITY, it cannot be defined in this extended logic and thus does not admit a finite basis. We show that membership to EA is L-complete, thereby improving previous complexity results and obtaining a new proof that the pseudovariety is not finitely based at the same time.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287da0"},"id":"arXiv:1805.00158v1","url":"http://arxiv.org/abs/1805.00158v1","updated":"2018-05-01T02:42:13Z","published":"2018-05-01T02:42:13Z","title":"Optimal Load-Balancing for High-Density Wireless Networks with\n  Flow-Level Dynamics","summary":"We consider the load-balancing design for forwarding incoming flows to access points (APs) in high-density wireless networks with both channel fading and flow-level dynamics, where each incoming flow has a certain amount of service demand and leaves the system once its service request is complete (referred as flow-level dynamic model). The efficient load-balancing design is strongly needed for supporting high-quality wireless connections in high-density areas. Despite the presence of a variety of earlier works on the design and analysis of the load-balancing schemes in wireless networks, there does not exist a work on the load-balancing design in the realistic flow-level dynamic model.   In this work, we propose a Join-the-Least-Workload (JLW) Algorithm that always forwards the incoming flows to the AP with the smallest workload in the presence of flow-level dynamics. However, our considered flow-level dynamic model differs from traditional queueing model for wireless networks in the following two aspects: (1) the dynamics of the flows is short-term and flows will leave the network once they received the desired amount of service; (2) each individual flow faces an independent channel fading. These differences pose significant challenges on the system performance analysis. To tackle these challenges, we perform Lyapunov-drift-based analysis of the stochastic network taking into account sharp flow-level dynamics. Our analysis reveals that our proposed JLW Algorithm not only achieves maximum system throughput, but also minimizes the total system workload in heavy-traffic regimes. Moreover, we observe from both our theoretical and simulation results that the mean total workload performance under the proposed JLW Algorithm does not degrade as the number of APs increases, which is strongly desirable in high-density wireless networks.","author":["Bin Li","Xiangqi Kong","Lei Wang"],"primaryCategory":"cs.NI","category":["cs.NI"],"timestamp_ms":{"$numberLong":"1525167733000"},"text":"We consider the load-balancing design for forwarding incoming flows to access points (APs) in high-density wireless networks with both channel fading and flow-level dynamics, where each incoming flow has a certain amount of service demand and leaves the system once its service request is complete (referred as flow-level dynamic model). The efficient load-balancing design is strongly needed for supporting high-quality wireless connections in high-density areas. Despite the presence of a variety of earlier works on the design and analysis of the load-balancing schemes in wireless networks, there does not exist a work on the load-balancing design in the realistic flow-level dynamic model.   In this work, we propose a Join-the-Least-Workload (JLW) Algorithm that always forwards the incoming flows to the AP with the smallest workload in the presence of flow-level dynamics. However, our considered flow-level dynamic model differs from traditional queueing model for wireless networks in the following two aspects: (1) the dynamics of the flows is short-term and flows will leave the network once they received the desired amount of service; (2) each individual flow faces an independent channel fading. These differences pose significant challenges on the system performance analysis. To tackle these challenges, we perform Lyapunov-drift-based analysis of the stochastic network taking into account sharp flow-level dynamics. Our analysis reveals that our proposed JLW Algorithm not only achieves maximum system throughput, but also minimizes the total system workload in heavy-traffic regimes. Moreover, we observe from both our theoretical and simulation results that the mean total workload performance under the proposed JLW Algorithm does not degrade as the number of APs increases, which is strongly desirable in high-density wireless networks.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d9f"},"id":"arXiv:1804.11002v1","url":"http://arxiv.org/abs/1804.11002v1","updated":"2018-04-29T23:14:29Z","published":"2018-04-29T23:14:29Z","title":"Precision Medicine as an Accelerator for Next Generation Cognitive\n  Supercomputing","summary":"In the past several years, we have taken advantage of a number of opportunities to advance the intersection of next generation high-performance computing AI and big data technologies through partnerships in precision medicine. Today we are in the throes of piecing together what is likely the most unique convergence of medical data and computer technologies. But more deeply, we observe that the traditional paradigm of computer simulation and prediction needs fundamental revision. This is the time for a number of reasons. We will review what the drivers are, why now, how this has been approached over the past several years, and where we are heading.","author":["Edmon Begoli","Jim Brase","Bambi DeLaRosa","Penelope Jones","Dimitri Kusnezov","Jason Paragas","Rick Stevens","Fred Streitz","Georgia Tourassi"],"primaryCategory":"cs.AI","category":["cs.AI","cs.CY","I.2.1; C.3"],"timestamp_ms":{"$numberLong":"1525068869000"},"text":"In the past several years, we have taken advantage of a number of opportunities to advance the intersection of next generation high-performance computing AI and big data technologies through partnerships in precision medicine. Today we are in the throes of piecing together what is likely the most unique convergence of medical data and computer technologies. But more deeply, we observe that the traditional paradigm of computer simulation and prediction needs fundamental revision. This is the time for a number of reasons. We will review what the drivers are, why now, how this has been approached over the past several years, and where we are heading.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d9e"},"id":"arXiv:1804.10731v1","url":"http://arxiv.org/abs/1804.10731v1","updated":"2018-04-28T03:29:22Z","published":"2018-04-28T03:29:22Z","title":"Sentiment Adaptive End-to-End Dialog Systems","summary":"End-to-end learning framework is useful for building dialog systems for its simplicity in training and efficiency in model updating. However, current end-to-end approaches only consider user semantic inputs in learning and under-utilize other user information. Therefore, we propose to include user sentiment obtained through multimodal information (acoustic, dialogic and textual), in the end-to-end learning framework to make systems more user-adaptive and effective. We incorporated user sentiment information in both supervised and reinforcement learning settings. In both settings, adding sentiment information reduced the dialog length and improved the task success rate on a bus information search task. This work is the first attempt to incorporate multimodal user information in the adaptive end-to-end dialog system training framework and attained state-of-the-art performance.","author":["Weiyan Shi","Zhou Yu"],"primaryCategory":"cs.CL","category":["cs.CL"],"timestamp_ms":{"$numberLong":"1524911362000"},"text":"End-to-end learning framework is useful for building dialog systems for its simplicity in training and efficiency in model updating. However, current end-to-end approaches only consider user semantic inputs in learning and under-utilize other user information. Therefore, we propose to include user sentiment obtained through multimodal information (acoustic, dialogic and textual), in the end-to-end learning framework to make systems more user-adaptive and effective. We incorporated user sentiment information in both supervised and reinforcement learning settings. In both settings, adding sentiment information reduced the dialog length and improved the task success rate on a bus information search task. This work is the first attempt to incorporate multimodal user information in the adaptive end-to-end dialog system training framework and attained state-of-the-art performance.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d9d"},"id":"arXiv:1805.00878v1","url":"http://arxiv.org/abs/1805.00878v1","updated":"2018-05-02T15:48:11Z","published":"2018-05-02T15:48:11Z","title":"Modelling tourism demand to Spain with machine learning techniques. The\n  impact of forecast horizon on model selection","summary":"This study assesses the influence of the forecast horizon on the forecasting performance of several machine learning techniques. We compare the fo recast accuracy of Support Vector Regression (SVR) to Neural Network (NN) models, using a linear model as a benchmark. We focus on international tourism demand to all seventeen regions of Spain. The SVR with a Gaussian radial basis function kernel outperforms the rest of the models for the longest forecast horizons. We also find that machine learning methods improve their forecasting accuracy with respect to linear models as forecast horizons increase. This result shows the suitability of SVR for medium and long term forecasting.","author":["Oscar Claveria","Enric Monte","Salvador Torra"],"primaryCategory":"stat.ML","category":["stat.ML","cs.LG"],"timestamp_ms":{"$numberLong":"1525301291000"},"text":"This study assesses the influence of the forecast horizon on the forecasting performance of several machine learning techniques. We compare the fo recast accuracy of Support Vector Regression (SVR) to Neural Network (NN) models, using a linear model as a benchmark. We focus on international tourism demand to all seventeen regions of Spain. The SVR with a Gaussian radial basis function kernel outperforms the rest of the models for the longest forecast horizons. We also find that machine learning methods improve their forecasting accuracy with respect to linear models as forecast horizons increase. This result shows the suitability of SVR for medium and long term forecasting.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d9c"},"id":"arXiv:1804.10776v1","url":"http://arxiv.org/abs/1804.10776v1","updated":"2018-04-28T09:50:21Z","published":"2018-04-28T09:50:21Z","title":"Multi Layered-Parallel Graph Convolutional Network (ML-PGCN) for Disease\n  Prediction","summary":"Structural data from Electronic Health Records as complementary information to imaging data for disease prediction. We incorporate novel weighting layer into the Graph Convolutional Networks, which weights every element of structural data by exploring its relation to the underlying disease. We demonstrate the superiority of our developed technique in terms of computational speed and obtained encouraging results where our method outperforms the state-of-the-art methods when applied to two publicly available datasets ABIDE and Chest X-ray in terms of relative performance for the accuracy of prediction by 5.31 % and 8.15 % and for the area under the ROC curve by 4.96 % and 10.36 % respectively. Additionally, the model is lightweight, fast and easily trainable.","author":["Anees Kazi","Shadi Albarqouni","Karsten Kortuem","Nassir Navab"],"primaryCategory":"cs.LG","category":["cs.LG","stat.ML"],"timestamp_ms":{"$numberLong":"1524934221000"},"text":"Structural data from Electronic Health Records as complementary information to imaging data for disease prediction. We incorporate novel weighting layer into the Graph Convolutional Networks, which weights every element of structural data by exploring its relation to the underlying disease. We demonstrate the superiority of our developed technique in terms of computational speed and obtained encouraging results where our method outperforms the state-of-the-art methods when applied to two publicly available datasets ABIDE and Chest X-ray in terms of relative performance for the accuracy of prediction by 5.31 % and 8.15 % and for the area under the ROC curve by 4.96 % and 10.36 % respectively. Additionally, the model is lightweight, fast and easily trainable.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d9b"},"id":"arXiv:1805.00089v1","url":"http://arxiv.org/abs/1805.00089v1","updated":"2018-04-30T20:34:16Z","published":"2018-04-30T20:34:16Z","title":"Concolic Testing for Deep Neural Networks","summary":"Concolic testing alternates between CONCrete program execution and symbOLIC analysis to explore the execution paths of a software program and to increase code coverage. In this paper, we develop the first concolic testing approach for Deep Neural Networks (DNNs). More specifically, we utilise quantified linear arithmetic over rationals to express test requirements that have been studied in the literature, and then develop a coherent method to perform concolic testing with the aim of better coverage. Our experimental results show the effectiveness of the concolic testing approach in both achieving high coverage and finding adversarial examples.","author":["Youcheng Sun","Min Wu","Wenjie Ruan","Xiaowei Huang","Marta Kwiatkowska","Daniel Kroening"],"primaryCategory":"cs.LG","category":["cs.LG","cs.SE","stat.ML"],"timestamp_ms":{"$numberLong":"1525145656000"},"text":"Concolic testing alternates between CONCrete program execution and symbOLIC analysis to explore the execution paths of a software program and to increase code coverage. In this paper, we develop the first concolic testing approach for Deep Neural Networks (DNNs). More specifically, we utilise quantified linear arithmetic over rationals to express test requirements that have been studied in the literature, and then develop a coherent method to perform concolic testing with the aim of better coverage. Our experimental results show the effectiveness of the concolic testing approach in both achieving high coverage and finding adversarial examples.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d9a"},"id":"arXiv:1804.09808v2","url":"http://arxiv.org/abs/1804.09808v2","updated":"2018-05-02T16:56:08Z","published":"2018-04-25T21:39:39Z","title":"Off the Beaten Track: Using Deep Learning to Interpolate Between Music\n  Genres","summary":"We describe a system based on deep learning that generates drum patterns in the electronic dance music domain. Experimental results reveal that generated patterns can be employed to produce musically sound and creative transitions between different genres, and that the process of generation is of interest to practitioners in the field.","author":["Tijn Borghuis","Alessandro Tibo","Simone Conforti","Luca Canciello","Lorenzo Brusci","Paolo Frasconi"],"primaryCategory":"cs.SD","category":["cs.SD","cs.LG","cs.MM","eess.AS"],"timestamp_ms":{"$numberLong":"1525305368000"},"text":"We describe a system based on deep learning that generates drum patterns in the electronic dance music domain. Experimental results reveal that generated patterns can be employed to produce musically sound and creative transitions between different genres, and that the process of generation is of interest to practitioners in the field.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d99"},"id":"arXiv:1805.00428v1","url":"http://arxiv.org/abs/1805.00428v1","updated":"2018-05-01T16:49:45Z","published":"2018-05-01T16:49:45Z","title":"Explore Recurrent Neural Network for PUE Attack Detection in Practical\n  CRN Models","summary":"The proliferation of the Internet of Things (IoTs) and pervasive use of many different types of mobile computing devices make wireless communication spectrum a precious resource. In order to accommodate the still fast increasing number of devices requesting wireless connection, more efficient, fine-grained spectrum allocation and sharing schemes are badly in need. Cognitive radio networks (CRNs) have been widely recognized as one promising solution, in which the secondary users (SUs) are allowed to share channels with licensed primary users (PUs) as long as bringing no interference to the normal operations of the PUs. However, malicious attackers or selfish SUs may mimic the behavior of PUs to occupy the channels illegally. It is nontrivial to accurately, timely detect such kind of primary user emulation (PUE) attacks. In this paper, an efficient PUE attacked detection method is introduced leveraging the recurrent neural network (RNN). After a fundamental algorithm using basic RNN, an advanced version taking advantage of long-short-term-memory (LSTM) is proposed, which is more efficient on processing time series with long term memory. The experimental study has provided deeper insights about the different performances the RNNs achieved and validated the effectiveness of the proposed detectors.","author":["Qi Dong","Yu Chen","Xiaohua Li","Kai Zeng"],"primaryCategory":"cs.NI","category":["cs.NI"],"timestamp_ms":{"$numberLong":"1525218585000"},"text":"The proliferation of the Internet of Things (IoTs) and pervasive use of many different types of mobile computing devices make wireless communication spectrum a precious resource. In order to accommodate the still fast increasing number of devices requesting wireless connection, more efficient, fine-grained spectrum allocation and sharing schemes are badly in need. Cognitive radio networks (CRNs) have been widely recognized as one promising solution, in which the secondary users (SUs) are allowed to share channels with licensed primary users (PUs) as long as bringing no interference to the normal operations of the PUs. However, malicious attackers or selfish SUs may mimic the behavior of PUs to occupy the channels illegally. It is nontrivial to accurately, timely detect such kind of primary user emulation (PUE) attacks. In this paper, an efficient PUE attacked detection method is introduced leveraging the recurrent neural network (RNN). After a fundamental algorithm using basic RNN, an advanced version taking advantage of long-short-term-memory (LSTM) is proposed, which is more efficient on processing time series with long term memory. The experimental study has provided deeper insights about the different performances the RNNs achieved and validated the effectiveness of the proposed detectors.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d98"},"id":"arXiv:1804.10752v1","url":"http://arxiv.org/abs/1804.10752v1","updated":"2018-04-28T06:54:11Z","published":"2018-04-28T06:54:11Z","title":"Syllable-Based Sequence-to-Sequence Speech Recognition with the\n  Transformer in Mandarin Chinese","summary":"Sequence-to-sequence attention-based models have recently shown very promising results on automatic speech recognition (ASR) tasks, which integrate an acoustic, pronunciation and language model into a single neural network. In these models, the Transformer, a new sequence-to-sequence attention-based model relying entirely on self-attention without using RNNs or convolutions, achieves a new single-model state-of-the-art BLEU on neural machine translation (NMT) tasks. Since the outstanding performance of the Transformer, we extend it to speech and concentrate on it as the basic architecture of sequence-to-sequence attention-based model on Mandarin Chinese ASR tasks. Furthermore, we investigate a comparison between syllable based model and context-independent phoneme (CI-phoneme) based model with the Transformer in Mandarin Chinese. Additionally, a greedy cascading decoder with the Transformer is proposed for mapping CI-phoneme sequences and syllable sequences into word sequences. Experiments on HKUST datasets demonstrate that syllable based model with the Transformer performs better than CI-phoneme based counterpart, and achieves a character error rate (CER) of \\emph{$28.77\\%$}, which is competitive to the state-of-the-art CER of $28.0\\%$ by the joint CTC-attention based encoder-decoder network.","author":["Shiyu Zhou","Linhao Dong","Shuang Xu","Bo Xu"],"primaryCategory":"eess.AS","category":["eess.AS","cs.CL","cs.SD"],"timestamp_ms":{"$numberLong":"1524923651000"},"text":"Sequence-to-sequence attention-based models have recently shown very promising results on automatic speech recognition (ASR) tasks, which integrate an acoustic, pronunciation and language model into a single neural network. In these models, the Transformer, a new sequence-to-sequence attention-based model relying entirely on self-attention without using RNNs or convolutions, achieves a new single-model state-of-the-art BLEU on neural machine translation (NMT) tasks. Since the outstanding performance of the Transformer, we extend it to speech and concentrate on it as the basic architecture of sequence-to-sequence attention-based model on Mandarin Chinese ASR tasks. Furthermore, we investigate a comparison between syllable based model and context-independent phoneme (CI-phoneme) based model with the Transformer in Mandarin Chinese. Additionally, a greedy cascading decoder with the Transformer is proposed for mapping CI-phoneme sequences and syllable sequences into word sequences. Experiments on HKUST datasets demonstrate that syllable based model with the Transformer performs better than CI-phoneme based counterpart, and achieves a character error rate (CER) of \\emph{$28.77\\%$}, which is competitive to the state-of-the-art CER of $28.0\\%$ by the joint CTC-attention based encoder-decoder network.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d97"},"id":"arXiv:1805.00632v1","url":"http://arxiv.org/abs/1805.00632v1","updated":"2018-05-02T05:28:46Z","published":"2018-05-02T05:28:46Z","title":"Interpretable Fully Convolutional Classification of Intrapapillary\n  Capillary Loops for Real-Time Detection of Early Squamous Neoplasia","summary":"In this work, we have concentrated our efforts on the interpretability of classification results coming from a fully convolutional neural network. Motivated by the classification of oesophageal tissue for real-time detection of early squamous neoplasia, the most frequent kind of oesophageal cancer in Asia, we present a new dataset and a novel deep learning method that by means of deep supervision and a newly introduced concept, the embedded Class Activation Map (eCAM), focuses on the interpretability of results as a design constraint of a convolutional network. We present a new approach to visualise attention that aims to give some insights on those areas of the oesophageal tissue that lead a network to conclude that the images belong to a particular class and compare them with those visual features employed by clinicians to produce a clinical diagnosis. In comparison to a baseline method which does not feature deep supervision but provides attention by grafting Class Activation Maps, we improve the F1-score from 87.3% to 92.7% and provide more detailed attention maps.","author":["Luis C. Garcia-Peraza-Herrera","Martin Everson","Wenqi Li","Inmanol Luengo","Lorenz Berger","Omer Ahmad","Laurence Lovat","Hsiu-Po Wang","Wen-Lun Wang","Rehan Haidry","Danail Stoyanov","Tom Vercauteren","Sebastien Ourselin"],"primaryCategory":"cs.CV","category":["cs.CV"],"timestamp_ms":{"$numberLong":"1525264126000"},"text":"In this work, we have concentrated our efforts on the interpretability of classification results coming from a fully convolutional neural network. Motivated by the classification of oesophageal tissue for real-time detection of early squamous neoplasia, the most frequent kind of oesophageal cancer in Asia, we present a new dataset and a novel deep learning method that by means of deep supervision and a newly introduced concept, the embedded Class Activation Map (eCAM), focuses on the interpretability of results as a design constraint of a convolutional network. We present a new approach to visualise attention that aims to give some insights on those areas of the oesophageal tissue that lead a network to conclude that the images belong to a particular class and compare them with those visual features employed by clinicians to produce a clinical diagnosis. In comparison to a baseline method which does not feature deep supervision but provides attention by grafting Class Activation Maps, we improve the F1-score from 87.3% to 92.7% and provide more detailed attention maps.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d96"},"id":"arXiv:1805.00676v1","url":"http://arxiv.org/abs/1805.00676v1","updated":"2018-05-02T08:47:38Z","published":"2018-05-02T08:47:38Z","title":"Text to Image Synthesis Using Generative Adversarial Networks","summary":"Generating images from natural language is one of the primary applications of recent conditional generative models. Besides testing our ability to model conditional, highly dimensional distributions, text to image synthesis has many exciting and practical applications such as photo editing or computer-aided content creation. Recent progress has been made using Generative Adversarial Networks (GANs). This material starts with a gentle introduction to these topics and discusses the existent state of the art models. Moreover, I propose Wasserstein GAN-CLS, a new model for conditional image generation based on the Wasserstein distance which offers guarantees of stability. Then, I show how the novel loss function of Wasserstein GAN-CLS can be used in a Conditional Progressive Growing GAN. In combination with the proposed loss, the model boosts by 7.07% the best Inception Score (on the Caltech birds dataset) of the models which use only the sentence-level visual semantics. The only model which performs better than the Conditional Wasserstein Progressive Growing GAN is the recently proposed AttnGAN which uses word-level visual semantics as well.","author":["Cristian Bodnar"],"primaryCategory":"cs.CV","category":["cs.CV","cs.CL"],"timestamp_ms":{"$numberLong":"1525276058000"},"text":"Generating images from natural language is one of the primary applications of recent conditional generative models. Besides testing our ability to model conditional, highly dimensional distributions, text to image synthesis has many exciting and practical applications such as photo editing or computer-aided content creation. Recent progress has been made using Generative Adversarial Networks (GANs). This material starts with a gentle introduction to these topics and discusses the existent state of the art models. Moreover, I propose Wasserstein GAN-CLS, a new model for conditional image generation based on the Wasserstein distance which offers guarantees of stability. Then, I show how the novel loss function of Wasserstein GAN-CLS can be used in a Conditional Progressive Growing GAN. In combination with the proposed loss, the model boosts by 7.07% the best Inception Score (on the Caltech birds dataset) of the models which use only the sentence-level visual semantics. The only model which performs better than the Conditional Wasserstein Progressive Growing GAN is the recently proposed AttnGAN which uses word-level visual semantics as well.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d95"},"id":"arXiv:1804.10975v1","url":"http://arxiv.org/abs/1804.10975v1","updated":"2018-04-29T18:33:21Z","published":"2018-04-29T18:33:21Z","title":"Matryoshka Networks: Predicting 3D Geometry via Nested Shape Layers","summary":"In this paper, we develop novel, efficient 2D encodings for 3D geometry, which enable reconstructing full 3D shapes from a single image at high resolution. The key idea is to pose 3D shape reconstruction as a 2D prediction problem. To that end, we first develop a simple baseline network that predicts entire voxel tubes at each pixel of a reference view. By leveraging well-proven architectures for 2D pixel-prediction tasks, we attain state-of-the-art results, clearly outperforming purely voxel-based approaches. We scale this baseline to higher resolutions by proposing a memory-efficient shape encoding, which recursively decomposes a 3D shape into nested shape layers, similar to the pieces of a Matryoshka doll. This allows reconstructing highly detailed shapes with complex topology, as demonstrated in extensive experiments; we clearly outperform previous octree-based approaches despite having a much simpler architecture using standard network components. Our Matryoshka networks further enable reconstructing shapes from IDs or shape similarity, as well as shape sampling.","author":["Stephan R. Richter","Stefan Roth"],"primaryCategory":"cs.CV","category":["cs.CV","I.4.8"],"timestamp_ms":{"$numberLong":"1525052001000"},"text":"In this paper, we develop novel, efficient 2D encodings for 3D geometry, which enable reconstructing full 3D shapes from a single image at high resolution. The key idea is to pose 3D shape reconstruction as a 2D prediction problem. To that end, we first develop a simple baseline network that predicts entire voxel tubes at each pixel of a reference view. By leveraging well-proven architectures for 2D pixel-prediction tasks, we attain state-of-the-art results, clearly outperforming purely voxel-based approaches. We scale this baseline to higher resolutions by proposing a memory-efficient shape encoding, which recursively decomposes a 3D shape into nested shape layers, similar to the pieces of a Matryoshka doll. This allows reconstructing highly detailed shapes with complex topology, as demonstrated in extensive experiments; we clearly outperform previous octree-based approaches despite having a much simpler architecture using standard network components. Our Matryoshka networks further enable reconstructing shapes from IDs or shape similarity, as well as shape sampling.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d94"},"id":"arXiv:1804.10707v1","url":"http://arxiv.org/abs/1804.10707v1","updated":"2018-04-27T22:42:45Z","published":"2018-04-27T22:42:45Z","title":"Secure Remote Credential Management with Mutual Attestation for\n  Constrained Sensing Platforms with TEEs","summary":"Trusted Execution Environments (TEEs) are rapidly emerging as the go-to root of trust for protecting sensitive applications and data using hardware-backed isolated worlds of execution -- surpassing related initiatives, such as Secure Elements, for constrained devices. TEEs are envisaged to provide sensitive IoT deployments with robust assurances regarding critical algorithm execution, tamper-resistant credential storage, and platform integrity via remote attestation. However, the challenge of remotely managing credentials between TEEs remains largely unaddressed in existing literature. Here, credentials must remain protected against untrusted system elements and transmitted over a secure channel with bi-directional trust assurances of their authenticity and operating states. In this paper, we present novel protocols for four key areas of remote TEE credential management using mutual attestation: backups, updates, migration, and revocation. The proposed protocols are agnostic to the TEE implementation and network architecture, developed in line with the requirements and threat model of IoT TEEs, and subjected to formal symbolic verification using Scyther, which found no attacks.","author":["Carlton Shepherd","Raja N. Akram","Konstantinos Markantonakis"],"primaryCategory":"cs.CR","category":["cs.CR"],"timestamp_ms":{"$numberLong":"1524894165000"},"text":"Trusted Execution Environments (TEEs) are rapidly emerging as the go-to root of trust for protecting sensitive applications and data using hardware-backed isolated worlds of execution -- surpassing related initiatives, such as Secure Elements, for constrained devices. TEEs are envisaged to provide sensitive IoT deployments with robust assurances regarding critical algorithm execution, tamper-resistant credential storage, and platform integrity via remote attestation. However, the challenge of remotely managing credentials between TEEs remains largely unaddressed in existing literature. Here, credentials must remain protected against untrusted system elements and transmitted over a secure channel with bi-directional trust assurances of their authenticity and operating states. In this paper, we present novel protocols for four key areas of remote TEE credential management using mutual attestation: backups, updates, migration, and revocation. The proposed protocols are agnostic to the TEE implementation and network architecture, developed in line with the requirements and threat model of IoT TEEs, and subjected to formal symbolic verification using Scyther, which found no attacks.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d93"},"id":"arXiv:1804.11259v1","url":"http://arxiv.org/abs/1804.11259v1","updated":"2018-04-30T15:06:35Z","published":"2018-04-30T15:06:35Z","title":"Interpreting weight maps in terms of cognitive or clinical neuroscience:\n  nonsense?","summary":"Since machine learning models have been applied to neuroimaging data, researchers have drawn conclusions from the derived weight maps. In particular, weight maps of classifiers between two conditions are often described as a proxy for the underlying signal differences between the conditions. Recent studies have however suggested that such weight maps could not reliably recover the source of the neural signals and even led to false positives (FP). In this work, we used semi-simulated data from ElectroCorticoGraphy (ECoG) to investigate how the signal-to-noise ratio and sparsity of the neural signal affect the similarity between signal and weights. We show that not all cases produce FP and that it is unlikely for FP features to have a high weight in most cases.","author":["Jessica Schrouff","Janaina Mourao-Miranda"],"primaryCategory":"stat.ML","category":["stat.ML","cs.LG"],"timestamp_ms":{"$numberLong":"1525125995000"},"text":"Since machine learning models have been applied to neuroimaging data, researchers have drawn conclusions from the derived weight maps. In particular, weight maps of classifiers between two conditions are often described as a proxy for the underlying signal differences between the conditions. Recent studies have however suggested that such weight maps could not reliably recover the source of the neural signals and even led to false positives (FP). In this work, we used semi-simulated data from ElectroCorticoGraphy (ECoG) to investigate how the signal-to-noise ratio and sparsity of the neural signal affect the similarity between signal and weights. We show that not all cases produce FP and that it is unlikely for FP features to have a high weight in most cases.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d92"},"id":"arXiv:1805.00858v1","url":"http://arxiv.org/abs/1805.00858v1","updated":"2018-05-02T15:08:53Z","published":"2018-05-02T15:08:53Z","title":"Maximum cuts in edge-colored graphs","summary":"The input of the Maximum Colored Cut problem consists of a graph $G=(V,E)$ with an edge-coloring $c:E\\to \\{1,2,3,\\ldots , p\\}$ and a positive integer $k$, and the question is whether $G$ has a nontrivial edge cut using at least $k$ colors. The Colorful Cut problem has the same input but asks for a nontrivial edge cut using all $p$ colors. Unlike what happens for the classical Maximum Cut problem, we prove that both problems are NP-complete even on complete, planar, or bounded treewidth graphs. Furthermore, we prove that Colorful Cut is NP-complete even when each color class induces a clique of size at most 3, but is trivially solvable when each color induces a $K_2$. On the positive side, we prove that Maximum Colored Cut is fixed-parameter tractable when parameterized by either $k$ or $p$, by constructing a cubic kernel in both cases.","author":["Luerbio Faria","Sulamita Klein","Ignasi Sau","Uéverton S. Souza","Rubens Sucupira"],"primaryCategory":"cs.DS","category":["cs.DS","cs.CG","cs.DM","math.CO","05C85","G.2.2; F.2.2"],"timestamp_ms":{"$numberLong":"1525298933000"},"text":"The input of the Maximum Colored Cut problem consists of a graph $G=(V,E)$ with an edge-coloring $c:E\\to \\{1,2,3,\\ldots , p\\}$ and a positive integer $k$, and the question is whether $G$ has a nontrivial edge cut using at least $k$ colors. The Colorful Cut problem has the same input but asks for a nontrivial edge cut using all $p$ colors. Unlike what happens for the classical Maximum Cut problem, we prove that both problems are NP-complete even on complete, planar, or bounded treewidth graphs. Furthermore, we prove that Colorful Cut is NP-complete even when each color class induces a clique of size at most 3, but is trivially solvable when each color induces a $K_2$. On the positive side, we prove that Maximum Colored Cut is fixed-parameter tractable when parameterized by either $k$ or $p$, by constructing a cubic kernel in both cases.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d91"},"id":"arXiv:1804.11040v1","url":"http://arxiv.org/abs/1804.11040v1","updated":"2018-04-30T03:58:05Z","published":"2018-04-30T03:58:05Z","title":"A Memory Controller with Row Buffer Locality Awareness for Hybrid Memory\n  Systems","summary":"Non-volatile memory (NVM) is a class of promising scalable memory technologies that can potentially offer higher capacity than DRAM at the same cost point. Unfortunately, the access latency and energy of NVM is often higher than those of DRAM, while the endurance of NVM is lower. Many DRAM-NVM hybrid memory systems use DRAM as a cache to NVM, to achieve the low access latency, low energy, and high endurance of DRAM, while taking advantage of the large capacity of NVM. A key question for a hybrid memory system is what data to cache in DRAM to best exploit the advantages of each technology while avoiding the disadvantages of each technology as much as possible.   We propose a new memory controller design that improves hybrid memory performance and energy efficiency. We observe that both DRAM and NVM banks employ row buffers that act as a cache for the most recently accessed memory row. Accesses that are row buffer hits incur similar latencies (and energy consumption) in both DRAM and NVM, whereas accesses that are row buffer misses incur longer latencies (and higher energy consumption) in NVM than in DRAM. To exploit this, we devise a policy that caches heavily-reused data that frequently misses in the NVM row buffers into DRAM. Our policy tracks the row buffer miss counts of recently-used rows in NVM, and caches in DRAM the rows that are predicted to incur frequent row buffer misses. Our proposed policy also takes into account the high write latencies of NVM, in addition to row buffer locality and more likely places the write-intensive pages in DRAM instead of NVM.","author":["HanBin Yoon","Justin Meza","Rachata Ausavarungnirun","Rachael A. Harding","Onur Mutlu"],"primaryCategory":"cs.AR","category":["cs.AR"],"timestamp_ms":{"$numberLong":"1525085885000"},"text":"Non-volatile memory (NVM) is a class of promising scalable memory technologies that can potentially offer higher capacity than DRAM at the same cost point. Unfortunately, the access latency and energy of NVM is often higher than those of DRAM, while the endurance of NVM is lower. Many DRAM-NVM hybrid memory systems use DRAM as a cache to NVM, to achieve the low access latency, low energy, and high endurance of DRAM, while taking advantage of the large capacity of NVM. A key question for a hybrid memory system is what data to cache in DRAM to best exploit the advantages of each technology while avoiding the disadvantages of each technology as much as possible.   We propose a new memory controller design that improves hybrid memory performance and energy efficiency. We observe that both DRAM and NVM banks employ row buffers that act as a cache for the most recently accessed memory row. Accesses that are row buffer hits incur similar latencies (and energy consumption) in both DRAM and NVM, whereas accesses that are row buffer misses incur longer latencies (and higher energy consumption) in NVM than in DRAM. To exploit this, we devise a policy that caches heavily-reused data that frequently misses in the NVM row buffers into DRAM. Our policy tracks the row buffer miss counts of recently-used rows in NVM, and caches in DRAM the rows that are predicted to incur frequent row buffer misses. Our proposed policy also takes into account the high write latencies of NVM, in addition to row buffer locality and more likely places the write-intensive pages in DRAM instead of NVM.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d90"},"id":"arXiv:1804.10934v1","url":"http://arxiv.org/abs/1804.10934v1","updated":"2018-04-29T14:00:07Z","published":"2018-04-29T14:00:07Z","title":"A Spatial Basis Coverage Approach For Uplink Training And Scheduling In\n  Massive MIMO Systems","summary":"Massive multiple-input multiple-output (massive MIMO) can provide large spectral and energy efficiency gains. Nevertheless, its potential is conditioned on acquiring accurate channel state information (CSI). In time division duplexing (TDD) systems, CSI is obtained through uplink training which is hindered by pilot contamination. The impact of this phenomenon can be relieved using spatial division multiplexing, which refers to partitioning users based on their spatial information and processing their signals accordingly. The performance of such schemes depend primarily on the implemented grouping method. In this paper, we propose a novel spatial grouping scheme that aims at managing pilot contamination while reducing the required training overhead in TDD massive MIMO. Herein, user specific decoding matrices are derived based on the columns of the discrete Fourier transform matrix (DFT), taken as a spatial basis. Copilot user groups are then formed in order to obtain the best coverage of the spatial basis with minimum overlapping between decoding matrices. We provide two algorithms that achieve the desired grouping and derive their respective performance guarantees. We also address inter-cell copilot interference through efficient pilot sequence allocation, leveraging the formed copilot groups. Various numerical results are provided to showcase the efficiency of the proposed algorithms.","author":["Salah Eddine Hajri","Mohamad Assaad"],"primaryCategory":"cs.IT","category":["cs.IT","math.IT"],"timestamp_ms":{"$numberLong":"1525035607000"},"text":"Massive multiple-input multiple-output (massive MIMO) can provide large spectral and energy efficiency gains. Nevertheless, its potential is conditioned on acquiring accurate channel state information (CSI). In time division duplexing (TDD) systems, CSI is obtained through uplink training which is hindered by pilot contamination. The impact of this phenomenon can be relieved using spatial division multiplexing, which refers to partitioning users based on their spatial information and processing their signals accordingly. The performance of such schemes depend primarily on the implemented grouping method. In this paper, we propose a novel spatial grouping scheme that aims at managing pilot contamination while reducing the required training overhead in TDD massive MIMO. Herein, user specific decoding matrices are derived based on the columns of the discrete Fourier transform matrix (DFT), taken as a spatial basis. Copilot user groups are then formed in order to obtain the best coverage of the spatial basis with minimum overlapping between decoding matrices. We provide two algorithms that achieve the desired grouping and derive their respective performance guarantees. We also address inter-cell copilot interference through efficient pilot sequence allocation, leveraging the formed copilot groups. Various numerical results are provided to showcase the efficiency of the proposed algorithms.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d8f"},"id":"arXiv:1804.10023v1","url":"http://arxiv.org/abs/1804.10023v1","updated":"2018-04-26T12:47:52Z","published":"2018-04-26T12:47:52Z","title":"Weak Labeling for Crowd Learning","summary":"Crowdsourcing has become very popular among the machine learning community as a way to obtain labels that allow a ground truth to be estimated for a given dataset. In most of the approaches that use crowdsourced labels, annotators are asked to provide, for each presented instance, a single class label. Such a request could be inefficient, that is, considering that the labelers may not be experts, that way to proceed could fail to take real advantage of the knowledge of the labelers. In this paper, the use of weak labeling for crowd learning is proposed, where the annotators may provide more than a single label per instance to try not to miss the real label. The main hypothesis is that, by allowing weak labeling, knowledge can be extracted from the labelers more efficiently by than in the standard crowd learning scenario. Empirical evidence which supports that hypothesis is presented.","author":["Iker Beñaran-Muñoz","Jerónimo Hernández-González","Aritz Pérez"],"primaryCategory":"stat.ML","category":["stat.ML","cs.LG","stat.ML"],"timestamp_ms":{"$numberLong":"1524772072000"},"text":"Crowdsourcing has become very popular among the machine learning community as a way to obtain labels that allow a ground truth to be estimated for a given dataset. In most of the approaches that use crowdsourced labels, annotators are asked to provide, for each presented instance, a single class label. Such a request could be inefficient, that is, considering that the labelers may not be experts, that way to proceed could fail to take real advantage of the knowledge of the labelers. In this paper, the use of weak labeling for crowd learning is proposed, where the annotators may provide more than a single label per instance to try not to miss the real label. The main hypothesis is that, by allowing weak labeling, knowledge can be extracted from the labelers more efficiently by than in the standard crowd learning scenario. Empirical evidence which supports that hypothesis is presented.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d8e"},"id":"arXiv:1804.09764v1","url":"http://arxiv.org/abs/1804.09764v1","updated":"2018-04-25T19:25:14Z","published":"2018-04-25T19:25:14Z","title":"High-Performance Massive Subgraph Counting using Pipelined\n  Adaptive-Group Communication","summary":"Subgraph counting aims to count the number of occurrences of a subgraph T (aka as a template) in a given graph G. The basic problem has found applications in diverse domains. The problem is known to be computationally challenging - the complexity grows both as a function of T and G. Recent applications have motivated solving such problems on massive networks with billions of vertices. In this chapter, we study the subgraph counting problem from a parallel computing perspective. We discuss efficient parallel algorithms for approximately resolving subgraph counting problems by using the color-coding technique. We then present several system-level strategies to substantially improve the overall performance of the algorithm in massive subgraph counting problems. We propose: 1) a novel pipelined Adaptive-Group communication pattern to improve inter-node scalability, 2) a fine-grained pipeline design to effectively reduce the memory space of intermediate results, 3) partitioning neighbor lists of subgraph vertices to achieve better thread concurrency and workload balance. Experimentation on an Intel Xeon E5 cluster shows that our implementation achieves 5x speedup of performance compared to the state-of-the-art work while reduces the peak memory utilization by a factor of 2 on large templates of 12 to 15 vertices and input graphs of 2 to 5 billions of edges.","author":["Langshi Chen","Bo Peng","Sabra Ossen","Anil Vullikanti","Madhav Marathe","Lei Jiang","Judy Qiu"],"primaryCategory":"cs.DC","category":["cs.DC"],"timestamp_ms":{"$numberLong":"1524709514000"},"text":"Subgraph counting aims to count the number of occurrences of a subgraph T (aka as a template) in a given graph G. The basic problem has found applications in diverse domains. The problem is known to be computationally challenging - the complexity grows both as a function of T and G. Recent applications have motivated solving such problems on massive networks with billions of vertices. In this chapter, we study the subgraph counting problem from a parallel computing perspective. We discuss efficient parallel algorithms for approximately resolving subgraph counting problems by using the color-coding technique. We then present several system-level strategies to substantially improve the overall performance of the algorithm in massive subgraph counting problems. We propose: 1) a novel pipelined Adaptive-Group communication pattern to improve inter-node scalability, 2) a fine-grained pipeline design to effectively reduce the memory space of intermediate results, 3) partitioning neighbor lists of subgraph vertices to achieve better thread concurrency and workload balance. Experimentation on an Intel Xeon E5 cluster shows that our implementation achieves 5x speedup of performance compared to the state-of-the-art work while reduces the peak memory utilization by a factor of 2 on large templates of 12 to 15 vertices and input graphs of 2 to 5 billions of edges.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d8d"},"id":"arXiv:1804.09851v1","url":"http://arxiv.org/abs/1804.09851v1","updated":"2018-04-26T01:43:24Z","published":"2018-04-26T01:43:24Z","title":"Capturing Capacity and Profit Gains with Base Station Sharing in mmWave\n  Cellular Networks","summary":"Due to the greater path loss, shadowing, and increased effect of blockage in millimeter wave cellular networks, base station sharing among network service providers has the potential to significantly improve overall network capacity. However, a service provider may find that despite the technical gains, sharing actually reduces its profits because it makes price competition between service providers tougher. In this work, a weighted scheduling algorithm is described, which gives greater control over how the airtime resource is allocated within a shared cell. It is shown that, under certain market conditions, there exist scheduling weights such that base station sharing is more profitable than not sharing for both service providers in a duopoly market, while still achieving almost as much network capacity as in a conventional base station sharing scenario. Thus, both technical and economic benefits of base station sharing may be realized simultaneously.","author":["Shahram Shahsavari","Fraida Fund","Elza Erkip","Shivendra S Panwar"],"primaryCategory":"cs.IT","category":["cs.IT","math.IT"],"timestamp_ms":{"$numberLong":"1524732204000"},"text":"Due to the greater path loss, shadowing, and increased effect of blockage in millimeter wave cellular networks, base station sharing among network service providers has the potential to significantly improve overall network capacity. However, a service provider may find that despite the technical gains, sharing actually reduces its profits because it makes price competition between service providers tougher. In this work, a weighted scheduling algorithm is described, which gives greater control over how the airtime resource is allocated within a shared cell. It is shown that, under certain market conditions, there exist scheduling weights such that base station sharing is more profitable than not sharing for both service providers in a duopoly market, while still achieving almost as much network capacity as in a conventional base station sharing scenario. Thus, both technical and economic benefits of base station sharing may be realized simultaneously.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d8c"},"id":"arXiv:1805.00316v1","url":"http://arxiv.org/abs/1805.00316v1","updated":"2018-05-01T13:17:39Z","published":"2018-05-01T13:17:39Z","title":"Versatile Auxiliary Classifier + Generative Adversarial Network\n  (VAC+GAN); Training Conditional Generators","summary":"One of the most interesting challenges in Artificial Intelligence is to train conditional generators which are able to provide labeled fake samples drawn from a specific distribution. In this work, a new framework is presented to train a deep conditional generator by placing a classifier in parallel with the discriminator and back propagate the classification error through the generator network. The method is versatile and is applicable to any variations of Generative Adversarial Network (GAN) implementation, and also is giving superior results compare to similar methods.","author":["Shabab Bazrafkan","Hossein Javidnia","Peter Corcoran"],"primaryCategory":"eess.IV","category":["eess.IV","cs.CV","cs.LG"],"timestamp_ms":{"$numberLong":"1525205859000"},"text":"One of the most interesting challenges in Artificial Intelligence is to train conditional generators which are able to provide labeled fake samples drawn from a specific distribution. In this work, a new framework is presented to train a deep conditional generator by placing a classifier in parallel with the discriminator and back propagate the classification error through the generator network. The method is versatile and is applicable to any variations of Generative Adversarial Network (GAN) implementation, and also is giving superior results compare to similar methods.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d8b"},"id":"arXiv:1804.10173v1","url":"http://arxiv.org/abs/1804.10173v1","updated":"2018-04-26T17:01:21Z","published":"2018-04-26T17:01:21Z","title":"Efficient and adaptive parameterized algorithms on modular\n  decompositions","summary":"We study the influence of a graph parameter called modular-width on the time complexity for optimally solving well-known polynomial problems such as Maximum Matching, Triangle Counting, and Maximum $s$-$t$ Vertex-Capacitated Flow. The modular-width of a graph depends on its (unique) modular decomposition tree, and can be computed in linear time $O(n+m)$ for graphs with $n$ vertices and $m$ edges. Modular decompositions are an important tool for graph algorithms, e.g., for linear-time recognition of certain graph classes. Throughout, we obtain efficient parameterized algorithms of running times $O(f(mw)n+m)$, $O(n+f(mw)m)$ , or $O(f(mw)+n+m)$ for graphs of modular-width $mw$. Our algorithm for Maximum Matching, running in time $O(mw^2\\log mw \\cdot n+m)$, is both faster and simpler than the recent $O(mw^4n+m)$ time algorithm of Coudert et al. (SODA 2018). For several other problems, e.g., Triangle Counting and Maximum $b$-Matching, we give adaptive algorithms, meaning that their running times match the best unparameterized algorithms for worst-case modular-width of $mw=\\Theta(n)$ and they outperform them already for $mw=o(n)$, until reaching linear time for $mw=O(1)$.","author":["Stefan Kratsch","Florian Nelles"],"primaryCategory":"cs.DS","category":["cs.DS"],"timestamp_ms":{"$numberLong":"1524787281000"},"text":"We study the influence of a graph parameter called modular-width on the time complexity for optimally solving well-known polynomial problems such as Maximum Matching, Triangle Counting, and Maximum $s$-$t$ Vertex-Capacitated Flow. The modular-width of a graph depends on its (unique) modular decomposition tree, and can be computed in linear time $O(n+m)$ for graphs with $n$ vertices and $m$ edges. Modular decompositions are an important tool for graph algorithms, e.g., for linear-time recognition of certain graph classes. Throughout, we obtain efficient parameterized algorithms of running times $O(f(mw)n+m)$, $O(n+f(mw)m)$ , or $O(f(mw)+n+m)$ for graphs of modular-width $mw$. Our algorithm for Maximum Matching, running in time $O(mw^2\\log mw \\cdot n+m)$, is both faster and simpler than the recent $O(mw^4n+m)$ time algorithm of Coudert et al. (SODA 2018). For several other problems, e.g., Triangle Counting and Maximum $b$-Matching, we give adaptive algorithms, meaning that their running times match the best unparameterized algorithms for worst-case modular-width of $mw=\\Theta(n)$ and they outperform them already for $mw=o(n)$, until reaching linear time for $mw=O(1)$.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d8a"},"id":"arXiv:1804.11146v1","url":"http://arxiv.org/abs/1804.11146v1","updated":"2018-04-30T12:14:32Z","published":"2018-04-30T12:14:32Z","title":"Cross-Modal Retrieval in the Cooking Context: Learning Semantic\n  Text-Image Embeddings","summary":"Designing powerful tools that support cooking activities has rapidly gained popularity due to the massive amounts of available data, as well as recent advances in machine learning that are capable of analyzing them. In this paper, we propose a cross-modal retrieval model aligning visual and textual data (like pictures of dishes and their recipes) in a shared representation space. We describe an effective learning scheme, capable of tackling large-scale problems, and validate it on the Recipe1M dataset containing nearly 1 million picture-recipe pairs. We show the effectiveness of our approach regarding previous state-of-the-art models and present qualitative results over computational cooking use cases.","author":["Micael Carvalho","Rémi Cadène","David Picard","Laure Soulier","Nicolas Thome","Matthieu Cord"],"primaryCategory":"cs.CL","category":["cs.CL","cs.CV","cs.IR"],"timestamp_ms":{"$numberLong":"1525115672000"},"text":"Designing powerful tools that support cooking activities has rapidly gained popularity due to the massive amounts of available data, as well as recent advances in machine learning that are capable of analyzing them. In this paper, we propose a cross-modal retrieval model aligning visual and textual data (like pictures of dishes and their recipes) in a shared representation space. We describe an effective learning scheme, capable of tackling large-scale problems, and validate it on the Recipe1M dataset containing nearly 1 million picture-recipe pairs. We show the effectiveness of our approach regarding previous state-of-the-art models and present qualitative results over computational cooking use cases.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d89"},"id":"arXiv:1804.11182v1","url":"http://arxiv.org/abs/1804.11182v1","updated":"2018-04-30T13:38:31Z","published":"2018-04-30T13:38:31Z","title":"Sketch-a-Classifier: Sketch-based Photo Classifier Generation","summary":"Contemporary deep learning techniques have made image recognition a reasonably reliable technology. However training effective photo classifiers typically takes numerous examples which limits image recognition's scalability and applicability to scenarios where images may not be available. This has motivated investigation into zero-shot learning, which addresses the issue via knowledge transfer from other modalities such as text. In this paper we investigate an alternative approach of synthesizing image classifiers: almost directly from a user's imagination, via free-hand sketch. This approach doesn't require the category to be nameable or describable via attributes as per zero-shot learning. We achieve this via training a {model regression} network to map from {free-hand sketch} space to the space of photo classifiers. It turns out that this mapping can be learned in a category-agnostic way, allowing photo classifiers for new categories to be synthesized by user with no need for annotated training photos. {We also demonstrate that this modality of classifier generation can also be used to enhance the granularity of an existing photo classifier, or as a complement to name-based zero-shot learning.","author":["Conghui Hu","Da Li","Yi-Zhe Song","Tao Xiang","Timothy M. Hospedales"],"primaryCategory":"cs.CV","category":["cs.CV"],"timestamp_ms":{"$numberLong":"1525120711000"},"text":"Contemporary deep learning techniques have made image recognition a reasonably reliable technology. However training effective photo classifiers typically takes numerous examples which limits image recognition's scalability and applicability to scenarios where images may not be available. This has motivated investigation into zero-shot learning, which addresses the issue via knowledge transfer from other modalities such as text. In this paper we investigate an alternative approach of synthesizing image classifiers: almost directly from a user's imagination, via free-hand sketch. This approach doesn't require the category to be nameable or describable via attributes as per zero-shot learning. We achieve this via training a {model regression} network to map from {free-hand sketch} space to the space of photo classifiers. It turns out that this mapping can be learned in a category-agnostic way, allowing photo classifiers for new categories to be synthesized by user with no need for annotated training photos. {We also demonstrate that this modality of classifier generation can also be used to enhance the granularity of an existing photo classifier, or as a complement to name-based zero-shot learning.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d88"},"id":"arXiv:1804.10726v1","url":"http://arxiv.org/abs/1804.10726v1","updated":"2018-04-28T02:58:00Z","published":"2018-04-28T02:58:00Z","title":"QDR-Tree: An Efcient Index Scheme for Complex Spatial Keyword Query","summary":"With the popularity of mobile devices and the development of geo-positioning technology, location-based services (LBS) attract much attention and top-k spatial keyword queries become increasingly complex. It is common to see that clients issue a query to find a restaurant serving pizza and steak, low in price and noise level particularly. However, most of prior works focused only on the spatial keyword while ignoring these independent numerical attributes. In this paper we demonstrate, for the first time, the Attributes-Aware Spatial Keyword Query (ASKQ), and devise a two-layer hybrid index structure called Quad-cluster Dual-filtering R-Tree (QDR-Tree). In the keyword cluster layer, a Quad-Cluster Tree (QC-Tree) is built based on the hierarchical clustering algorithm using kernel k-means to classify keywords. In the spatial layer, for each leaf node of the QC-Tree, we attach a Dual-Filtering R-Tree (DR-Tree) with two filtering algorithms, namely, keyword bitmap-based and attributes skyline-based filtering. Accordingly, efficient query processing algorithms are proposed. Through theoretical analysis, we have verified the optimization both in processing time and space consumption. Finally, massive experiments with real-data demonstrate the efficiency and effectiveness of QDR-Tree.","author":["Xinshi Zang","Peiwen Hao","Xiaofeng Gao","Bin Yao","Guihai Chen"],"primaryCategory":"cs.DS","category":["cs.DS","cs.DB"],"timestamp_ms":{"$numberLong":"1524909480000"},"text":"With the popularity of mobile devices and the development of geo-positioning technology, location-based services (LBS) attract much attention and top-k spatial keyword queries become increasingly complex. It is common to see that clients issue a query to find a restaurant serving pizza and steak, low in price and noise level particularly. However, most of prior works focused only on the spatial keyword while ignoring these independent numerical attributes. In this paper we demonstrate, for the first time, the Attributes-Aware Spatial Keyword Query (ASKQ), and devise a two-layer hybrid index structure called Quad-cluster Dual-filtering R-Tree (QDR-Tree). In the keyword cluster layer, a Quad-Cluster Tree (QC-Tree) is built based on the hierarchical clustering algorithm using kernel k-means to classify keywords. In the spatial layer, for each leaf node of the QC-Tree, we attach a Dual-Filtering R-Tree (DR-Tree) with two filtering algorithms, namely, keyword bitmap-based and attributes skyline-based filtering. Accordingly, efficient query processing algorithms are proposed. Through theoretical analysis, we have verified the optimization both in processing time and space consumption. Finally, massive experiments with real-data demonstrate the efficiency and effectiveness of QDR-Tree.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d87"},"id":"arXiv:1804.10994v1","url":"http://arxiv.org/abs/1804.10994v1","updated":"2018-04-29T21:49:05Z","published":"2018-04-29T21:49:05Z","title":"Transmission Capacity of Full-Duplex MIMO Ad-Hoc Network with Limited\n  Self-Interference Cancellation","summary":"In this paper, we propose a joint transceiver beamforming design to mitigate self-interference and inter-node interference simultaneously for full-duplex multiple-input and multiple-output ad-hoc network, and derive the transmission capacity upper bound (TC-UB) for the considered network. Condition on a specified transceiver antenna's configuration, we allow the self-interference effect to be cancelled at transmitter side, and offer an additional degree-of-freedom at receiver side for more inter-node interference cancellation. In addition, due to the imperfect self-interference channel estimation, the derivation of TC-UB needs to take the channel estimation error into account. In this case, the conventional method to obtain the TC-UB is not applicable. This motivates us to exploit the dominating interferer region and then utilize Newton-Raphson method to iteratively formulate the TC-UB. The results show that the derived TC-UB is quite close to the actual one especially when the number of receive-antenna is small. Moreover, our proposed beamforming design outperforms the existing beamforming strategies.","author":["Jiancao Hou","Mohammad Shikh-Bahaei"],"primaryCategory":"cs.IT","category":["cs.IT","math.IT"],"timestamp_ms":{"$numberLong":"1525063745000"},"text":"In this paper, we propose a joint transceiver beamforming design to mitigate self-interference and inter-node interference simultaneously for full-duplex multiple-input and multiple-output ad-hoc network, and derive the transmission capacity upper bound (TC-UB) for the considered network. Condition on a specified transceiver antenna's configuration, we allow the self-interference effect to be cancelled at transmitter side, and offer an additional degree-of-freedom at receiver side for more inter-node interference cancellation. In addition, due to the imperfect self-interference channel estimation, the derivation of TC-UB needs to take the channel estimation error into account. In this case, the conventional method to obtain the TC-UB is not applicable. This motivates us to exploit the dominating interferer region and then utilize Newton-Raphson method to iteratively formulate the TC-UB. The results show that the derived TC-UB is quite close to the actual one especially when the number of receive-antenna is small. Moreover, our proposed beamforming design outperforms the existing beamforming strategies.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d86"},"id":"arXiv:1805.00844v1","url":"http://arxiv.org/abs/1805.00844v1","updated":"2018-05-02T14:30:17Z","published":"2018-05-02T14:30:17Z","title":"Greening Internet of Things for Smart Everythings with A\n  Green-Environment Life: A Survey and Future Prospects","summary":"Tremendous technology development in the field of Internet of Things (IoT) has changed the way we work and live. Although the numerous advantages of IoT are enriching our society, it should be reminded that the IoT also consumes energy, embraces toxic pollution and E-waste. These place new stress on the environments and smart world. In order to increase the benefits and reduce the harm of IoT, there is an increasing desire to move toward green IoT. Green IoT is seen as the future of IoT that is environmentally friendly. To achieve that, it is necessary to put a lot of measures to reduce carbon footprint, conserve fewer resources, and promote efficient techniques for energy usage. It is the reason for moving towards green IoT, where the machines, communications, sensors, clouds, and internet are alongside energy efficiency and reducing carbon emission. This paper presents a thorough survey of the current on-going research work and potential technologies of green IoT with an intention to provide some clues for future green IoT research.","author":["S. H. Alsamhi","Ou Ma","M. Samar Ansari","Qingliang Meng"],"primaryCategory":"eess.SP","category":["eess.SP","cs.NI"],"timestamp_ms":{"$numberLong":"1525296617000"},"text":"Tremendous technology development in the field of Internet of Things (IoT) has changed the way we work and live. Although the numerous advantages of IoT are enriching our society, it should be reminded that the IoT also consumes energy, embraces toxic pollution and E-waste. These place new stress on the environments and smart world. In order to increase the benefits and reduce the harm of IoT, there is an increasing desire to move toward green IoT. Green IoT is seen as the future of IoT that is environmentally friendly. To achieve that, it is necessary to put a lot of measures to reduce carbon footprint, conserve fewer resources, and promote efficient techniques for energy usage. It is the reason for moving towards green IoT, where the machines, communications, sensors, clouds, and internet are alongside energy efficiency and reducing carbon emission. This paper presents a thorough survey of the current on-going research work and potential technologies of green IoT with an intention to provide some clues for future green IoT research.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d85"},"id":"arXiv:1804.10795v1","url":"http://arxiv.org/abs/1804.10795v1","updated":"2018-04-28T12:35:27Z","published":"2018-04-28T12:35:27Z","title":"User-Sensitive Recommendation Ensemble with Clustered Multi-Task\n  Learning","summary":"This paper considers recommendation algorithm ensembles in a user-sensitive manner. Recently researchers have proposed various effective recommendation algorithms, which utilized different aspects of the data and different techniques. However, the \"user skewed prediction\" problem may exist for almost all recommendation algorithms -- algorithms with best average predictive accuracy may cover up that the algorithms may perform poorly for some part of users, which will lead to biased services in real scenarios. In this paper, we propose a user-sensitive ensemble method named \"UREC\" to address this issue. We first cluster users based on the recommendation predictions, then we use multi-task learning to learn the user-sensitive ensemble function for the users. In addition, to alleviate the negative effects of new user problem to clustering users, we propose an approximate approach based on a spectral relaxation. Experiments on real-world datasets demonstrate the superiority of our methods.","author":["Menghan Wang","Xiaolin Zheng","Kun Zhang"],"primaryCategory":"cs.IR","category":["cs.IR"],"timestamp_ms":{"$numberLong":"1524944127000"},"text":"This paper considers recommendation algorithm ensembles in a user-sensitive manner. Recently researchers have proposed various effective recommendation algorithms, which utilized different aspects of the data and different techniques. However, the \"user skewed prediction\" problem may exist for almost all recommendation algorithms -- algorithms with best average predictive accuracy may cover up that the algorithms may perform poorly for some part of users, which will lead to biased services in real scenarios. In this paper, we propose a user-sensitive ensemble method named \"UREC\" to address this issue. We first cluster users based on the recommendation predictions, then we use multi-task learning to learn the user-sensitive ensemble function for the users. In addition, to alleviate the negative effects of new user problem to clustering users, we propose an approximate approach based on a spectral relaxation. Experiments on real-world datasets demonstrate the superiority of our methods.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d84"},"id":"arXiv:1805.00169v1","url":"http://arxiv.org/abs/1805.00169v1","updated":"2018-05-01T03:12:16Z","published":"2018-05-01T03:12:16Z","title":"Multi-Step Knowledge-Aided Iterative ESPRIT for Direction Finding","summary":"In this work, we propose a subspace-based algorithm for DOA estimation which iteratively reduces the disturbance factors of the estimated data covariance matrix and incorporates prior knowledge which is gradually obtained on line. An analysis of the MSE of the reshaped data covariance matrix is carried out along with comparisons between computational complexities of the proposed and existing algorithms. Simulations focusing on closely-spaced sources, where they are uncorrelated and correlated, illustrate the improvements achieved.","author":["S. F. B. Pinto","R. C. de Lamare"],"primaryCategory":"eess.SP","category":["eess.SP","cs.LG","stat.ML"],"timestamp_ms":{"$numberLong":"1525169536000"},"text":"In this work, we propose a subspace-based algorithm for DOA estimation which iteratively reduces the disturbance factors of the estimated data covariance matrix and incorporates prior knowledge which is gradually obtained on line. An analysis of the MSE of the reshaped data covariance matrix is carried out along with comparisons between computational complexities of the proposed and existing algorithms. Simulations focusing on closely-spaced sources, where they are uncorrelated and correlated, illustrate the improvements achieved.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d83"},"id":"arXiv:1804.09769v1","url":"http://arxiv.org/abs/1804.09769v1","updated":"2018-04-25T19:35:56Z","published":"2018-04-25T19:35:56Z","title":"TypeSQL: Knowledge-based Type-Aware Neural Text-to-SQL Generation","summary":"Interacting with relational databases through natural language helps users of any background easily query and analyze a vast amount of data. This requires a system that understands users' questions and converts them to SQL queries automatically. In this paper we present a novel approach, TypeSQL, which views this problem as a slot filling task. Additionally, TypeSQL utilizes type information to better understand rare entities and numbers in natural language questions. We test this idea on the WikiSQL dataset and outperform the prior state-of-the-art by 5.5% in much less time. We also show that accessing the content of databases can significantly improve the performance when users' queries are not well-formed. TypeSQL gets 82.6% accuracy, a 17.5% absolute improvement compared to the previous content-sensitive model.","author":["Tao Yu","Zifan Li","Zilin Zhang","Rui Zhang","Dragomir Radev"],"primaryCategory":"cs.CL","category":["cs.CL"],"timestamp_ms":{"$numberLong":"1524710156000"},"text":"Interacting with relational databases through natural language helps users of any background easily query and analyze a vast amount of data. This requires a system that understands users' questions and converts them to SQL queries automatically. In this paper we present a novel approach, TypeSQL, which views this problem as a slot filling task. Additionally, TypeSQL utilizes type information to better understand rare entities and numbers in natural language questions. We test this idea on the WikiSQL dataset and outperform the prior state-of-the-art by 5.5% in much less time. We also show that accessing the content of databases can significantly improve the performance when users' queries are not well-formed. TypeSQL gets 82.6% accuracy, a 17.5% absolute improvement compared to the previous content-sensitive model.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d82"},"id":"arXiv:1805.00613v1","url":"http://arxiv.org/abs/1805.00613v1","updated":"2018-05-02T03:49:39Z","published":"2018-05-02T03:49:39Z","title":"Deep Perm-Set Net: Learn to Predict Sets with Unknown Permutation and\n  Cardinality Using Deep Neural Networks","summary":"We present a novel approach for learning to predict sets with unknown permutation and cardinality using deep neural networks. Even though the output of many real-world problems, e.g. object detection, are naturally expressed as sets of entities, existing deep learning architectures hinder a trivial extension to deal with this unstructured output. Even deep architectures that handle sequential data, such as recurrent neural networks, can only output an ordered set and may not guarantee a valid solution, i.e. a set with unique elements. In this paper, we derive a mathematical formulation for set prediction using feed-forward neural networks, where the output has unknown and unfixed cardinality and permutation. Specifically, in our formulation we incorporate the permutation as unobservable variable and estimate its distribution during the learning process using alternating optimization. We demonstrate the validity of this formulation on two relevant problems including object detection and a complex CAPTCHA test.","author":["S. Hamid Rezatofighi","Roman Kaskman","Farbod T. Motlagh","Qinfeng Shi","Daniel Cremers","Laura Leal-Taixé","Ian Reid"],"primaryCategory":"cs.CV","category":["cs.CV"],"timestamp_ms":{"$numberLong":"1525258179000"},"text":"We present a novel approach for learning to predict sets with unknown permutation and cardinality using deep neural networks. Even though the output of many real-world problems, e.g. object detection, are naturally expressed as sets of entities, existing deep learning architectures hinder a trivial extension to deal with this unstructured output. Even deep architectures that handle sequential data, such as recurrent neural networks, can only output an ordered set and may not guarantee a valid solution, i.e. a set with unique elements. In this paper, we derive a mathematical formulation for set prediction using feed-forward neural networks, where the output has unknown and unfixed cardinality and permutation. Specifically, in our formulation we incorporate the permutation as unobservable variable and estimate its distribution during the learning process using alternating optimization. We demonstrate the validity of this formulation on two relevant problems including object detection and a complex CAPTCHA test.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d81"},"id":"arXiv:1805.00652v1","url":"http://arxiv.org/abs/1805.00652v1","updated":"2018-05-02T07:21:45Z","published":"2018-05-02T07:21:45Z","title":"MX-LSTM: mixing tracklets and vislets to jointly forecast trajectories\n  and head poses","summary":"Recent approaches on trajectory forecasting use tracklets to predict the future positions of pedestrians exploiting Long Short Term Memory (LSTM) architectures. This paper shows that adding vislets, that is, short sequences of head pose estimations, allows to increase significantly the trajectory forecasting performance. We then propose to use vislets in a novel framework called MX-LSTM, capturing the interplay between tracklets and vislets thanks to a joint unconstrained optimization of full covariance matrices during the LSTM backpropagation. At the same time, MX-LSTM predicts the future head poses, increasing the standard capabilities of the long-term trajectory forecasting approaches. With standard head pose estimators and an attentional-based social pooling, MX-LSTM scores the new trajectory forecasting state-of-the-art in all the considered datasets (Zara01, Zara02, UCY, and TownCentre) with a dramatic margin when the pedestrians slow down, a case where most of the forecasting approaches struggle to provide an accurate solution.","author":["Irtiza Hasan","Francesco Setti","Theodore Tsesmelis","Alessio Del Bue","Fabio Galasso","Marco Cristani"],"primaryCategory":"cs.CV","category":["cs.CV"],"timestamp_ms":{"$numberLong":"1525270905000"},"text":"Recent approaches on trajectory forecasting use tracklets to predict the future positions of pedestrians exploiting Long Short Term Memory (LSTM) architectures. This paper shows that adding vislets, that is, short sequences of head pose estimations, allows to increase significantly the trajectory forecasting performance. We then propose to use vislets in a novel framework called MX-LSTM, capturing the interplay between tracklets and vislets thanks to a joint unconstrained optimization of full covariance matrices during the LSTM backpropagation. At the same time, MX-LSTM predicts the future head poses, increasing the standard capabilities of the long-term trajectory forecasting approaches. With standard head pose estimators and an attentional-based social pooling, MX-LSTM scores the new trajectory forecasting state-of-the-art in all the considered datasets (Zara01, Zara02, UCY, and TownCentre) with a dramatic margin when the pedestrians slow down, a case where most of the forecasting approaches struggle to provide an accurate solution.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d80"},"id":"arXiv:1804.11254v2","url":"http://arxiv.org/abs/1804.11254v2","updated":"2018-05-01T17:45:17Z","published":"2018-04-30T14:59:56Z","title":"Inherent Biases in Reference-based Evaluation for Grammatical Error\n  Correction and Text Simplification","summary":"The prevalent use of too few references for evaluating text-to-text generation is known to bias estimates of their quality (henceforth, {\\it low coverage bias} or LCB). This paper shows that overcoming LCB in Grammatical Error Correction (GEC) evaluation cannot be attained by re-scaling or by increasing the number of references in any feasible range, contrary to previous suggestions. This is due to the long-tailed distribution of valid corrections for a sentence. Concretely, we show that LCB incentivizes GEC systems to avoid correcting even when they can generate a valid correction. Consequently, existing systems obtain comparable or superior performance compared to humans, by making few but targeted changes to the input. Similar effects on Text Simplification further support our claims.","author":["Leshem Choshen","Omeri Abend"],"primaryCategory":"cs.CL","category":["cs.CL"],"timestamp_ms":{"$numberLong":"1525221917000"},"text":"The prevalent use of too few references for evaluating text-to-text generation is known to bias estimates of their quality (henceforth, {\\it low coverage bias} or LCB). This paper shows that overcoming LCB in Grammatical Error Correction (GEC) evaluation cannot be attained by re-scaling or by increasing the number of references in any feasible range, contrary to previous suggestions. This is due to the long-tailed distribution of valid corrections for a sentence. Concretely, we show that LCB incentivizes GEC systems to avoid correcting even when they can generate a valid correction. Consequently, existing systems obtain comparable or superior performance compared to humans, by making few but targeted changes to the input. Similar effects on Text Simplification further support our claims.","relevant":0},{"_id":{"$oid":"5aea850a5a59986e57287d7f"},"id":"arXiv:1805.00195v1","url":"http://arxiv.org/abs/1805.00195v1","updated":"2018-05-01T05:52:12Z","published":"2018-05-01T05:52:12Z","title":"An Annotated Corpus for Machine Reading of Instructions in Wet Lab\n  Protocols","summary":"We describe an effort to annotate a corpus of natural language instructions consisting of 622 wet lab protocols to facilitate automatic or semi-automatic conversion of protocols into a machine-readable format and benefit biological research. Experimental results demonstrate the utility of our corpus for developing machine learning approaches to shallow semantic parsing of instructional texts. We make our annotated Wet Lab Protocol Corpus available to the research community.","author":["Chaitanya Kulkarni","Wei Xu","Alan Ritter","Raghu Machiraju"],"primaryCategory":"cs.CL","category":["cs.CL","cs.AI"],"timestamp_ms":{"$numberLong":"1525179132000"},"text":"We describe an effort to annotate a corpus of natural language instructions consisting of 622 wet lab protocols to facilitate automatic or semi-automatic conversion of protocols into a machine-readable format and benefit biological research. Experimental results demonstrate the utility of our corpus for developing machine learning approaches to shallow semantic parsing of instructional texts. We make our annotated Wet Lab Protocol Corpus available to the research community.","relevant":0}]